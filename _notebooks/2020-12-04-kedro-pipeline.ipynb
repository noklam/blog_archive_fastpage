{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Kedro - pipeline for data science\n",
    "> Kedro\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- sticky_rank: 1\n",
    "- author: noklam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we need a pipeline tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist often starts their development with a Jupyter Notebook. As the notebook grows larger, it's inevitable to convert it to a python script. It starts with one file, then another one, and it accumulates quickly. Converting a notebook could be more than just pasting the code in a script. It involves careful thinking and refactoring.\n",
    "\n",
    "A pipeline library can be helpful in a few ways:\n",
    "- modular pipeline, it can be executed partially.\n",
    "- easily run in parallel\n",
    "- check for loop dependecies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Kedro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to make your data science code reproducible, modular and  well-documented. For example, you can easily create a template for new projects, build a documentation site, lint your code and always have an expected structure to find your config and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kedro is a lightweight pipeline library without need to setup infracstructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to Airflow or Luigi, Kedro is much more lightweighted. It helps you to write production-ready code, and let data engineer and data scientist work together with the same code base. It also has good Jupyter support, so data scientist can still use the tool that they are familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T06:30:00.055986Z",
     "start_time": "2020-09-04T06:30:00.049983Z"
    }
   },
   "source": [
    "```python\n",
    "def split_data(data: pd.DataFrame, example_test_data_ratio: float):\n",
    "    ...\n",
    "    return dict(\n",
    "        train_x=train_data_x,\n",
    "        train_y=train_data_y,\n",
    "        test_x=test_data_x,\n",
    "        test_y=test_data_y,\n",
    "    )\n",
    "```\n",
    "`Node` is the core component of kedro `Pipeline`. For example, we have a python function that split data into train/test set. A node take 4 arguments. `func`, `inputs`, `outputs`, `name`. To use this function as a node, we would write something like this.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "node(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n",
    "                outputs= dict(\n",
    "                train_x=\"example_train_x\",\n",
    "                train_y=\"example_train_y\",\n",
    "                test_x=\"example_test_x\",\n",
    "                test_y=\"example_test_y\",\n",
    "                ),\n",
    "         name=\"split_data\")\n",
    "```\n",
    "\n",
    "It's fairly simple, and it resemble the original function. The only significant difference is, `split_data` takes a `df` and `float`, but in our nodes, it becomes a list of strings. I will explain it in **Section 3.2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T06:38:32.012629Z",
     "start_time": "2020-09-04T06:38:32.005629Z"
    }
   },
   "source": [
    "Pipeline is nothing more than a list of `Node`, it helps you to reuse nodes for different pipelines\n",
    "```python\n",
    "Pipeline([ṅode(),\n",
    "         [node(),\n",
    "            ...]])\n",
    "```\n",
    "\n",
    "Here is an simple `Pipeline` which does splitting data, train a model, make predictions, and report metrics.\n",
    "```python\n",
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                split_data,\n",
    "                [\"example_iris_data\", \"params:example_test_data_ratio\"],\n",
    "                dict(\n",
    "                    train_x=\"example_train_x\",\n",
    "                    train_y=\"example_train_y\",\n",
    "                    test_x=\"example_test_x\",\n",
    "                    test_y=\"example_test_y\",\n",
    "                ),\n",
    "            ),\n",
    "            node(\n",
    "                train_model,\n",
    "                [\"example_train_x\", \"example_train_y\", \"parameters\"],\n",
    "                \"example_model\",\n",
    "            ),\n",
    "            node(\n",
    "                predict,\n",
    "                dict(model=\"example_model\", test_x=\"example_test_x\"),\n",
    "                \"example_predictions\",\n",
    "            ),\n",
    "            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report1'),\n",
    "            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report2'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "You can also use **node tags** or writing different defined pipeline to reuse your node easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kedro Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, Kedro always form a **graph** for your entire pipelines, which can be visaulized with this command.\n",
    "\n",
    "`kedro viz`\n",
    "\n",
    "This starts a web server that visualizes the dependencies of your function, parameters and data,you can also filter some nodes of function with the UI. \n",
    "\n",
    "![viz](images/kedro_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kedro Run, partial pipeline, parallel exeuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/graph.jpg \"A simple pipeline graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T06:44:08.755482Z",
     "start_time": "2020-09-04T06:44:08.749484Z"
    }
   },
   "source": [
    "You can execute your pipeline partially with this command. This with execute your pipeline from A to C except the last Node D.\n",
    "\n",
    "`kedro run --from-nodes=\"A, B, C\"`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay attention to this graph, Node B and Node C has no dependency, they only depends on Node A. With kedro, you can parallelize this exeuction for free by using this command.\n",
    "\n",
    "`kedro run --parallel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have basic understand of what is `Node` and `Pipeline`, you also learnt that you can use `kedro run` command to execute your pipeline with different options. Before I jump into other `kedro` features, let me explain a bit more about **functional programming**. This concept is at the heart of data processing library like `spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional programming, means using functions to program literally. It may sounds silly, but bear with me.\n",
    "\n",
    "**Pure Function** has these characteristics:\n",
    "    1. No side effect, it won't change state outside of the function scope.\n",
    "    2. If you repeating running the same function with same input(argument), it should give you the same output. \n",
    "    3. Easy to parallel if there is no data dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this simple function that add 1 to your input:\n",
    "```python\n",
    "\n",
    "def func1(x):\n",
    "    x=x+1\n",
    "\n",
    "def func2(x):\n",
    "    return x+1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "var1 = 1\n",
    "var2 = 1\n",
    "\n",
    "\n",
    "func1(var1) # var1=2\n",
    "func2(var2) # var2=2\n",
    "```\n",
    "\n",
    "They both add 1 to your input, so which version is a better function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T06:53:32.955884Z",
     "start_time": "2020-09-04T06:53:32.949887Z"
    }
   },
   "source": [
    "```python\n",
    "func1(var1) # var1=3\n",
    "func2(var2) # var2=2    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider if we run this function twice. `func1` changes the result to 3, while `func2` still give you 2. I argue `func2` is better in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this matter? Or how is it going to be useful at all? Well, it makes debugging much easier. It is because you only need to debug code inside a function, not 200 lines of code before it. This greatly reduce the complexity that you have to worried about your data. This fundamental principle is what powering the pipeline, and the reason why you can just use `kedro run --parallel` to parallelize some computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be easier to write test for function. `func1` is harder to test, because you need to consider all possible code path. You may end up need to write verbose test cases like this.\n",
    "```python\n",
    "def test_case1():\n",
    "    func_A()\n",
    "    func_B()\n",
    "    \n",
    "def test_case2():\n",
    "    func_A()\n",
    "    func_A()\n",
    "    func_B()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does using Kedro helps to achieve this? Think about `func1`, if it is written as a `Node`, it will look like this.  \n",
    "```python\n",
    "Node(func1, inputs=var1, output=None, name=\"func1\")\n",
    "```\n",
    "Since it is a Node without any output, it will have no impact to the downstreams. In order to use that variable, you will naturally writing code looks more like `func2` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one more example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "def func3(x):\n",
    "    return x+k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func3(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider `func3`, it is a valid Python function. You can run it in a notebook or in a script, but it wouldn't be possible for a Node, sinec a Node only have access to its input. It will just throw an error to you immediately.\n",
    "\n",
    "```pyton\n",
    "node(func3, inputs='x', outputs='some_result', name='func3')\n",
    "```\n",
    "\n",
    "By writing nodes, you limit your function to only access variable within its scope. It helps to prevent a lot of bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose program to pipeline is not just copy and paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T07:06:15.240215Z",
     "start_time": "2020-09-04T07:06:15.234215Z"
    }
   },
   "source": [
    "I hope the examples demonstrate how writing nodes help transform your code towards functional style. In reality, decoupling your functions from a programming is not straight forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this example.\n",
    "![na](images/nan.jpg)\n",
    "\n",
    "Look at how data `np.nan` is changed. This wouldn't be a problem if we have one program, since we will just passing all variable in memroy, without the step that writing and reading from a file.\n",
    "\n",
    " Error like these are subtle and dangerous, it may not throw error, but ruining our features quality. We have better chance to catch these error in a small program, but it would be much harder to isolate the issue if we have 1000 lines of code. The sooner you integrate it into your pipeline, the easier the integration is. In fact, we can do better. We could introduce test case for validating data, I would explain more in **Section 3.5**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Catalog & Paramaeters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![catalog](images/catalog.jpg \"Supported integration\")\n",
    "Data Catalog is an API for Dataset. It includes a Data Model from from raw data, feature, to reporting layer and a standard Data I/O API. It integrates with pandas, spark, SQLAlchemy and Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Data Catalog, you would first need to define your dataset in the `catalog.yml`. You will have give it a `name`\n",
    "and `type`, denoting whether it is a SQL query or a CSV. Optionally, you can pass in any arguments that are supported from the underlying API as well.\n",
    "\n",
    "```yaml\n",
    "example_iris_data:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/01_raw/iris.csv\n",
    "\n",
    "```\n",
    "\n",
    "### Connect Data Catalog with Node\n",
    "Let's reuse our `split_data` function. When you create a node that using the `split_data` function, you would pass in the string of the dataset instead of an actual dataframe, the Reading/Writing operation is handled by Kedro, so you don't have to write to_csv() or read_csv() yourself.\n",
    "\n",
    "`parameters.yml`\n",
    "```yaml\n",
    "example_test_data_ratio: 0.2\n",
    "```\n",
    "\n",
    "A node using the `split_data` function.\n",
    "```python\n",
    "\n",
    "node(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n",
    "                outputs= dict(\n",
    "                train_x=\"example_train_x\",\n",
    "                train_y=\"example_train_y\",\n",
    "                test_x=\"example_test_x\",\n",
    "                test_y=\"example_test_y\",\n",
    "                ),\n",
    "         name=\"split_data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the inputs \"example_iris_data\" is refering to a dataset defined by `catalog.yml`, kedro will load the csv for you. Same applies for `params:example_test_data_ratio`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `catalog` and `parmaeters`, it already makes your program cleaner. You now have a single file to manager all data source, and a single file contains all parameters, which is configurable. Your functions now is parameterized, you can simply change configuration in a single file without going into every possible script to change a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Catalog abstract away the Data I/O logic from the data processing function.\n",
    "\n",
    "It process data and write a file.\n",
    "```python\n",
    "def process_data(df):\n",
    "    ... # do some processing\n",
    "    df.to_csv('xxx.csv')\n",
    "```\n",
    "\n",
    "It only process data\n",
    "```python\n",
    "def process_data(df):\n",
    "    ... #do some processing\n",
    "    return df\n",
    "```\n",
    "\n",
    "This applies the **single-responsibility principle (SRP)**, meaning that your function is only doing one thing at a time. There are many benefits from it, for example, it makes data versioning easier. I will explain this in **Section 3.3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T07:12:40.591249Z",
     "start_time": "2020-09-04T07:12:40.584252Z"
    }
   },
   "source": [
    "### Memory Dataset (optional to skip)\n",
    "Remember our we pass in a string to our node, and it will look for the corresponding dataset? What if we do not define it? It could be a lot of work if we need to define everything. Besides, some variable are not needed to be written out as a file, it could just stay as in memory.\n",
    "\n",
    "In fact, kedro use `MemroyDataset` by default. Which means you could simply pass in a string that is not defined, the string will be use as the name of the variable. There are more useful dataset like `CacheDataset`, you can find more details in this link.\n",
    "\n",
    "https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.s. When using kedro pipeline, you only define the node's inputs and outputs, but you never defined the order of execution. From my experience, there are pros and cons. The benefits is, your code is less coupled, and due to this, kedro is able to execute your pipeline in parallel whenever possible to speed up your program. However, it means the order of execution is not guaranteed, this may cause unexpected effect. For example, if you are training a machine learning model, it is common to set a random seed at the beginning. Due to the randomness of execution, you may not get identical result, as the order of execution is different everytime, thus the sequence of the random number used is random too. In general this is not a big problem, but if you have a strong need to make sure you have identical output (e.g. regression test), it may cause some trouble and you need to use dummy input and output to force kedro run your pipeline in a specific order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
