{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson learnt from Kaggle - Bengali Image Classification Competition\n",
    "> Lesson learnt from kaggle competition\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [ml, Kaggle]\n",
    "- image: images/bengali_00_header.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have teamed up with a friend to participate in the [Bengali Image Classification Competition](https://www.kaggle.com/c/bengaliai-cv19/?utm_medium=email&utm_source=intercom&utm_campaign=bengaliai-email-launch). We struggled to get a high rank in the Public leaderboard throughout the competition.  In the end, the result is a big surprise to everyone as the leaderboard shook a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Public Leaderboard](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_0_public_lb.png?raw=true \"Public Leaderboard has a much higher score, >0.99 recall!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Public Leaderboard](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_1_private_lb.png?raw=true \"Note that the rank shook for over 1000!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image classification competition. We need to predict 3 parts of __Bengali__ characters `root`, `consonant` and `vowel`. It is a typical classification tasks like the __MNIST__ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grapheme example](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_2_grapheme.png?raw=true \"Examples of characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "The competition use macro-recall as the evaluation metric. In general, people get >96% recall in training, the tops are even getting >99% recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "python\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "scores = []\n",
    "for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "    y_true_subset = solution[solution[component] == component]['target'].values\n",
    "    y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "    scores.append(sklearn.metrics.recall_score(\n",
    "        y_true_subset, y_pred_subset, average='macro'))\n",
    "final_score = np.average(scores, weights=[2,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Bigger still better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with `xresnet50`, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast. \n",
    "\n",
    "Near the end of the competition, we switch to a larger model `se-resnext101`. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with \\~0.3-0.5% recall. It is a big improvement as the recall is very high (\\~0.97), in other words, it reduces __\\~10%__  error solely by just using a better model, not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are never \"enough\" data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation.\n",
    "We start with rotation + scale. We also find __MixUp__ and __CutMix__ is very effective to boost the performance. It also gives us roughly __10%__ boost initially from 0.96 -> 0.964 recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [CutMix](https://arxiv.org/abs/1905.04899) & [MixUp](https://arxiv.org/pdf/1710.09412.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of Augmentation](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_3_data_aug.png?raw=true \"Augmentation Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mixup_ is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2.\n",
    "\n",
    "_CutMix_ shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one.\n",
    "\n",
    "It always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging of Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I normally just log my experiment with a simple CSV and some printing message. This start to get tedious when there are more than 1 people to work. It is important to communicate the results of experiments. I explore `Hydra` and `wandb` in this competition and they are very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hydra](https://hydra.cc/)\n",
    "![Hydra for configuration composition](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_4_hydra.png?raw=true)\n",
    "\n",
    "It is often a good idea to make your experiment configurable. We use `Hydra` for this purpose and it is useful to compose different configuration group.\n",
    "By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model.\n",
    "\n",
    "I have written [an short example](https://mediumnok.ml/coding/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html) for how to use __Hydra__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Wandb](https://www.wandb.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__wandb__ (Weight & Biases) does a few things. It provides built-in functions that automatically  log all your model statistics, you can also log your custom metrics with simple functions.\n",
    "\n",
    "* Compare the configuration of different experiments to find out the model with the best performance.\n",
    "* Built-in function for logging model weights and gradient for debugging purpose.\n",
    "* Log any metrics that you want\n",
    "\n",
    "All of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_06.jpg?raw=true \"Screenshot of wandb UI for cross model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Stochastic Weight Averaging](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/)\n",
    "This is a simple yet effective technique which gives about 0.3-0.4% boost to my model. In simple words, it takes __snapshots__ of the model weights during training and takes an average at the end. It provides a cheap way to do models ensemble while you are only training 1 model. This is important for this competition as it allows me to keep training time short enough to allow feedback within hours and reduce over-fitting.)\n",
    "\n",
    "\n",
    "\n",
    "# ![image.png](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_5_swa.png?raw=true \"Stochastic Weight Averaging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger is better (image size)\n",
    "We downsample our image size to 128x128 throughout the competition, as it makes the model train faster and we believe most technique should be transferable to larger image size. It is important to keep your feedback loop short enough (hours if not days). You want your training data as small as possible while keeping them transferable to your full dataset. Once we scale our image to full size, it takes almost 20 hours to train a single model, and we only have little chance to tune the hyper-parameters before the competition end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug & Checkpoint\n",
    "There was a time we develop our model separately and we didn't sync our code for a while. We refactor our code during the time and it was a huge mistake. It turns out our pre-refactor code trains much better model and we introduce some unknown bug. It is almost impossible to find out as we change multiple things. It is so hard to debug a neural network and testing it thoroughly  is important. Injecting a large amount of code may help you to run an experiment earlier, but you may pay much more time to debug it afterwards.\n",
    "\n",
    "I think this is applicable even if you are working alone.\n",
    "* Keep your changes small.\n",
    "* Establish a baseline early, always do a regression test after a new feature introduced (especially after code refactoring)\n",
    "* Create checkpoint to rollback anytime, especially if you are not working on it every day.\n",
    "\n",
    "Implementation is the key of Kaggle competition (in real life too). It does not matter how great your model is, a tiny little bug could have damaged your model silently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use auxiliary label\n",
    "![image.png](https://github.com/noklam/mediumnok/blob/master/_notebooks/nb_img/bengali_7.png?raw=true \"The extra grapheme label\")\n",
    "As mentioned earlier, this competition requires to predict the `root`, `vowel` and the `consonant` part. In the training data, they actually provide the `grapheme` too. Lots of people saying that if you train with the `grapheme`, it improves the model greatly and get the recall >98% easily.\n",
    "\n",
    "This is something we could not reproduce throughout the competition, we tried it in the very last minute but it does not seem to improve our model. It turns out lots of people are overfitting the data, as the testing dataset has much more unseen character.\n",
    "\n",
    "But it is still a great remark that training with labels that is not your final desired output could still be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight loss\n",
    "The distribution of the training dataset is very imbalance, but to get a good result, we need to predict every single class accurately (macro recall). To deal with this issue, we choose to use class weights, where a higher weight would be applied to rare samples. We don't have an ablation study for this, but it seems to help close the gap between accuracy & recall and allows us to train the model slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a teammate!\n",
    "Lastly, please go and find a teammate if you can. It is very common to start a Kaggle competition, but not so easy to finish them. I have stopped for a month during the competition due to my job. It is really hard to get back to the competition after you stopped for so long. Getting a teammate helps to motivate you and in the end, it is a great learning experience for both of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Model\n",
    "We also tried to use a pretrained model, as it allows shorter training and gives better performance by transfer learning (Using weights learn from a large dataset to as initial weight). It also gives our model a bit of improvement. \n",
    "\n",
    "* Finetune the model head, while keeping other layers freeze (except BatchNorm layer).\n",
    "* Unfreeze the model, train all the layers together.\n",
    "\n",
    "I also tried training the model directly with discriminating learning rate while not freezing any layer at all. It performs similarly  to freezing fine-tuning , so I end up just start training the entire model from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the code works, don't touch it\n",
    "This is probably not a good habit usually, but I suggest not to do it for a competition. We spent lots of time for debugging our code after code refactoring and end up just rolling back to an older commit and cherry-picks new features. In a competition, you don't have enough time to test everything. You do not need a nice abstract class for all your features, some refactoring to keep your function/class clean is probably needed, but do not overspend your time on it. It is even common to jump between frameworks (you may find other's Kernel useful), so it is not possible to structure your code perfectly.\n",
    "\n",
    "* If someone has create a working submission script, use it!\n",
    "* If someone has create a working pre-processing function, use it!\n",
    "\n",
    "Don't spend time on trying to optimize these code unless it is necessary, it is often not worth it in a competition context. You should focus on adding new features, trying out new model, testing with new augmentation technique instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This is a great learning experience and refreshes  some of my outdated computer vision model knowledge. If you have never joined a competition, find a friend and get started. If you have just finished one, try writing it out and share your experience. 😉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
