{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial-yahoo",
   "metadata": {},
   "source": [
    "# Full Stack Deep Learning Notes - Lecture 01\n",
    "> Lecture & Lab notes, explain `DataModules`, `Trainer` `LightningModule`.\n",
    "\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: noklam\n",
    "- categories: [\"python\", \"fsdl\",\"pytorch_lightning\"]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-investing",
   "metadata": {},
   "source": [
    "## Advantages over unstructured PyTorch\n",
    "\n",
    "* Models become hardware agnostic\n",
    "* Code is clear to read because engineering code is abstracted away\n",
    "* Easier to reproduce\n",
    "* Make fewer mistakes because lightning handles the tricky engineering\n",
    "* Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n",
    "* Lightning has dozens of integrations with popular machine learning tools.\n",
    "* [Tested rigorously with every new PR](https://github.com/PyTorchLightning/pytorch-lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n",
    "* Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-interval",
   "metadata": {},
   "source": [
    "# Basic Trainer\n",
    "https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "subject-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class SimpleLightningModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sudden-irrigation",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 7.9 K \n",
      "--------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181d5c74571c4beebe725f7968f9603f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n",
    "\n",
    "mnist_model = SimpleLightningModel()\n",
    "trainer = pl.Trainer(gpus=None, progress_bar_refresh_rate=20, max_epochs=1)    \n",
    "trainer.fit(mnist_model, train_loader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-voltage",
   "metadata": {},
   "source": [
    "If you def `train_dataloader`, `Trainer` will use it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "molecular-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(self):\n",
    "    # REQUIRED\n",
    "    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beginning-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleLightningModel.train_dataloader  = train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "graphic-microwave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 7.9 K \n",
      "--------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e32fc914f2f427595370b655aa83233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_model = SimpleLightningModel()\n",
    "trainer = Trainer(max_epochs=1)\n",
    "trainer.fit(pl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-cruise",
   "metadata": {},
   "source": [
    "`training_step()`,  `train_dataloader()`,`configure_optimizers()` are essential for `LightningModule`.\n",
    "\n",
    "Lifecycle\n",
    "The methods in the LightningModule are called in this order:\n",
    "\n",
    "* `__init__`\n",
    "* `prepare_data`\n",
    "* `configure_optimizers`\n",
    "* `train_dataloader`\n",
    "\n",
    "If you define a validation loop then\n",
    "`val_dataloader`\n",
    "\n",
    "And if you define a test loop:\n",
    "`test_dataloader`\n",
    "\n",
    "You will find `Trainer.fit()` automatically do validation and testing for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "meaningful-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_nb):\n",
    "    # OPTIONAL\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    return {'val_loss': F.cross_entropy(y_hat, y)}\n",
    "\n",
    "def validation_epoch_end(self, outputs):\n",
    "    # OPTIONAL\n",
    "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    tensorboard_logs = {'val_loss': avg_loss}\n",
    "    print(\"Validation Loss: \", avg_loss)\n",
    "    return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "def val_dataloader(self):\n",
    "    # OPTIONAL\n",
    "    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "requested-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleLightningModel.validation_step = validation_step\n",
    "SimpleLightningModel.validation_epoch_end = validation_epoch_end\n",
    "SimpleLightningModel.val_dataloader = val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "separate-lawrence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 7.9 K \n",
      "--------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  tensor(2.3084)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692b7ce421f842018c01dbc37c9537af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  tensor(1.1287)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_model = SimpleLightningModel()\n",
    "trainer = Trainer(max_epochs=2)\n",
    "trainer.fit(pl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-pacific",
   "metadata": {},
   "source": [
    "> Note: If you are running the above cell, you will see validation progress bar in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-performer",
   "metadata": {},
   "source": [
    "By using the trainer you automatically get:\n",
    "* Tensorboard logging\n",
    "* Model checkpointing\n",
    "* Training and validation loop\n",
    "* early-stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-ending",
   "metadata": {},
   "source": [
    "# Pytorch nn.Module versus pl.LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "exotic-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agreed-escape",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0745, 0.0237, 0.4719, 0.6037, 0.6015, 0.0921, 0.5982, 0.4860, 0.0959,\n",
       "         0.5204],\n",
       "        [0.2481, 0.2893, 0.5760, 0.3834, 0.6479, 0.0508, 0.5352, 0.5702, 0.4732,\n",
       "         0.3867],\n",
       "        [0.3467, 0.3321, 0.8570, 0.0983, 0.9210, 0.1848, 0.7397, 0.1350, 0.2646,\n",
       "         0.7202],\n",
       "        [0.6952, 0.8071, 0.1428, 0.3600, 0.1514, 0.2246, 0.8887, 0.9971, 0.0257,\n",
       "         0.5519],\n",
       "        [0.7547, 0.7165, 0.3677, 0.6642, 0.9991, 0.6585, 0.8673, 0.5005, 0.1843,\n",
       "         0.1360],\n",
       "        [0.1809, 0.0794, 0.5101, 0.6751, 0.2822, 0.6695, 0.8085, 0.2127, 0.7562,\n",
       "         0.9859],\n",
       "        [0.5914, 0.4481, 0.5107, 0.0032, 0.9766, 0.4627, 0.1520, 0.2915, 0.4323,\n",
       "         0.3833],\n",
       "        [0.6371, 0.7782, 0.7762, 0.4197, 0.2566, 0.7240, 0.0759, 0.9976, 0.6020,\n",
       "         0.9528],\n",
       "        [0.7674, 0.4044, 0.3497, 0.9784, 0.9318, 0.7313, 0.2962, 0.6555, 0.5570,\n",
       "         0.9998],\n",
       "        [0.1155, 0.8013, 0.7982, 0.5713, 0.2252, 0.4513, 0.8395, 0.7791, 0.1929,\n",
       "         0.7707]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((10,10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "affiliated-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePytorchModel(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "based-split",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4fe0686b1b72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimplePytorchModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch_model = SimplePytorchModel()\n",
    "torch_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-martin",
   "metadata": {},
   "source": [
    "In python, a `NotImplementedError` usually appears when you inherit an abstract class, it is a way to tell you that you should implement `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "manufactured-disco",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1243,  0.2997,  0.0861,  0.1849,  0.7241,  0.2632, -0.0680, -0.2111,\n",
       "         -0.2606,  0.0837],\n",
       "        [-0.0055,  0.1734,  0.2746,  0.1991,  0.6859,  0.2768,  0.0025, -0.2273,\n",
       "         -0.1930,  0.2122],\n",
       "        [-0.1407,  0.2008,  0.3773,  0.0956,  0.9796,  0.1915,  0.2936, -0.0837,\n",
       "         -0.3146,  0.0808],\n",
       "        [-0.0511,  0.1153,  0.2846,  0.2106,  0.7390,  0.0737, -0.1066, -0.3968,\n",
       "         -0.3212,  0.2819],\n",
       "        [-0.3408,  0.3093,  0.3826,  0.0783,  0.5542,  0.1298, -0.1768, -0.1407,\n",
       "         -0.4774,  0.1776],\n",
       "        [-0.1892,  0.2563,  0.1489, -0.0091,  0.4639,  0.1332, -0.0166, -0.3798,\n",
       "         -0.4021,  0.2960],\n",
       "        [-0.1463,  0.0375,  0.4741,  0.0881,  0.5674, -0.0446,  0.1802, -0.2256,\n",
       "         -0.3006,  0.0376],\n",
       "        [-0.1006, -0.1654,  0.3519,  0.3158,  0.5454, -0.0781,  0.0866, -0.4032,\n",
       "         -0.5419,  0.2580],\n",
       "        [-0.4006,  0.3089,  0.3450, -0.1411,  0.4353, -0.0416, -0.1630, -0.4652,\n",
       "         -0.7266,  0.1949],\n",
       "        [-0.1350,  0.0554,  0.1492,  0.4462,  0.8991,  0.2545,  0.1237, -0.1321,\n",
       "         -0.4591,  0.2725]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimplePytorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,10)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "        \n",
    "torch_model = SimplePytorchModel()\n",
    "torch_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-diploma",
   "metadata": {},
   "source": [
    "`pl.LightningModule` is a higher level class for nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "lined-yacht",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-defa0843dab4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpl_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleLightningModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpl_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \"\"\"\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class SimpleLightningModel(pl.LightningModule):\n",
    "    ...\n",
    "    \n",
    "pl_model = SimpleLightningModel()\n",
    "pl_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-isolation",
   "metadata": {},
   "source": [
    "It shouldn't surprise you the same error pop out again, after all, `pl.LightningModule` is a high level wrapper for `nn.Module`. So we need to implement what is the `forward` method too. We can confirm this with this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deadly-contributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(pl.LightningModule, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "synthetic-irish",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9430e-01, -3.2665e-01,  1.5439e-01, -9.5051e-02, -2.6667e-01,\n",
       "          7.0515e-01,  5.4318e-01,  4.8522e-02,  2.2087e-01,  4.6927e-02],\n",
       "        [-1.9757e-01, -4.1862e-01,  1.0334e-01, -1.7735e-01, -3.7793e-01,\n",
       "          7.6570e-01,  5.1128e-01, -5.9839e-04,  2.5192e-01,  9.6547e-02],\n",
       "        [-2.1917e-01, -3.4533e-01,  1.6259e-01, -3.4603e-02, -5.8233e-01,\n",
       "          7.6317e-01,  4.2289e-01, -5.8673e-02,  1.8833e-01,  9.4830e-02],\n",
       "        [ 1.8358e-01, -4.9185e-01,  3.7877e-01, -2.4924e-03,  8.9796e-02,\n",
       "          8.3502e-01,  6.2751e-01, -8.9419e-02,  5.8510e-01,  4.9892e-01],\n",
       "        [-4.1500e-01, -5.1444e-01,  3.3273e-01, -1.9838e-01, -2.7256e-01,\n",
       "          7.2250e-01,  3.3026e-01, -3.0803e-01,  4.8670e-01, -7.5673e-02],\n",
       "        [-3.1485e-01, -5.7277e-01,  1.1172e-01,  2.0040e-01, -1.3642e-01,\n",
       "          1.1535e+00,  4.7762e-01,  1.8485e-01, -1.2243e-01, -7.5894e-02],\n",
       "        [-4.0921e-01, -4.7966e-01,  6.6770e-02, -2.1177e-01, -6.4936e-01,\n",
       "          6.5091e-01,  1.9740e-01, -2.5598e-01,  6.5671e-02,  1.9597e-01],\n",
       "        [-9.3814e-02, -6.7715e-01,  1.8347e-01, -2.4216e-01, -2.0083e-01,\n",
       "          1.1088e+00,  4.1320e-01, -3.5082e-01,  1.6069e-01,  6.4193e-01],\n",
       "        [-4.7541e-01, -8.7359e-01,  2.3989e-01, -3.2175e-01, -2.7573e-01,\n",
       "          9.9955e-01,  3.8217e-01, -2.8564e-01,  1.1412e-02,  7.2301e-02],\n",
       "        [-1.6360e-03, -3.6030e-01,  2.6286e-01,  5.9354e-02,  7.0063e-02,\n",
       "          1.0381e+00,  5.0484e-01, -8.8854e-02,  3.9800e-01,  3.4168e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleLightningModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "pl_model = SimpleLightningModel()\n",
    "pl_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-authentication",
   "metadata": {},
   "source": [
    "# Pytorch Dataloader versus pl.DataMoudle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-trust",
   "metadata": {},
   "source": [
    "A DataModule implements 5 key methods:\n",
    "* prepare_data (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode, e.g. split data).\n",
    "* setup (things to do on every accelerator in distributed mode, e.g. download data).\n",
    "* train_dataloader the training dataloader.\n",
    "* val_dataloader the val dataloader(s).\n",
    "* test_dataloader the test dataloader(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-hometown",
   "metadata": {},
   "source": [
    "> Note: Why do we need to to `setup`? It's more a design choice, the benefit of doing so is that the framework takes care how to do distributed training in most efficient way. On the other hand, if you only doing local training on 1 GPU, there is not much benefit of doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-observer",
   "metadata": {},
   "source": [
    "# Trainer.tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-backing",
   "metadata": {},
   "source": [
    "```python\n",
    "    def tune(self, model, train_dataloader, val_dataloaders, datamodule):\n",
    "        # Run auto batch size scaling\n",
    "        if self.trainer.auto_scale_batch_size:\n",
    "            if isinstance(self.trainer.auto_scale_batch_size, bool):\n",
    "                self.trainer.auto_scale_batch_size = 'power'\n",
    "            self.scale_batch_size(\n",
    "                model,\n",
    "                mode=self.trainer.auto_scale_batch_size,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloaders=val_dataloaders,\n",
    "                datamodule=datamodule,\n",
    "            )\n",
    "\n",
    "        # Run learning rate finder:\n",
    "        if self.trainer.auto_lr_find:\n",
    "            self.lr_find(model, update_attr=True)\n",
    "```            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-defensive",
   "metadata": {},
   "source": [
    "The main usage of `Trainer.tune()` is to automatically find the best learning rate and batch size according to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-place",
   "metadata": {},
   "source": [
    "# Now Back to our Lab1 (training/run_experiment.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-consumption",
   "metadata": {},
   "source": [
    "I slightly modified the script so it can be run inside a notebook instead of using `argparse`. We change these arguments to variable instead.\n",
    "\n",
    "`python3 training/run_experiment.py --model_class=MLP --data_class=MNIST --max_epochs=5 --gpus=1 --fc1=4 --fc2=8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "blind-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add current directory so we can import the library\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"text_recognizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "packed-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\"\"\"Experiment-running framework.\"\"\"\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from text_recognizer import lit_models\n",
    "\n",
    "\n",
    "# In order to ensure reproducible experiments, we must set random seeds.\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def _import_class(module_and_class_name: str) -> type:\n",
    "    \"\"\"Import class from a module, e.g. 'text_recognizer.models.MLP'\"\"\"\n",
    "    module_name, class_name = module_and_class_name.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    class_ = getattr(module, class_name)\n",
    "    return class_\n",
    "\n",
    "\n",
    "def _setup_parser():\n",
    "    \"\"\"Set up Python's ArgumentParser with data, model, trainer, and other arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(add_help=False)\n",
    "\n",
    "    # Add Trainer specific arguments, such as --max_epochs, --gpus, --precision\n",
    "    trainer_parser = pl.Trainer.add_argparse_args(parser)\n",
    "    trainer_parser._action_groups[1].title = \"Trainer Args\"  # pylint: disable=protected-access\n",
    "    parser = argparse.ArgumentParser(add_help=False, parents=[trainer_parser])\n",
    "\n",
    "    # Basic arguments\n",
    "    parser.add_argument(\"--data_class\", type=str, default=\"MNIST\")\n",
    "    parser.add_argument(\"--model_class\", type=str, default=\"MLP\")\n",
    "    parser.add_argument(\"--load_checkpoint\", type=str, default=None)\n",
    "\n",
    "    # Get the data and model classes, so that we can add their specific arguments\n",
    "    temp_args, _ = parser.parse_known_args()\n",
    "    data_class = _import_class(f\"text_recognizer.data.{temp_args.data_class}\")\n",
    "    model_class = _import_class(f\"text_recognizer.models.{temp_args.model_class}\")\n",
    "\n",
    "    # Get data, model, and LitModel specific arguments\n",
    "    data_group = parser.add_argument_group(\"Data Args\")\n",
    "    data_class.add_to_argparse(data_group)\n",
    "\n",
    "    model_group = parser.add_argument_group(\"Model Args\")\n",
    "    model_class.add_to_argparse(model_group)\n",
    "\n",
    "    lit_model_group = parser.add_argument_group(\"LitModel Args\")\n",
    "    lit_models.BaseLitModel.add_to_argparse(lit_model_group)\n",
    "\n",
    "    parser.add_argument(\"--help\", \"-h\", action=\"help\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "pacific-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _setup_parser()\n",
    "args = parser.parse_args([\n",
    "    '--model_class',\n",
    "    'MLP',\n",
    "    '--data_class',\n",
    "    'MNIST',\n",
    "    '--max_epochs',\n",
    "    '5',\n",
    "    '--gpus',\n",
    "    '0',\n",
    "    '--fc1',\n",
    "    '4',\n",
    "    '--fc2',\n",
    "    '8',\n",
    "    ])\n",
    "\n",
    "data_class = _import_class(f\"text_recognizer.data.{args.data_class}\")\n",
    "model_class = _import_class(f\"text_recognizer.models.{args.model_class}\")\n",
    "\n",
    "data = data_class(args)\n",
    "model = model_class(data_config=data.config(), args=args)\n",
    "\n",
    "if args.loss not in ('ctc', 'transformer'):\n",
    "    lit_model_class = lit_models.BaseLitModel\n",
    "\n",
    "if args.load_checkpoint is not None:\n",
    "    lit_model = lit_model_class.load_from_checkpoint(args.load_checkpoint, args=args, model=model)\n",
    "else:\n",
    "    lit_model = lit_model_class(args=args, model=model)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\"training/logs\")\n",
    "\n",
    "callbacks = [pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)]\n",
    "args.weights_summary = \"full\"  # Print full summary of the model\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger, default_root_dir=\"training/logs\")\n",
    "trainer.tune(lit_model, datamodule=data)  # If passing --auto_lr_find, this will set learning rate\n",
    "trainer.fit(lit_model, datamodule=data)\n",
    "trainer.test(lit_model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-criminal",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer.tune(lit_model, datamodule=data)  # If passing --auto_lr_find, this will set learning rate\n",
    "trainer.fit(lit_model, datamodule=data)\n",
    "trainer.test(lit_model, datamodule=data)\n",
    "```\n",
    "1. First line try to find the optimal batch size\n",
    "2. Second line try to trains 5 epochs\n",
    "3. Run test defined in `DataModule`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}