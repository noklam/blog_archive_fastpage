{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet\n",
    "> Kedro pipeline offers some nice feature like automatically release data in memory that is no longer need. How is this possible? Let's dive deep into the code.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: noklam\n",
    "- categories: [\"python\", \"kedro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I am gonna explain some `kedro` internals to understnad how kedor manage your dataset. If you always write imperative python code, you may find that writing `nodes` and `pipeline` is a little bti awkward. They may seems less intuitive, however, it also enable some interesting featrue.\n",
    "\n",
    "This article assumes you have basic understanding of `kedro`, I will focus on `CacheDataSet` and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous **Out of Memory (OOM)** issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I'll keep everything compact in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '../_demo/kedro_dag_demo'\n",
      "C:\\Users\\channo\\OneDrive - The Chinese University of Hong Kong\\Nok\\project-data\\blog__mediumnok\\_demo\\kedro_dag_demo\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%cd ../_demo/kedro_dag_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kedro\n",
    "kedro.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.io import DataCatalog, MemoryDataSet, CachedDataSet\n",
    "from kedro.extras.datasets.pandas import CSVDataSet\n",
    "from kedro.pipeline import node, Pipeline\n",
    "from kedro.runner import SequentialRunner\n",
    "\n",
    "# Prepare a data catalog\n",
    "data_catalog = DataCatalog({\"iris\": CSVDataSet('data/01_raw/iris.csv')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have a pipeline follows this execution order: **A -> B -> C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from kedro.pipeline import Pipeline, node\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def A(df):\n",
    "    print('Loading the Iris Dataset')\n",
    "    return 'Step1'\n",
    "\n",
    "\n",
    "def B(dummy):\n",
    "    return 'Step2'\n",
    "\n",
    "\n",
    "def C(dummy):\n",
    "    return 'Step3'\n",
    "\n",
    "\n",
    "pipeline = Pipeline([node(A, \"iris\", \"A\"),\n",
    "                     node(B, \"A\", \"B\"),\n",
    "                     node(C, \"B\", \"C\"),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To zoom in to the pipeline, we can use `Hook` to print out the catalog after every node's run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.framework.hooks import hook_impl\n",
    "from kedro.framework.hooks import get_hook_manager\n",
    "from pprint import pprint\n",
    "\n",
    "def apply_dict(d):\n",
    "    new_dict = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, CachedDataSet):\n",
    "            if v._cache.exists():\n",
    "                print(v._cache._data)\n",
    "                new_dict[k] = 'In Memory'\n",
    "            else:\n",
    "                new_dict[k] ='Cache Deleted'\n",
    "        elif v.exists():\n",
    "            new_dict[k] = 'In Memory'\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "class DebugHook:\n",
    "    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n",
    "    whenever an error is triggered within a pipeline. The local scope from when the\n",
    "    exception occured is available within this debugging session.\n",
    "    \"\"\"\n",
    "    @hook_impl\n",
    "    def after_node_run(self, node, catalog):\n",
    "        # adding extra behaviour to a single node\n",
    "        print(f\"Finish node {node.name}\")\n",
    "        pprint(f\"Print Catalog {apply_dict(catalog._data_sets)}\")\n",
    "#         pprint(f\"Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}\")\n",
    "        print(\"*****************************\")\n",
    "        \n",
    "hook_manager = get_hook_manager()\n",
    "debug_hook = hook_manager.register(DebugHook());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hook will print out dataset that exist in data catalog. It is a bit tricky because `kedro` did not delete the dataset, it marked the underlying data as `_EMPTY` object instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Iris Dataset\n",
      "Finish node A([iris]) -> [A]\n",
      "\"Print Catalog {'iris': 'In Memory'}\"\n",
      "*****************************\n",
      "Finish node B([A]) -> [B]\n",
      "\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n",
      "*****************************\n",
      "Finish node C([B]) -> [C]\n",
      "\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "# Create a runner to run the pipeline\n",
    "runner = SequentialRunner()\n",
    "\n",
    "# Run the pipeline\n",
    "runner.run(pipeline, data_catalog);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what happened when a `SequentialRunner` runs a pipeline.\n",
    "\n",
    "It is interesting to note that `kedro` takes a similar approach to `Python`, it uses `reference counting` to control the dataset life cycle. If you are interested, I have another post to dive into [Python Memory Management](https://noklam.github.io/blog/python-internal/2021/05/29/Python-Internal-Series-Python-GIL-And-Memory.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "            # decrement load counts and release any data sets we've finished with\n",
    "            for data_set in node.inputs:\n",
    "                load_counts[data_set] -= 1\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n",
    "                    catalog.release(data_set)\n",
    "            for data_set in node.outputs:\n",
    "                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n",
    "                    catalog.release(data_set)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CacheDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `release` do? It will remove the underlying data if this data is stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# In CSVDataSet\n",
    "https://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178\n",
    "```python\n",
    "def _release(self) -> None:\n",
    "    super()._release()\n",
    "    self._invalidate_cache()\n",
    "```\n",
    "    \n",
    "\n",
    "```python\n",
    "# In CacheDataSet\n",
    "def _release(self) -> None:\n",
    "    self._cache.release()\n",
    "    self._dataset.release()\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# In MemoryDataSet\n",
    "def _release(self) -> None:\n",
    "    self._data = _EMPTY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can test if it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))\n",
    "d.load()\n",
    "d._cache._data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._cache.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Iris Dataset\n",
      "Finish node A([iris]) -> [A]\n",
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "0             5.1          3.5           1.4          0.2     setosa\n",
      "1             4.9          3.0           1.4          0.2     setosa\n",
      "2             4.7          3.2           1.3          0.2     setosa\n",
      "3             4.6          3.1           1.5          0.2     setosa\n",
      "4             5.0          3.6           1.4          0.2     setosa\n",
      "..            ...          ...           ...          ...        ...\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "\"Print Catalog {'iris': 'In Memory'}\"\n",
      "*****************************\n",
      "Finish node B([A]) -> [B]\n",
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "0             5.1          3.5           1.4          0.2     setosa\n",
      "1             4.9          3.0           1.4          0.2     setosa\n",
      "2             4.7          3.2           1.3          0.2     setosa\n",
      "3             4.6          3.1           1.5          0.2     setosa\n",
      "4             5.0          3.6           1.4          0.2     setosa\n",
      "..            ...          ...           ...          ...        ...\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n",
      "*****************************\n",
      "Finish node C([B]) -> [C]\n",
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "0             5.1          3.5           1.4          0.2     setosa\n",
      "1             4.9          3.0           1.4          0.2     setosa\n",
      "2             4.7          3.2           1.3          0.2     setosa\n",
      "3             4.6          3.1           1.5          0.2     setosa\n",
      "4             5.0          3.6           1.4          0.2     setosa\n",
      "..            ...          ...           ...          ...        ...\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n",
      "*****************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 'Step3'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_catalog = DataCatalog({\"iris\": CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))})\n",
    "runner.run(pipeline, data_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is persisted throughout the entire pipeline, why? We can monkey patch the `SequentialRunner` to check why is this happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A potential bug or undesired beahvior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from kedro.runner.runner import AbstractRunner, run_node\n",
    "\n",
    "def _run(\n",
    "    self, pipeline, catalog, run_id = None\n",
    ") -> None:\n",
    "    \"\"\"The method implementing sequential pipeline running.\n",
    "\n",
    "    Args:\n",
    "        pipeline: The ``Pipeline`` to run.\n",
    "        catalog: The ``DataCatalog`` from which to fetch data.\n",
    "        run_id: The id of the run.\n",
    "\n",
    "    Raises:\n",
    "        Exception: in case of any downstream node failure.\n",
    "    \"\"\"\n",
    "    nodes = pipeline.nodes\n",
    "    done_nodes = set()\n",
    "\n",
    "    load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n",
    "\n",
    "    for exec_index, node in enumerate(nodes):\n",
    "        try:\n",
    "            run_node(node, catalog, self._is_async, run_id)\n",
    "            done_nodes.add(node)\n",
    "        except Exception:\n",
    "            self._suggest_resume_scenario(pipeline, done_nodes)\n",
    "            raise\n",
    "            \n",
    "        # print load counts for every node run\n",
    "        pprint(f\"{load_counts}\")\n",
    "        print(\"pipeline input: \", pipeline.inputs())\n",
    "        print(\"pipeline output: \", pipeline.outputs())\n",
    "\n",
    "        # decrement load counts and release any data sets we've finished with\n",
    "        for data_set in node.inputs:\n",
    "            load_counts[data_set] -= 1\n",
    "            if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n",
    "                catalog.release(data_set)\n",
    "        for data_set in node.outputs:\n",
    "            if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n",
    "                catalog.release(data_set)\n",
    "\n",
    "        self._logger.info(\n",
    "            \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n",
    "        )\n",
    "        \n",
    "SequentialRunner._run = _run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we re-run the pipeline. Let's reset the hook to only print related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintHook:\n",
    "    @hook_impl\n",
    "    def after_node_run(self, node, catalog):\n",
    "        # adding extra behaviour to a single node\n",
    "        print(f\"Finish node {node.name}\")\n",
    "        print(\"*****************************\")\n",
    "        \n",
    "\n",
    "hook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working.\n",
    "print_hook = hook_manager.register(PrintHook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Iris Dataset\n",
      "Finish node A([iris]) -> [A]\n",
      "*****************************\n",
      "\"Counter({'iris': 1, 'A': 1, 'B': 1})\"\n",
      "pipeline input:  {'iris'}\n",
      "pipeline output:  {'C'}\n",
      "Finish node B([A]) -> [B]\n",
      "*****************************\n",
      "\"Counter({'A': 1, 'B': 1, 'iris': 0})\"\n",
      "pipeline input:  {'iris'}\n",
      "pipeline output:  {'C'}\n",
      "Finish node C([B]) -> [C]\n",
      "*****************************\n",
      "\"Counter({'B': 1, 'iris': 0, 'A': 0})\"\n",
      "pipeline input:  {'iris'}\n",
      "pipeline output:  {'C'}\n"
     ]
    }
   ],
   "source": [
    "# Create a runner to run the pipeline\n",
    "runner = SequentialRunner()\n",
    "\n",
    "# Run the pipeline\n",
    "runner.run(pipeline, data_catalog);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reason why the iris data is kept becasue it is always in `pipeline.inputs()`, which I think is not what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
