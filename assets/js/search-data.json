{
  
    
        "post0": {
            "title": "EuroPython 2022 - Conference Notes & Summary",
            "content": "Schedule: https://ep2022.europython.eu/schedule/2022-07-11 . Session that I attended: . Properties testing with Hypothesis | TDD Development with Pytest | . Bulletproof Python &#8211; Property-Based Testing with Hypothesis . The term property based testing isn&#39;t too important. In a nutshell hypothesis is a python library that help you to write (better) tests by modifying your workflow. . Prepare mock data Provide a specification of data, let hypothesis do the work | Perform some operation | Assert the result with expected value | The rationale behind this is . Note: People write code don&#8217;t come up with good test. For example, you can generate integers with hypotesis.strategies.integers, it does something smart under the hood so it&#39;s not just random number but more meaningful test. For example, you usually want to test for zero, negative number, positive number, large number. hypoethsis try to maximize the variety of tests and you just need to give it a specification. . You can also generate more sophisticated data, for example, a tuple of two integers, where the second integer has to be larger than the first one. . @st.composite def list_and_index(draw, elements=st.integers()):     first = draw(elements)     second = draw(st.integers(min_value=first + 1))     return (first, second) . Think of it as your virtual QA buddy. . TDD Development with pytest . Workflow for TDD . Pick one bug/missing feature | Write a test that fails | Minimal amount of code that pass - (even hard coded!) | Refactor | There are good question ask around . In case of you don&#39;t know what&#39;s the expected answer, how do you write test that fails meaningfully? | . I jump out of the session because of a call, so not too comments about this session. I like the idea of TDD, but I am not sure how to do this in a meaningful way instead of creating tests mechanically. Some natural benefit comes with it is you start thinking about your design from the beginning - because code doesn&#39;t exists yet, it&#39;s could still be useful exercise to write code that are guarantee to fail. .",
            "url": "https://noklam.github.io/blog/europython/2022/07/11/EuroPython2022-Summary.html",
            "relUrl": "/europython/2022/07/11/EuroPython2022-Summary.html",
            "date": " • Jul 11, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Full Stack Deep Learning Notes - Lecture 01",
            "content": "",
            "url": "https://noklam.github.io/blog/fsdl/2022/07/10/template.html",
            "relUrl": "/fsdl/2022/07/10/template.html",
            "date": " • Jul 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing with Mocking",
            "content": "What is Mocking? . pytest-mock . One of the mainstream mocking library is the standard one from unittest, there are also pytest plugin pytest-mock which wraps on unittest. . %load_ext ipython_pytest . %%pytest def test_sum(): assert 1 == 1 . ============================= test session starts ============================= platform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: C: Users lrcno AppData Local Temp tmpiih077gv plugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.06s ============================== . Mocking is important for a few reasons. . You want to have fast unittest (within second) | You don&#39;t want to put loading or have any side-effect to your actual servers/database (e.g. mock writing to a database) | . Mock and MagicMock . There are two main mock object you can used with the standard unittest library from unittest.mock. . from unittest.mock import Mock, MagicMock, patch . Mock . mock = Mock() . With the Mock object, you can treat it like a magic object that have any attributes or methods. . mock.super_method(), mock.attribute_that_does_not_exist_at_all . (&lt;Mock name=&#39;mock.super_method()&#39; id=&#39;1587554283232&#39;&gt;, &lt;Mock name=&#39;mock.attribute_that_does_not_exist_at_all&#39; id=&#39;1587554282512&#39;&gt;) . str(mock) . &#34;&lt;Mock id=&#39;1587554282848&#39;&gt;&#34; . MagicMock . The &quot;magic&quot; comes from the magic methods of python object, for example, when you add two object together, it is calling the __add__ magic method under the hook. . mock + mock . TypeError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_20248/2007264031.py in &lt;cell line: 1&gt;() -&gt; 1 mock + mock TypeError: unsupported operand type(s) for +: &#39;Mock&#39; and &#39;Mock&#39; . magic_mock = MagicMock() . magic_mock + magic_mock . &lt;MagicMock name=&#39;mock.__add__()&#39; id=&#39;1587563722784&#39;&gt; . With MagicMock, you get these magic methods for free, this is why adding two mock will not throw an error but adding two Mock will result in a TypeError . Let say we want to mock the pandas.read_csv function, because we don&#39;t actually want it to read a data, but just return some mock data whenever it is called. It&#39;s easier to explain with an example. . Mocking with real library . %%pytest import pandas as pd def test_read_csv(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. mocker.patch(&quot;pandas.read_csv&quot;, return_value = &quot;fake_data&quot;) assert pd.read_csv(&quot;some_data&quot;) == &quot;fake_data&quot; . ============================= test session starts ============================= platform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: C: Users lrcno AppData Local Temp tmpka9zv6ev plugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.09s ============================== . In reality, you should get a Dataframe object, but here we mock the return value to return a str, and you can see the test actually pass. . mocker.patch with create=True . %%pytest import pandas as pd def test_read_csv(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. mocker.patch(&quot;pandas.read_special_csv&quot;, return_value = &quot;fake_data&quot;, create=False) assert pd.read_special_csv(&quot;some_data&quot;) == &quot;fake_data&quot; . ============================= test session starts ============================= platform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: C: Users lrcno AppData Local Temp tmpzbddlxxg plugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0 collected 1 item _ipytesttmp.py F [100%] ================================== FAILURES =================================== ________________________________ test_read_csv ________________________________ mocker = &lt;pytest_mock.plugin.MockFixture object at 0x00000171B28B1820&gt; def test_read_csv(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. &gt; mocker.patch(&#34;pandas.read_special_csv&#34;, return_value = &#34;fake_data&#34;, create=False) _ipytesttmp.py:4: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .. .. .. .. miniconda3 lib site-packages pytest_mock plugin.py:193: in __call__ return self._start_patch(self.mock_module.patch, *args, **kwargs) .. .. .. .. miniconda3 lib site-packages pytest_mock plugin.py:157: in _start_patch mocked = p.start() .. .. .. .. miniconda3 lib unittest mock.py:1529: in start result = self.__enter__() .. .. .. .. miniconda3 lib unittest mock.py:1393: in __enter__ original, local = self.get_original() _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = &lt;unittest.mock._patch object at 0x00000171B28B10D0&gt; def get_original(self): target = self.getter() name = self.attribute original = DEFAULT local = False try: original = target.__dict__[name] except (AttributeError, KeyError): original = getattr(target, name, DEFAULT) else: local = True if name in _builtins and isinstance(target, ModuleType): self.create = True if not self.create and original is DEFAULT: &gt; raise AttributeError( &#34;%s does not have the attribute %r&#34; % (target, name) ) E AttributeError: &lt;module &#39;pandas&#39; from &#39;c: users lrcno miniconda3 lib site-packages pandas __init__.py&#39;&gt; does not have the attribute &#39;read_special_csv&#39; .. .. .. .. miniconda3 lib unittest mock.py:1366: AttributeError =========================== short test summary info =========================== FAILED _ipytesttmp.py::test_read_csv - AttributeError: &lt;module &#39;pandas&#39; from ... ============================== 1 failed in 0.43s ============================== . Now we fail the test because pandas.read_special_csv does not exist. However, with create=True you can make the test pass again. Normally you won&#39;t want to do this, but it is an option that available. . %%pytest import pandas as pd def test_read_csv(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. mocker.patch(&quot;pandas.read_special_csv&quot;, return_value = &quot;fake_data&quot;, create=True) assert pd.read_special_csv(&quot;some_data&quot;) == &quot;fake_data&quot; . ============================= test session starts ============================= platform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 rootdir: C: Users lrcno AppData Local Temp tmphqbckliw plugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.10s ============================== . More often, you would want your mock resemble your real object, which means it has the same attributes and method, but it should fails when the method being called isn&#39;t valid. You may specify the return_value with the mock type . %%pytest -vvv import pandas as pd from unittest.mock import Mock import pytest def test_read_csv_valid_method(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. mocker.patch(&quot;pandas.read_csv&quot;, return_value = Mock(pd.DataFrame)) df = pd.read_csv(&quot;some_data&quot;) df.mean() # A DataFrame method def test_read_csv_invalid_method(mocker): # mocker is a special pytest fixture, so even though we haven&#39;t define it here but pytest understands it. mocker.patch(&quot;pandas.read_csv&quot;, return_value = Mock(pd.DataFrame)) df = pd.read_csv(&quot;some_data&quot;) with pytest.raises(Exception): df.not_a_dataframe_method() . ============================= test session starts ============================= platform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- c: users lrcno miniconda3 python.exe cachedir: .pytest_cache rootdir: C: Users lrcno AppData Local Temp tmpyfiqtkoy plugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0 collecting ... collected 2 items _ipytesttmp.py::test_read_csv_valid_method PASSED [ 50%] _ipytesttmp.py::test_read_csv_invalid_method PASSED [100%] ============================== 2 passed in 0.16s ============================== .",
            "url": "https://noklam.github.io/blog/python/2022/05/30/mocking-with-pytest-patch.html",
            "relUrl": "/python/2022/05/30/mocking-with-pytest-patch.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "How to achieve Partial Immutability with Python's dataclass?",
            "content": "from dataclasses import dataclass, field, astuple . With dataclass, you can set frozen=True to ensure immutablilty. . @dataclass(frozen=True) class FrozenDataClass: a: int b: int frozen = FrozenDataClass(1,2) frozen.c = 3 . FrozenInstanceError Traceback (most recent call last) Input In [2], in &lt;cell line: 7&gt;() 4 b: int 6 frozen = FrozenDataClass(1,2) -&gt; 7 frozen.c = 3 File &lt;string&gt;:4, in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;c&#39; . Mutating a frozen dataclass is not possible, but what if I need to compose some logic? __post_init__() method is how you can customize logic. . post_init assignment . @dataclass class FrozenDataClass: a: int b: int def __post_init__(self): self.c = self.a + self.b frozen = FrozenDataClass(1,2) . frozen.a, frozen.b, frozen.c . (1, 2, 3) . Do notice that I removed the frozen=True flag, see what happen if I put it back. . What if you just want some of your attribute frozen? . The good old @property? . @dataclass class PartialFrozenDataClass: a: int # a should be frozen b: int # Should be mutable @property def b(self): return self.b p = PartialFrozenDataClass(1,2) . AttributeError Traceback (most recent call last) Input In [9], in &lt;cell line: 10&gt;() 6 @property 7 def b(self): 8 return self.b &gt; 10 p = PartialFrozenDataClass(1,2) File &lt;string&gt;:4, in __init__(self, a, b) AttributeError: can&#39;t set attribute . It doesn&#39;t work! . post_init assignment in a frozen dataclass &#10046; . @dataclass(frozen=True) class FrozenDataClass: a: int b: int def __post_init__(self): self.c = self.a + self.b frozen = FrozenDataClass(1,2) . FrozenInstanceError Traceback (most recent call last) Input In [5], in &lt;cell line: 8&gt;() 6 def __post_init__(self): 7 self.c = self.a + self.b -&gt; 8 frozen = FrozenDataClass(1,2) File &lt;string&gt;:5, in __init__(self, a, b) Input In [5], in FrozenDataClass.__post_init__(self) 6 def __post_init__(self): -&gt; 7 self.c = self.a + self.b File &lt;string&gt;:4, in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;c&#39; . It doesn&#39;t work! Because the frozen flag will block any assignment even in the __post_init__ method. . workaround . @dataclass(frozen=True) class FrozenDataClass: a: int b: int def __post_init__(self): object.__setattr__(self, &#39;c&#39;, self.a + self.b) frozen = FrozenDataClass(1,2) frozen.a, frozen.b, frozen.c . (1, 2, 3) . @dataclass(frozen=True) class FrozenDataClass: a: int b: int def __post_init__(self): super().__setattr__(&#39;c&#39;, self.a + self.b) frozen = FrozenDataClass(1,2) frozen.a, frozen.b, frozen.c . (1, 2, 3) . frozen.c = 3 . FrozenInstanceError Traceback (most recent call last) Input In [19], in &lt;cell line: 1&gt;() -&gt; 1 frozen.c = 3 File &lt;string&gt;:4, in __setattr__(self, name, value) FrozenInstanceError: cannot assign to field &#39;c&#39; . It works as expected, the workaround here is using object.__setattr__. The way dataclass achieve immutability is by blocking assignment in the __setattr__ method. This trick works because we are using the object class method instead of the cls method, thus it won&#39;t stop us assign new attribute. More details can be found in Python Standard Doc. . Conclusion . You can&#39;t really use the normal @property trick either. The post-init assignment in a frozen dataclass is the only workaround. However, with object.__setattr__ it confuses the IDE and it doesn&#39;t understand it is actually an member of the class which is kind of annoying. .",
            "url": "https://noklam.github.io/blog/python/2022/04/22/python-dataclass-partiala-immutable.html",
            "relUrl": "/python/2022/04/22/python-dataclass-partiala-immutable.html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Journey of understanding Python and programming language",
            "content": "To be written… . What is Python Interpreter? . What is Bytecode? . Python Virtual Machine . Compiler . EBNF Grammar . LLVM .",
            "url": "https://noklam.github.io/blog/python/2022/02/10/journey-of-understanding-python-and-programming-langauge.html",
            "relUrl": "/python/2022/02/10/journey-of-understanding-python-and-programming-langauge.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "What can we learn from Shipping Crisis as a Data Scientist?",
            "content": "Even if you are not working in shipping industry, you probably heard about shipping cost is skyrocking for the last year. COVID is clearly the initial disruption, but the story does not end there. Recently, Long Beach’s port congestion is at a historcial scale, there are now more than 70+ ships waiting outside the port, the typical number is 1 or 2. . You may think the terminal must be busy as hell, so did I, but it is actualy far from the truth. In fact, the port is actually paralyzed. The reason surprised me a lot, it is not because of lacking of driver or empty containers, but yard space. Container are being unloaded from ships, then they are being put at the container yard before they go into depot or being stuffed again. . On a high level, it is caused by a negative feedback loop which COVID probably contributed a lot, as it caused a lot of disruption to the supply chain. . Port Congestion -&gt; Containers pilled up at container yard since it is waiting to be loaded on ship | Container yard space is taken up by cotnainers, less space is available | A container need to be put on a chassis before it is loaded, but as the container yard is full, empty containers stuck on the chassis and they need to be unloaded before you put a stuffed container. | Less Chassis is available to load stuff, so it further slow down the process | The loop complete and it starts from 1 again | . This is a simplified story, you can find more details from this twitter thread from flexport’s CEO Ryan. There are more constraints that making this load/unload process inefficient, so the whole process is jammed. Think about a restaurant with limited amount of trays, you need to get a tray if you want to get food. But because there are too many customers, it jammed the door . So there are many customers holding an empty tray while many food are waiting to be served. . Ryan point out a very important lesson here, that is, you need to choose your bottleneck, and it should really be the capital intensive assets. Going back to our restaurant’s analogy, chef and space is probably the most expensive assets, so we should try to keep the utilization high. A simple solution is to buy more trays, so that it won’t be jammed. Ofcourse, you can also find a larger space, build a bigger door, but that will cost you more money too. . For shipping, the terminal’s crane should be the most capital intensive, so we should try our best to keep it working 24/7 to digest the terminal queue. . This is a simple idea yet it is powerful and it strikes me hard. As a data scientist, I work on optimization problem. To maximize the output of a system, we can use linear programming. When we are solving this problem, we are asking question like this. . Given x1 Terminals, x2 drivers, x3 containers, x4 ships, what is the maximize output of this system and how do you arrange them to achieve so? . However, if you are a product/business analyst, a better question may be . What is the output of this system if I add more container yard space? . By changing the input of the system, you may achieve much better result. But as a data scientist, we often stuck in a mode that how do we optimize x metrics with these features. So we may end up spending months and try to schedule ships and driver perfectly to load 10% more container, but you can actually increase loading efficiency by 50% simply by adding more yard space. It feels like cheating as a scientific question, since this is not we asked originally, but this happened a lot in a business context. . We are not trying to find the best algorithm to solve a problem, the algorithm is just one way of doing it. We may get surprising result by just tweaking the asked question a little bit. . I am curious about what is the limiting factor in our current supply chain system, and how sensitive it is to the environment. Is forecasting &amp; optimization the right way to do it? Do we actually need a precise forecast or we can have a bit of redundancy (like in this case, having extra yard space which could be a waste but improve the system robustness)? This is questions that we need to ask ourselves constantly, as the true question is often not asked, but explored after lots of iterations. We need to, and we have to ask the right question, and that is an art more elegant than an algorithm in my opinion. . I do not know if Ryan’s word are 100% true, but it reminds me an important lesson. The right solution (question) may be simple, but it may not be obvious. Have we exploited all the simple solution before we went nuts with fancy algorithms? . p.s. Apologised as I don’t have time to proofread but simply try to write down the snapshot of my current mind [2021-11-18] . Reference . Yesterday I rented a boat and took the leader of one of Flexport&#39;s partners in Long Beach on a 3 hour of the port complex. Here&#39;s a thread about what I learned. . &mdash; Ryan Petersen (@typesfast) October 22, 2021 https://twitter.com/typesfast/status/1451543776992845834?s=20 https://www.facebook.com/669645890/posts/10159859049175891/ unroll version: https://threadreaderapp.com/thread/1451543776992845834.html .",
            "url": "https://noklam.github.io/blog/product/2021/11/18/what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
            "relUrl": "/product/2021/11/18/what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Python FileNotFoundError or You have a really long file path?",
            "content": "FileNotFoundError? Not so quick . . To illustrate the issue, I perpared some fake file. The script is simple, it just read a file with plain text, except that the filename is really long. . . Unforuntately, even though the file exists, Python gives me a FileNotFoundError, how come? However long debugging, I found out that this is related to the filename that exist only on Windows. . This StackOverflow thread explain this issue. . Maximum Path Length Limitation In the Windows API (with some exceptions discussed in the following paragraphs), the maximum length for a path is MAX_PATH, which is defined as 260 characters. A local path is structured in the following order:drive letter, colon, backslash, name components separated by backslashes, and a terminating null character. For example, the maximum path on drive D is &quot;D:*some 256-character path string*&quot; where &quot;&quot; represents the invisible terminating null character for the current system codepage. (The characters &lt; &gt; are used here for visual clarity and cannot be part of a valid path string.)&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Solution - Registry . Updating your Registry can solve this problem. . . After applying the config, I can finally read the file. :) . Summary (TLDR version) . Window filesystem only allow 256 characters, beyond that you will have trouble to open the file. | Python will not be able to see this file and throw FileNotFoundError (I have no idea, anyone know why is that?) | You can update registry to enable long file path in Window to fix this issue. | . (Bonus: Window actually has weird behavior for long filepath, you can try to break it with different ways.) . &lt;/div&gt; .",
            "url": "https://noklam.github.io/blog/python/2021/08/18/python-file-not-found-long-file-path-window.html",
            "relUrl": "/python/2021/08/18/python-file-not-found-long-file-path-window.html",
            "date": " • Aug 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Kedro Telemetry with Grafana",
            "content": "Adding the hooks . https://kedro.readthedocs.io/en/latest/07_extend_kedro/02_hooks.html#add-observability-to-your-pipeline . # &lt;your_project&gt;/src/&lt;your_project&gt;/hooks.py import sys from typing import Any, Dict import statsd from kedro.framework.hooks import hook_impl from kedro.pipeline.node import Node class PipelineMonitoringHooks: def __init__(self): self._timers = {} self._client = statsd.StatsClient(prefix=&quot;kedro&quot;) @hook_impl def before_node_run(self, node: Node) -&gt; None: node_timer = self._client.timer(node.name) node_timer.start() self._timers[node.short_name] = node_timer @hook_impl def after_node_run(self, node: Node, inputs: Dict[str, Any]) -&gt; None: self._timers[node.short_name].stop() for dataset_name, dataset_value in inputs.items(): self._client.gauge(dataset_name + &quot;_size&quot;, sys.getsizeof(dataset_value)) @hook_impl def after_pipeline_run(self): self._client.incr(&quot;run&quot;) . You will need to add this hooks inside your settings.py. HOOKS = (ProjectHooks(), PipelineMonitoringHooks()) . Setup the necessary components . The easiest way to do this is via Docker, I have found a Docker Image that just work perfectly locally. For production, you would want to setup the backend properly. . Grafana (Dashboard) . docker run -d -p 3000:3000 -p 80:80 grafana/grafana . StatsD and Graphite (The database) . docker run -d --name graphite --restart=always -p 80:80 -p 2003-2004:2003-2004 -p 2023-2024:2023-2024 -p 8125:8125/udp -p 8126:8126 graphiteapp/graphite-statsd # https://hub.docker.com/r/graphiteapp/graphite-statsd . Then you need to add Graphite from Data Source via the UI http://localhost:3000 . Your Graphite URL will either be http://localhost:80, http://127.0.0.1:80 or http://192.168.0.1:80. I actually spend hours just to realize my IP was not localhost . .",
            "url": "https://noklam.github.io/blog/python/kedro/2021/07/14/kedro-telemetry-with-grafana.html",
            "relUrl": "/python/kedro/2021/07/14/kedro-telemetry-with-grafana.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "5 Minutes Data Science Design Patterns I - Callback",
            "content": ". Note: These series are written as a quick introduction to software design for data scientists, something that is lightweight than the Design Pattern Bible - Clean Code I wish exists when I first started to learn. Design patterns refer to reusable solutions to some common problems, and some happen to be useful for data science. There is a good chance that someone else has solved your problem before. When used wisely, it helps to reduce the complexity of your code. . So, What is Callback after all? . Callback function, or call after, simply means a function will be called after another function. It is a piece of executable code (function) that passed as an argument to another function. [1] . def foo(x, callback=None): print(&#39;foo!&#39;) if callback: callback(x) . foo(&#39;123&#39;) . foo! . foo(&#39;123&#39;, print) . foo! 123 . Here I pass the function print as a callback, hence the string 123 get printed after foo!. . Why do I need to use Callback? . Callback is very common in high-level deep learning libraries, most likely you will find them in the training loop. . fastai - fastai provide high-level API for PyTorch | Keras - the high-level API for Tensorflow | ignite - they use event &amp; handler, which provides more flexibility in their opinion | . import numpy as np # A boring training Loop def train(x): n_epochs = 3 n_batches = 2 loss = 20 for epoch in range(n_epochs): for batch in range(n_batches): loss = loss - 1 # Pretend we are training the model . x = np.ones(10) train(x); . So, let&#39;s say you now want to print the loss at the end of an epoch. You can just add 1 lines of code. . The simple approach . def train_with_print(x): n_epochs = 3 n_batches = 2 loss = 20 for epoch in range(n_epochs): for batch in range(n_batches): loss = loss - 1 # Pretend we are training the model print(f&#39;End of Epoch. Epoch: {epoch}, Loss: {loss}&#39;) return loss . train_with_print(x); . End of Epoch. Epoch: 0, Loss: 18 End of Epoch. Epoch: 1, Loss: 16 End of Epoch. Epoch: 2, Loss: 14 . Callback approach . Or you call add a PrintCallback, which does the same thing but with a bit more code. . class Callback: def on_epoch_start(self, x): pass def on_epoch_end(self, x): pass def on_batch_start(self, x): pass def on_batch_end(self, x): pass class PrintCallback(Callback): def on_epoch_end(self, x): print(f&#39;End of Epoch. Loss: {x}&#39;) def train_with_callback(x, callback=None): n_epochs = 3 n_batches = 2 loss = 20 for epoch in range(n_epochs): callback.on_epoch_start(loss) for batch in range(n_batches): callback.on_batch_start(loss) loss = loss - 1 # Pretend we are training the model callback.on_batch_end(loss) callback.on_epoch_end(loss) . train_with_callback(x, callback=PrintCallback()); . End of Epoch. Loss: 18 End of Epoch. Loss: 16 End of Epoch. Loss: 14 . Usually, a callback defines a few particular events on_xxx_xxx, which indicate that the function will be executed according to the corresponding condition. So all callbacks will inherit the base class Callback, and override the desired function, here we only implemented the on_epoch_end method because we only want to show the loss at the end. . It may seem awkward to write so many more code to do one simple thing, but there are good reasons. Consider now you need to add more features, how would you do it? . ModelCheckpoint | Early Stopping | LearningRateScheduler | . You can just add code in the loop, but it will start growing into a really giant function. It is impossible to test this function because it does 10 things at the same time. In addition, the extra code may not even be related to the training logic, they are just there to save the model or plot a chart. So, it is best to separate the logic. A function should only do 1 thing according to the Single Responsibility Principle. It helps you to reduce the complexity as it provides a nice abstraction, you are only modifying code within the specific callback you are interested. . Add some more sauce! . When using the Callback Pattern, I can just implement a few more classes and the training loop is barely touched. Here we introduce a new class Callbacks because we need to execute more than 1 callback, it is used for holding all callbacks and executed them sequentially. . class Callbacks: &quot;&quot;&quot; It is the container for callback &quot;&quot;&quot; def __init__(self, callbacks): self.callbacks = callbacks def on_epoch_start(self, x): for callback in self.callbacks: callback.on_epoch_start(x) def on_epoch_end(self, x): for callback in self.callbacks: callback.on_epoch_end(x) def on_batch_start(self, x): for callback in self.callbacks: callback.on_batch_start(x) def on_batch_end(self, x): for callback in self.callbacks: callback.on_batch_end(x) . Then we implement the new Callback one by one, here we only have the pseudocode, but you should get the gist. For example, we only need to save the model at the end of an epoch, thus we implement the method on_epoch_end with a ModelCheckPoint callback. . class PrintCallback(Callback): def on_epoch_end(self, x): print(f&#39;[{type(self).__name__}]: End of Epoch. Loss: {x}&#39;) class ModelCheckPoint(Callback): def on_epoch_end(self, x): print(f&#39;[{type(self).__name__}]: Save Model&#39;) class EarlyStoppingCallback(Callback): def on_epoch_end(self, x): if x &lt; 16: print(f&#39;[{type(self).__name__}]: Early Stopped&#39;) class LearningRateScheduler(Callback): def on_batch_end(self, x): print(f&#39; [{type(self).__name__}]: Reduce learning rate&#39;) . And we also modify the training loop a bit, the argument now takes a Callbacks which contain zero to many callbacks. . def train_with_callbacks(x, callbacks=None): n_epochs = 2 n_batches = 3 loss = 20 for epoch in range(n_epochs): callbacks.on_epoch_start(loss) # on_epoch_start for batch in range(n_batches): callbacks.on_batch_start(loss) # on_batch_start loss = loss - 1 # Pretend we are training the model callbacks.on_batch_end(loss) # on_batch_end callbacks.on_epoch_end(loss) # on_epoch_end . callbacks = Callbacks([PrintCallback(), ModelCheckPoint(), EarlyStoppingCallback(), LearningRateScheduler()]) train_with_callbacks(x, callbacks=callbacks) . [LearningRateScheduler]: Reduce learning rate [LearningRateScheduler]: Reduce learning rate [LearningRateScheduler]: Reduce learning rate [PrintCallback]: End of Epoch. Loss: 17 [ModelCheckPoint]: Save Model [LearningRateScheduler]: Reduce learning rate [LearningRateScheduler]: Reduce learning rate [LearningRateScheduler]: Reduce learning rate [PrintCallback]: End of Epoch. Loss: 14 [ModelCheckPoint]: Save Model [EarlyStoppingCallback]: Early Stopped . Hopefully, it convinces you Callback makes the code cleaner and easier to maintain. If you just use plain if-else statements, you may end up with a big chunk of if-else clauses. . fastai - fastai provide high-level API for PyTorch | Keras - the high-level API for Tensorflow | ignite - they use event &amp; handler, which provides more flexibility in their opinion | . Reference . https://stackoverflow.com/questions/824234/what-is-a-callback-function |",
            "url": "https://noklam.github.io/blog/python/design%20pattern/software/2021/07/10/5minutes-data-science-design-pattern-callback.html",
            "relUrl": "/python/design%20pattern/software/2021/07/10/5minutes-data-science-design-pattern-callback.html",
            "date": " • Jul 10, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet",
            "content": "Today I am gonna explain some kedro internals to understnad how kedor manage your dataset. If you always write imperative python code, you may find that writing nodes and pipeline is a little bti awkward. They may seems less intuitive, however, it also enable some interesting featrue. . This article assumes you have basic understanding of kedro, I will focus on CacheDataSet and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous Out of Memory (OOM) issue. . To start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I&#39;ll keep everything compact in a notebook. . import kedro kedro.__version__ . &#39;0.17.4&#39; . from kedro.io import DataCatalog, MemoryDataSet, CachedDataSet from kedro.extras.datasets.pandas import CSVDataSet from kedro.pipeline import node, Pipeline from kedro.runner import SequentialRunner # Prepare a data catalog data_catalog = DataCatalog({&quot;iris&quot;: CSVDataSet(&#39;data/01_raw/iris.csv&#39;)}) . Next, we have a pipeline follows this execution order: A -&gt; B -&gt; C . from kedro.pipeline import Pipeline, node import pandas as pd def A(df): print(&#39;Loading the Iris Dataset&#39;) return &#39;Step1&#39; def B(dummy): return &#39;Step2&#39; def C(dummy): return &#39;Step3&#39; pipeline = Pipeline([node(A, &quot;iris&quot;, &quot;A&quot;), node(B, &quot;A&quot;, &quot;B&quot;), node(C, &quot;B&quot;, &quot;C&quot;), ]) . c: programdata miniconda3 lib site-packages ipykernel ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . To zoom in to the pipeline, we can use Hook to print out the catalog after every node&#39;s run. . from kedro.framework.hooks import hook_impl from kedro.framework.hooks import get_hook_manager from pprint import pprint def apply_dict(d): new_dict = {} for k, v in d.items(): if isinstance(v, CachedDataSet): if v._cache.exists(): print(v._cache._data) new_dict[k] = &#39;In Memory&#39; else: new_dict[k] =&#39;Cache Deleted&#39; elif v.exists(): new_dict[k] = &#39;In Memory&#39; return new_dict class DebugHook: &quot;&quot;&quot;A hook class for creating a post mortem debugging with the PDB debugger whenever an error is triggered within a pipeline. The local scope from when the exception occured is available within this debugging session. &quot;&quot;&quot; @hook_impl def after_node_run(self, node, catalog): # adding extra behaviour to a single node print(f&quot;Finish node {node.name}&quot;) pprint(f&quot;Print Catalog {apply_dict(catalog._data_sets)}&quot;) # pprint(f&quot;Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}&quot;) print(&quot;*****************************&quot;) hook_manager = get_hook_manager() debug_hook = hook_manager.register(DebugHook()); . This hook will print out dataset that exist in data catalog. It is a bit tricky because kedro did not delete the dataset, it marked the underlying data as _EMPTY object instead. . runner = SequentialRunner() # Run the pipeline runner.run(pipeline, data_catalog); . Loading the Iris Dataset Finish node A([iris]) -&gt; [A] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;}&#34; ***************************** Finish node B([A]) -&gt; [B] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;, &#39;A&#39;: &#39;In Memory&#39;}&#34; ***************************** Finish node C([B]) -&gt; [C] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;, &#39;B&#39;: &#39;In Memory&#39;}&#34; ***************************** . Let&#39;s have a look at what happened when a SequentialRunner runs a pipeline. . It is interesting to note that kedro takes a similar approach to Python, it uses reference counting to control the dataset life cycle. If you are interested, I have another post to dive into Python Memory Management. . # decrement load counts and release any data sets we&#39;ve finished with for data_set in node.inputs: load_counts[data_set] -= 1 if load_counts[data_set] &lt; 1 and data_set not in pipeline.inputs(): catalog.release(data_set) for data_set in node.outputs: if load_counts[data_set] &lt; 1 and data_set not in pipeline.outputs(): catalog.release(data_set) . CacheDataSet . What does release do? It will remove the underlying data if this data is stored in memory. . # In CSVDataSet https://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178 python def _release(self) -&gt; None: super()._release() self._invalidate_cache() . # In CacheDataSet def _release(self) -&gt; None: self._cache.release() self._dataset.release() . # In MemoryDataSet def _release(self) -&gt; None: self._data = _EMPTY . First, we can test if it works as expected. . d = CachedDataSet(CSVDataSet(&#39;data/01_raw/iris.csv&#39;)) d.load() d._cache._data.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . d.exists() . c: programdata miniconda3 lib site-packages ipykernel ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . True . d.release() . d._cache.exists() . False . This is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline. . data_catalog = DataCatalog({&quot;iris&quot;: CachedDataSet(CSVDataSet(&#39;data/01_raw/iris.csv&#39;))}) runner.run(pipeline, data_catalog) . Loading the Iris Dataset Finish node A([iris]) -&gt; [A] sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [150 rows x 5 columns] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;}&#34; ***************************** Finish node B([A]) -&gt; [B] sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [150 rows x 5 columns] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;, &#39;A&#39;: &#39;In Memory&#39;}&#34; ***************************** Finish node C([B]) -&gt; [C] sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 virginica 146 6.3 2.5 5.0 1.9 virginica 147 6.5 3.0 5.2 2.0 virginica 148 6.2 3.4 5.4 2.3 virginica 149 5.9 3.0 5.1 1.8 virginica [150 rows x 5 columns] &#34;Print Catalog {&#39;iris&#39;: &#39;In Memory&#39;, &#39;B&#39;: &#39;In Memory&#39;}&#34; ***************************** . {&#39;C&#39;: &#39;Step3&#39;} . The dataset is persisted throughout the entire pipeline, why? We can monkey patch the SequentialRunner to check why is this happening. . A potential bug or undesired beahvior? . from collections import Counter from itertools import chain from kedro.runner.runner import AbstractRunner, run_node def _run( self, pipeline, catalog, run_id = None ) -&gt; None: &quot;&quot;&quot;The method implementing sequential pipeline running. Args: pipeline: The ``Pipeline`` to run. catalog: The ``DataCatalog`` from which to fetch data. run_id: The id of the run. Raises: Exception: in case of any downstream node failure. &quot;&quot;&quot; nodes = pipeline.nodes done_nodes = set() load_counts = Counter(chain.from_iterable(n.inputs for n in nodes)) for exec_index, node in enumerate(nodes): try: run_node(node, catalog, self._is_async, run_id) done_nodes.add(node) except Exception: self._suggest_resume_scenario(pipeline, done_nodes) raise # print load counts for every node run pprint(f&quot;{load_counts}&quot;) print(&quot;pipeline input: &quot;, pipeline.inputs()) print(&quot;pipeline output: &quot;, pipeline.outputs()) # decrement load counts and release any data sets we&#39;ve finished with for data_set in node.inputs: load_counts[data_set] -= 1 if load_counts[data_set] &lt; 1 and data_set not in pipeline.inputs(): catalog.release(data_set) for data_set in node.outputs: if load_counts[data_set] &lt; 1 and data_set not in pipeline.outputs(): catalog.release(data_set) self._logger.info( &quot;Completed %d out of %d tasks&quot;, exec_index + 1, len(nodes) ) SequentialRunner._run = _run . c: programdata miniconda3 lib site-packages ipykernel ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . Now we re-run the pipeline. Let&#39;s reset the hook to only print related information. . class PrintHook: @hook_impl def after_node_run(self, node, catalog): # adding extra behaviour to a single node print(f&quot;Finish node {node.name}&quot;) print(&quot;*****************************&quot;) hook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working. print_hook = hook_manager.register(PrintHook()) . runner = SequentialRunner() # Run the pipeline runner.run(pipeline, data_catalog); . Loading the Iris Dataset Finish node A([iris]) -&gt; [A] ***************************** &#34;Counter({&#39;iris&#39;: 1, &#39;A&#39;: 1, &#39;B&#39;: 1})&#34; pipeline input: {&#39;iris&#39;} pipeline output: {&#39;C&#39;} Finish node B([A]) -&gt; [B] ***************************** &#34;Counter({&#39;A&#39;: 1, &#39;B&#39;: 1, &#39;iris&#39;: 0})&#34; pipeline input: {&#39;iris&#39;} pipeline output: {&#39;C&#39;} Finish node C([B]) -&gt; [C] ***************************** &#34;Counter({&#39;B&#39;: 1, &#39;iris&#39;: 0, &#39;A&#39;: 0})&#34; pipeline input: {&#39;iris&#39;} pipeline output: {&#39;C&#39;} . Conclusion . So the reason why the iris data is kept becasue it is always in pipeline.inputs(), which I think is not what we wanted. .",
            "url": "https://noklam.github.io/blog/python/kedro/2021/07/02/kedro-datacatalog.html",
            "relUrl": "/python/kedro/2021/07/02/kedro-datacatalog.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Jupyter Superpower - Extend SQL analysis with Python",
            "content": "If you have ever written SQL queries to extract data from a database, chances are you are familiar with an IDE like the screenshot below. The IDE offers features like auto-completion, visualize the query output, display the table schema and the ER diagram. Whenever you need to write a query, this is your go-to tool. However, you may want to add Jupyter Notebook into your toolkit. It improves my productivity by complementing some missing features in IDE. . . # !pip install ipython_sql %load_ext sql %config SqlMagic.displaycon = False %config SqlMagic.feedback = False # Download the file from https://github.com/cwoodruff/ChinookDatabase/blob/master/Scripts/Chinook_Sqlite.sqlite %sql sqlite:///sales.sqlite.db from pathlib import Path DATA_DIR = Path(&#39;../_demo/sql_notebook&#39;) . . %%sql select ProductId, Sum(Unit) from Sales group by ProductId; . ProductId Sum(Unit) . 1 | 210 | . 2 | 50 | . 3 | 30 | . Notebook as a self-contained report . As a data scientist/data analyst, you write SQL queries for ad-hoc analyses all the time. After getting the right data, you make nice-looking charts and put them in a PowerPoint and you are ready to present your findings. Unlike a well-defined ETL job, you are exploring the data and testing your hypotheses all the time. You make assumptions, which is often wrong but you only realized it after a few weeks. But all you got is a CSV that you cannot recall how it was generated in the first place. . Data is not stationary, why should your analysis be? I have seen many screenshots, fragmented scripts flying around in organizations. As a data scientist, I learned that you need to be cautious about what you heard. Don&#39;t trust peoples&#39; words easily, verify the result! To achieve that, we need to know exactly how the data was extracted, what kind of assumptions have been made? Unfortunately, this information usually is not available. As a result, people are redoing the same analysis over and over. You will be surprised that this is very common in organizations. In fact, numbers often do not align because every department has its own definition for a given metric. It is not shared among the organization, and verbal communication is inaccurate and error-prone. It would be really nice if anyone in the organization can reproduce the same result with just a single click. Jupyter Notebook can achieve that reproducibility and keep your entire analysis (documentation, data, and code) in the same place. . Notebook as an extension of IDE . Writing SQL queries in a notebook gives you extra flexibility of a full programming language alongside SQL. For example: . Write complex processing logic that is not easy in pure SQL | Create visualizations directly from SQL results without exporting to an intermediate CSV | . For instance, you can pipe your SQL query with pandas and then make a plot. It allows you to generate analysis with richer content. If you find bugs in your code, you can modify the code and re-run the analysis. This reduces the hustles to reproduce an analysis greatly. In contrast, if your analysis is reading data from an anonymous exported CSV, it is almost guaranteed that the definition of the data will be lost. No one will be able to reproduce the dataset. . You can make use of the ipython_sql library to make queries in a notebook. To do this, you need to use the magic function with the inline magic % or cell magic %%. . sales = %sql SELECT * from sales LIMIT 3 sales . ProductId Unit IsDeleted . 1 | 10 | 1 | . 1 | 10 | 1 | . 2 | 10 | 0 | . To make it fancier, you can even parameterize your query with variables. Tools like papermill allows you to parameterize your notebook. If you execute the notebook regularly with a scheduler, you can get a updated dashboard. To reference the python variable, the $ sign is used. . table = &quot;sales&quot; query = f&quot;SELECT * from {table} LIMIT 3&quot; sales = %sql $query sales . ProductId Unit IsDeleted . 1 | 10 | 1 | . 1 | 10 | 1 | . 2 | 10 | 0 | . With a little bit of python code, you can make a nice plot to summarize your finding. You can even make an interactive plot if you want. This is a very powerful way to extend your analysis. . import seaborn as sns sales = %sql SELECT * FROM SALES sales_df = sales.DataFrame() sales_df = sales_df.groupby(&#39;ProductId&#39;, as_index=False).sum() ax = sns.barplot(x=&#39;ProductId&#39;, y=&#39;Unit&#39;, data=sales_df) ax.set_title(&#39;Sales by ProductId&#39;); . Notebook as a collaboration tool . Jupyter Notebook is flexible and it fits extremely well with exploratory data analysis. To share to a non-coder, you can share the notebook or export it as an HTML file. They can read the report or any cached executed result. If they need to verify the data or add some extra plots, they can do it easily themselves. . It is true that Jupyter Notebook has an infamous reputation. It is not friendly to version control, it&#39;s hard to collaborate with notebooks. Luckily, there are efforts that make collaboration in notebook a lot easier now. . Here what I did not show you is that the table has an isDeleted column. Some of the records are invalid and we should exclude them. In reality, this happens frequently when you are dealing with hundreds of tables that you are not familiar with. These tables are made for applications, transactions, and they do not have analytic in mind. Data Analytic is usually an afterthought. Therefore, you need to consult the SME or the maintainer of that tables. It takes many iterations to get the correct data that can be used to produce useful insight. . With ReviewNB, you can publish your result and invite some domain expert to review your analysis. This is where notebook shine, this kind of workflow is not possible with just the SQL script or a screenshot of your finding. The notebook itself is a useful documentation and collaboration tool. . Step 1 - Review PR online . . You can view your notebook and add comments on a particular cell on ReviewNB. This lowers the technical barrier as your analysts do not have to understand Git. He can review changes and make comments on the web without the need to pull code at all. As soon as your analyst makes a suggestion, you can make changes. . Step 2 - Review Changes . . Once you have made changes to the notebook, you can review it side by side. This is very trivial to do it in your local machine. Without ReviewNB, you have to pull both notebooks separately. As Git tracks line-level changes, you can&#39;t really read the changes as it consists of a lot of confusing noise. It would also be impossible to view changes about the chart with git. . Step 3 - Resolve Discussion . . Once the changes are reviewed, you can resolve the discussion and share your insight with the team. You can publish the notebook to internal sharing platform like knowledge-repo to organize the analysis. . I hope this convince you that Notebook is a good choice for adhoc analytics. It is possible to collaborate with notebook with proper software in place. Regarless if you use notebook or not, you should try your best to document the process. Let&#39;s make more reproducible analyses! .",
            "url": "https://noklam.github.io/blog/python/reviewnb/sql/2021/06/26/Jupyter-SQL-Notebook.html",
            "relUrl": "/python/reviewnb/sql/2021/06/26/Jupyter-SQL-Notebook.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "A logging.config.dictConfig() issue in python",
            "content": "import logging from clearml import Task conf_logging = {&quot;version&quot;:1, &quot;formatters&quot;:{ &quot;simple&quot;:{ &quot;format&quot;:&quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;} } } t = Task.init(project_name=&quot;test&quot;) logging.config.dictConfig(conf_logging) logging.info(&quot;INFO!&quot;) logging.debug(&quot;DEBUG!&quot;) logging.warning(&quot;WARN!&quot;) print(&quot;PRINT!&quot;) . With this code block, you will find no print() or logging is sent to ClearML logging Console. Turns out kedro use logging.config.dictConfig(conf_logging) as the default and causing this issue. . A quick fix is to add &quot;incremental&quot;: True in the config dict. In the standard documentation, the default is False, which means the configuration will replace existing one, thus removing the clearml handlers, and causing the issue I had. . conf_logging = {&quot;version&quot;:1, &quot;incremental&quot;: True &quot;formatters&quot;:{ &quot;simple&quot;:{ &quot;format&quot;:&quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;} } } .",
            "url": "https://noklam.github.io/blog/python/2021/06/20/logging-config-dict-issue-kedro.html",
            "relUrl": "/python/2021/06/20/logging-config-dict-issue-kedro.html",
            "date": " • Jun 20, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
            "content": "Is GIL a bad design? . Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly. . GIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide: . It speed ups single thread program. | It is compatible with many C Program thanks to the C API of CPysthon. | . Global Interpreter Lock a.k.a Mutex Lock . To start with, GIL is a mutex lock. . Why GIL is needed in the first place? . Memory management. Python use something called &quot;reference counting&quot;, which make it different from many modern programming lanaguage. It is what allow Python programmer to lay back and let Python take care when to release memory. Precisely, it is actually the C program controlling the memory life cycle for Python (Cpython). Cpython is known as the default Python interpreter. It first compiles Python to intermediate bytecode (.pyc files). These bytecode then being interpreted by a virtual machine ane executed. It is worth to mention that other variants of Python exist, i.e. IronPython(C#), Jython(Java), Pypy(Python) and they have different memory management mechanisms. . Python Memory Management - Reference Count &amp; Garbage Collection (gc) . import sys . sys.getrefcount(a) . 3 . Reference counting is a simple idea. The intuition is that if a particular object is not referenced by anything, it can be recycled since it will not be used anymore. . For example, the list [1] is now referenced by the variable a, so the reference count is incremented by 1. . import sys a = [1] sys.getrefcount(a) . 2 . Note that the reference count is 2 instead of 1. . The first reference is a = [1] | When the variable a is passed to sys.getrefcount(a) as an argument, it also increases the reference count. | del a . When del a is called, the list [1] have 0 reference count, and it is collected by Python automatically behind the scene. . Lock &amp; Deadlock . Memory Management . Reference . https://www.youtube.com/watch?v=KVKufdTphKs&amp;t=731s | https://realpython.com/python-gil/ | https://devguide.python.org/garbage_collector/ | .",
            "url": "https://noklam.github.io/blog/python-internal/2021/05/29/Python-Internal-Series-Python-GIL-And-Memory.html",
            "relUrl": "/python-internal/2021/05/29/Python-Internal-Series-Python-GIL-And-Memory.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Full Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network",
            "content": "LSTM . Reference: https://colah.github.io/posts/2015-08-Understanding-LSTMs/ . The diagrams are from Chris Colah&#39;s blog. . RNN LSTM . | | . Forget Gate - Control the magnitude of cell state should be kept. Sigmoid range from (0 to 1). If 0, it means we should throw away the state cell, if 1 we keep everything. . Input Gate - Control what relevant information can be added from the current step. It takes hidden step from last step and the current input into consideration. | Output Gate - finalize the next hidden state | . # Google Neurl Machine Translation (GNMT) . It more or less follow the attention mechanism described here. . https://blog.floydhub.com/attention-mechanism/#luong-att-step6 . . 1.If you take the dot product of 1 encoder vector (at t_i) and decoder, you get a scalar. (Alignment Score) (1,h) * (h,1) -&gt; (1,1) . If encoder have 5 time_step, repeat the above steps -&gt; You get a vector with length of 5 (A vector of Alignment Scores) (5,h) *(h,1) -&gt; (5,1) | Take softmax of the alignments scores -&gt; (attention weights which sum to 1) (5,1) | Take dot product of encoders state with attention weights (h, 5) * (5, 1) -&gt; (h, 1), where h stands for dimension of hidden state. The result is a &quot;Context Vector&quot; |",
            "url": "https://noklam.github.io/blog/fsdl/2021/04/16/full-stack-deep-learning-lecture-03.html",
            "relUrl": "/fsdl/2021/04/16/full-stack-deep-learning-lecture-03.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Meta-Analysis of SQL Query, Graph - Part 0 [Help needed, I am looking for open dataset of actual user SQL query log]",
            "content": "Recently, I have started to think more about Data Catalog, Data Discovery. As data scientists, we spent significant time building a data pipeline. We spend a large chunk of the time understanding the context of the data, and how to use it correctly. Most data catalog provides information about the Schema and Column or Table description. However, we deal with dirty data in reality, a table may not be perfectly design, the business process could change but the table schema could not catch up. So you end up having to apply a lot of additional conditions to filter out data, i.e. you may want to look at column A before 2019-Jan but look at column B after 2019-Jan. The ultimate version of the query contains a lot of integration efforts and it is a mental model of how a certain person views the data. This information is usually communicate verbally and very hard to be re-used. . I have been thinking a lot about how can we communicate this information, or so-called domain knowledge more efficiently. I am thinking whether Analyzing user’s actual query log could add more insight into the data. . For example, I may answer these questions if I could analyze SQL query logs How often is this table queried (and by who?) How is this table related to other tables? How are they usually joined together? What is the most common filtering condition added when people are querying this table? . So far, I only have rough ideas and need helps to get an Open Dataset of user SQL query logs. If you know such a dataset exists, please email me @ mediumnok@gmail.com Thanks! .",
            "url": "https://noklam.github.io/blog/python/2021/04/03/Meta-Analysis-of-sql-query-Part-0.html",
            "relUrl": "/python/2021/04/03/Meta-Analysis-of-sql-query-Part-0.html",
            "date": " • Apr 3, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Full Stack Deep Learning Notes - Lecture 02",
            "content": "CNN -Why 55 filter used in LeNet is replaced by 33 now? Why not 2*2? .",
            "url": "https://noklam.github.io/blog/fsdl/2021/03/29/full-stack-deep-learning-lecture-02-CNN.html",
            "relUrl": "/fsdl/2021/03/29/full-stack-deep-learning-lecture-02-CNN.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Microsoft Azure - DP100",
            "content": "Last Updated: 2021-04-22 . Warning - On Azure website, it states that exam outline will be updated on May 20, 2021. Make sure you check out what&#39;s changed. . Introduction . There are 49 questions, some questions have more scores. You need to pass with at least 700/1000 points. For my exam, there are around 10-15% questions that are quite hard, but as long as you score the easy one, you should be able to pass the exam. . There are different type of questions. . Cases Studies - Usually there are some scenarios provided, and you can not go back to previous questions once you answered. For other questions, you can review anytime you want. | Matching - You need to pick a few choice and arrange them in order. i.e. How to create an envioronment tht fulfill the requirements provided. | Multiple Choices Typical MC - It may ask to to select Azure Machine Learning Service/Azure Machine Learning Studio/Azure Databricks/Azure Kubernetes Service (AKS) | Syntax type questions (there are quite a few of them) | Diagram (Azure ML Designer) | . | . I struggled with the first few questions and scratching my head for a while. Don&#39;t panic if you just can&#39;t remember the answer, take a guess, marked it as review question (You can do this with the UI). At the end, I scored 809/1000. . How to prepare for the exam . I spent roughly 15 hours to prepare this exam. Half of the time I used on the lab, the other half for reading docs and historical exam questions. Prior to this exam, I have a little bit experience cloud and I work as a Data Scientist, so it gives me some edges for this exam. But you don&#39;t need to have a lot background knowledge, most of the data science concepts tested is very general and you can learnt from the labs. . Official Suggested Materials . ❗ https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/ - This should be your main focus, try to finish the labs and read the tutorials. You need to understand the use case of different products and familiar yourself with the syntax of Azure ML SDK etc. | https://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/ - You should at least finish 1 of the lab to get some sense of the UI. It would be included in the exam for sure (2-3 questions maybe) | https://docs.microsoft.com/en-us/learn/paths/create-machine-learn-models/ - I didn&#39;t spend much time on it as most of them are baisc data science concepts. You would need to how to apply different types of models (Regression/Classification/Time Series) &amp; AutoML for given scenario. | . Key Concepts . I am pretty sure these concepts will come up in every exam set, so be prepared. . Workspace | DataStore/Blobstore | Compute Target Cluster/VM/ACI link | . | . Compute Target . Machine Learning Studio - single/multi . Development/Experiment - Local Machine/Cloud VM. | Scale up to larger data/distributed - training compute target Azure Machine Learning compute cluster | Azure Machine Learning compute instance | . | Deploy Compute Target (You need to able to judge the most appropiate option based on the context.) Local web service | Azure Kubernetes Service (AKS) | Azure Container Instances | Azure Machine Learning compute clusters (Batch Inference) | . | . DataStore . Azure Storage (blob and file containers) | Azure Data Lake stores | Azure SQL Database | Azure Databricks file system (DBFS) | . Syntax Type Questions . For me, this are the hard questions. There are at least 10 questions that requires you to remember some syntax. If you did not prepared for this, you will find all options seems to be correct. These are the questions that I encountered in my exam. . Come back to here to check your knowledge after you finish the labs. . Run vs mlflow (How to log a metric? What is the syntax with or without mlflow?) run = Run.get_context() | mflow | . | Workspace/Config (how to create a workspace? How to get a reference of a specific workspace? What are the required arguments?) link | Training a model and register a model/deploy | AutoML - Think about what you would do if you are using AutoML. How to retrieve the best iteration model provided the experiment name or Run ID? | What are the Early Stopping choices you can use? link | . | Pipelines - I didn&#39;t prepare well for this part, there are a few questions related to this topic. Again, familiar yourself with the syntax. How to create/publish/schedule a pipeline, what are the syntax? | Do you publish a pipeline or schedule the pipeline first? link | How to retrieve a publihsed pipeline? link | . | How to troubleshoot a service? Where can you find the log? link | Explainer (What are the different use cases for different explainer? What are their limitation?) link How many Explainers are avaiable? (ans: 3!) | . | . Example Questiions . You can also leverage the exam simulator, the free demo version will give you access to 19 questions, if you are willing to pay, you can access all 120 questions. Out of the 19 questions, there are around 4~5 similar questions appear in my DP-100 exam. If your goal is to pass this exam . Website for DP-100, they are updated frequently | Questions - Look for the &quot;Free VCE Files&quot; | Example Simulator) | . Finanlly, good luck to your exam. Try compare and contrast the workflow when you are doing the tutorials. Overall, Azure is well aware with the MLOps trend, the platform makes it quite easy to handle machine learning pipeline and deploy a model. Once you can related it to your daily work, you would find most of the steps are reasonable and easier to remember. .",
            "url": "https://noklam.github.io/blog/azure/2021/03/27/Microsoft-Azure-DP100.html",
            "relUrl": "/azure/2021/03/27/Microsoft-Azure-DP100.html",
            "date": " • Mar 27, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Full Stack Deep Learning Notes - Lecture 01",
            "content": "",
            "url": "https://noklam.github.io/blog/fsdl/2021/03/24/full-stack-deep-learning-lecture-03.html",
            "relUrl": "/fsdl/2021/03/24/full-stack-deep-learning-lecture-03.html",
            "date": " • Mar 24, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Full Stack Deep Learning Notes - Lecture 01",
            "content": "",
            "url": "https://noklam.github.io/blog/fsdl/2021/03/21/template.html",
            "relUrl": "/fsdl/2021/03/21/template.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Full Stack Deep Learning Notes - Lecture 01",
            "content": "Advantages over unstructured PyTorch . Models become hardware agnostic | Code is clear to read because engineering code is abstracted away | Easier to reproduce | Make fewer mistakes because lightning handles the tricky engineering | Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate | Lightning has dozens of integrations with popular machine learning tools. | Tested rigorously with every new PR. We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs. | Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch). | . Basic Trainer . https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html . from pytorch_lightning import Trainer import os import torch from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision.datasets import MNIST from torchvision import transforms import pytorch_lightning as pl class SimpleLightningModel(pl.LightningModule): def __init__(self): super().__init__() self.l1 = torch.nn.Linear(28 * 28, 10) def forward(self, x): return torch.relu(self.l1(x.view(x.size(0), -1))) def training_step(self, batch, batch_nb): x, y = batch loss = F.cross_entropy(self(x), y) tensorboard_logs = {&#39;train_loss&#39;: loss} return {&#39;loss&#39;: loss, &#39;log&#39;: tensorboard_logs} def configure_optimizers(self): return torch.optim.Adam(self.parameters(), lr=0.02) . . train_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32) mnist_model = SimpleLightningModel() trainer = pl.Trainer(gpus=None, progress_bar_refresh_rate=20, max_epochs=1) trainer.fit(mnist_model, train_loader) . GPU available: False, used: False TPU available: None, using: 0 TPU cores | Name | Type | Params -- 0 | l1 | Linear | 7.9 K -- 7.9 K Trainable params 0 Non-trainable params 7.9 K Total params 0.031 Total estimated model params size (MB) c: programdata miniconda3 lib site-packages pytorch_lightning utilities distributed.py:51: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0 Please use self.log(...) inside the lightningModule instead. # log on a step or aggregate epoch metric to the logger and/or progress bar (inside LightningModule) self.log(&#39;train_loss&#39;, loss, on_step=True, on_epoch=True, prog_bar=True) warnings.warn(*args, **kwargs) . . c: programdata miniconda3 lib site-packages pytorch_lightning utilities distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown... warnings.warn(*args, **kwargs) . 1 . If you def train_dataloader, Trainer will use it automatically. . def train_dataloader(self): # REQUIRED return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32) . SimpleLightningModel.train_dataloader = train_dataloader . pl_model = SimpleLightningModel() trainer = Trainer(max_epochs=1) trainer.fit(pl_model) . GPU available: False, used: False TPU available: None, using: 0 TPU cores | Name | Type | Params -- 0 | l1 | Linear | 7.9 K -- 7.9 K Trainable params 0 Non-trainable params 7.9 K Total params 0.031 Total estimated model params size (MB) . . 1 . training_step(), train_dataloader(),configure_optimizers() are essential for LightningModule. . Lifecycle The methods in the LightningModule are called in this order: . __init__ | prepare_data | configure_optimizers | train_dataloader | . If you define a validation loop then val_dataloader . And if you define a test loop: test_dataloader . You will find Trainer.fit() automatically do validation and testing for you. . def validation_step(self, batch, batch_nb): # OPTIONAL x, y = batch y_hat = self(x) return {&#39;val_loss&#39;: F.cross_entropy(y_hat, y)} def validation_epoch_end(self, outputs): # OPTIONAL avg_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean() tensorboard_logs = {&#39;val_loss&#39;: avg_loss} print(&quot;Validation Loss: &quot;, avg_loss) return {&#39;val_loss&#39;: avg_loss, &#39;log&#39;: tensorboard_logs} def val_dataloader(self): # OPTIONAL return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32) . SimpleLightningModel.validation_step = validation_step SimpleLightningModel.validation_epoch_end = validation_epoch_end SimpleLightningModel.val_dataloader = val_dataloader . pl_model = SimpleLightningModel() trainer = Trainer(max_epochs=2) trainer.fit(pl_model) . GPU available: False, used: False TPU available: None, using: 0 TPU cores | Name | Type | Params -- 0 | l1 | Linear | 7.9 K -- 7.9 K Trainable params 0 Non-trainable params 7.9 K Total params 0.031 Total estimated model params size (MB) . Validation Loss: tensor(2.3084) Validation Loss: tensor(1.1287) . c: programdata miniconda3 lib site-packages pytorch_lightning utilities distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown... warnings.warn(*args, **kwargs) . 1 . . Note: If you are running the above cell, you will see validation progress bar in action. . By using the trainer you automatically get: . Tensorboard logging | Model checkpointing | Training and validation loop | early-stopping | . Pytorch nn.Module versus pl.LightningModule . import torch import pytorch_lightning as pl from torch import nn . x = torch.rand((10,10)) x . tensor([[0.0745, 0.0237, 0.4719, 0.6037, 0.6015, 0.0921, 0.5982, 0.4860, 0.0959, 0.5204], [0.2481, 0.2893, 0.5760, 0.3834, 0.6479, 0.0508, 0.5352, 0.5702, 0.4732, 0.3867], [0.3467, 0.3321, 0.8570, 0.0983, 0.9210, 0.1848, 0.7397, 0.1350, 0.2646, 0.7202], [0.6952, 0.8071, 0.1428, 0.3600, 0.1514, 0.2246, 0.8887, 0.9971, 0.0257, 0.5519], [0.7547, 0.7165, 0.3677, 0.6642, 0.9991, 0.6585, 0.8673, 0.5005, 0.1843, 0.1360], [0.1809, 0.0794, 0.5101, 0.6751, 0.2822, 0.6695, 0.8085, 0.2127, 0.7562, 0.9859], [0.5914, 0.4481, 0.5107, 0.0032, 0.9766, 0.4627, 0.1520, 0.2915, 0.4323, 0.3833], [0.6371, 0.7782, 0.7762, 0.4197, 0.2566, 0.7240, 0.0759, 0.9976, 0.6020, 0.9528], [0.7674, 0.4044, 0.3497, 0.9784, 0.9318, 0.7313, 0.2962, 0.6555, 0.5570, 0.9998], [0.1155, 0.8013, 0.7982, 0.5713, 0.2252, 0.4513, 0.8395, 0.7791, 0.1929, 0.7707]]) . class SimplePytorchModel(nn.Module): ... . torch_model = SimplePytorchModel() torch_model(x) . NotImplementedError Traceback (most recent call last) &lt;ipython-input-29-4fe0686b1b72&gt; in &lt;module&gt; 1 torch_model = SimplePytorchModel() -&gt; 2 torch_model(x) c: programdata miniconda3 lib site-packages torch nn modules module.py in _call_impl(self, *input, **kwargs) 887 result = self._slow_forward(*input, **kwargs) 888 else: --&gt; 889 result = self.forward(*input, **kwargs) 890 for hook in itertools.chain( 891 _global_forward_hooks.values(), c: programdata miniconda3 lib site-packages torch nn modules module.py in _forward_unimplemented(self, *input) 199 registered hooks while the latter silently ignores them. 200 &#34;&#34;&#34; --&gt; 201 raise NotImplementedError 202 203 NotImplementedError: . In python, a NotImplementedError usually appears when you inherit an abstract class, it is a way to tell you that you should implement forward method. . class SimplePytorchModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(10,10) def forward(self,x): return self.linear(x) torch_model = SimplePytorchModel() torch_model(x) . tensor([[-0.1243, 0.2997, 0.0861, 0.1849, 0.7241, 0.2632, -0.0680, -0.2111, -0.2606, 0.0837], [-0.0055, 0.1734, 0.2746, 0.1991, 0.6859, 0.2768, 0.0025, -0.2273, -0.1930, 0.2122], [-0.1407, 0.2008, 0.3773, 0.0956, 0.9796, 0.1915, 0.2936, -0.0837, -0.3146, 0.0808], [-0.0511, 0.1153, 0.2846, 0.2106, 0.7390, 0.0737, -0.1066, -0.3968, -0.3212, 0.2819], [-0.3408, 0.3093, 0.3826, 0.0783, 0.5542, 0.1298, -0.1768, -0.1407, -0.4774, 0.1776], [-0.1892, 0.2563, 0.1489, -0.0091, 0.4639, 0.1332, -0.0166, -0.3798, -0.4021, 0.2960], [-0.1463, 0.0375, 0.4741, 0.0881, 0.5674, -0.0446, 0.1802, -0.2256, -0.3006, 0.0376], [-0.1006, -0.1654, 0.3519, 0.3158, 0.5454, -0.0781, 0.0866, -0.4032, -0.5419, 0.2580], [-0.4006, 0.3089, 0.3450, -0.1411, 0.4353, -0.0416, -0.1630, -0.4652, -0.7266, 0.1949], [-0.1350, 0.0554, 0.1492, 0.4462, 0.8991, 0.2545, 0.1237, -0.1321, -0.4591, 0.2725]], grad_fn=&lt;AddmmBackward&gt;) . pl.LightningModule is a higher level class for nn.Module. . class SimpleLightningModel(pl.LightningModule): ... pl_model = SimpleLightningModel() pl_model(x) . NotImplementedError Traceback (most recent call last) &lt;ipython-input-31-defa0843dab4&gt; in &lt;module&gt; 3 4 pl_model = SimpleLightningModel() -&gt; 5 pl_model(x) c: programdata miniconda3 lib site-packages torch nn modules module.py in _call_impl(self, *input, **kwargs) 887 result = self._slow_forward(*input, **kwargs) 888 else: --&gt; 889 result = self.forward(*input, **kwargs) 890 for hook in itertools.chain( 891 _global_forward_hooks.values(), c: programdata miniconda3 lib site-packages pytorch_lightning core lightning.py in forward(self, *args, **kwargs) 504 505 &#34;&#34;&#34; --&gt; 506 return super().forward(*args, **kwargs) 507 508 def training_step(self, *args, **kwargs): c: programdata miniconda3 lib site-packages torch nn modules module.py in _forward_unimplemented(self, *input) 199 registered hooks while the latter silently ignores them. 200 &#34;&#34;&#34; --&gt; 201 raise NotImplementedError 202 203 NotImplementedError: . It shouldn&#39;t surprise you the same error pop out again, after all, pl.LightningModule is a high level wrapper for nn.Module. So we need to implement what is the forward method too. We can confirm this with this line. . issubclass(pl.LightningModule, nn.Module) . True . class SimpleLightningModel(pl.LightningModule): def __init__(self): super().__init__() self.linear = nn.Linear(10,10) def forward(self,x): return self.linear(x) pl_model = SimpleLightningModel() pl_model(x) . tensor([[-1.9430e-01, -3.2665e-01, 1.5439e-01, -9.5051e-02, -2.6667e-01, 7.0515e-01, 5.4318e-01, 4.8522e-02, 2.2087e-01, 4.6927e-02], [-1.9757e-01, -4.1862e-01, 1.0334e-01, -1.7735e-01, -3.7793e-01, 7.6570e-01, 5.1128e-01, -5.9839e-04, 2.5192e-01, 9.6547e-02], [-2.1917e-01, -3.4533e-01, 1.6259e-01, -3.4603e-02, -5.8233e-01, 7.6317e-01, 4.2289e-01, -5.8673e-02, 1.8833e-01, 9.4830e-02], [ 1.8358e-01, -4.9185e-01, 3.7877e-01, -2.4924e-03, 8.9796e-02, 8.3502e-01, 6.2751e-01, -8.9419e-02, 5.8510e-01, 4.9892e-01], [-4.1500e-01, -5.1444e-01, 3.3273e-01, -1.9838e-01, -2.7256e-01, 7.2250e-01, 3.3026e-01, -3.0803e-01, 4.8670e-01, -7.5673e-02], [-3.1485e-01, -5.7277e-01, 1.1172e-01, 2.0040e-01, -1.3642e-01, 1.1535e+00, 4.7762e-01, 1.8485e-01, -1.2243e-01, -7.5894e-02], [-4.0921e-01, -4.7966e-01, 6.6770e-02, -2.1177e-01, -6.4936e-01, 6.5091e-01, 1.9740e-01, -2.5598e-01, 6.5671e-02, 1.9597e-01], [-9.3814e-02, -6.7715e-01, 1.8347e-01, -2.4216e-01, -2.0083e-01, 1.1088e+00, 4.1320e-01, -3.5082e-01, 1.6069e-01, 6.4193e-01], [-4.7541e-01, -8.7359e-01, 2.3989e-01, -3.2175e-01, -2.7573e-01, 9.9955e-01, 3.8217e-01, -2.8564e-01, 1.1412e-02, 7.2301e-02], [-1.6360e-03, -3.6030e-01, 2.6286e-01, 5.9354e-02, 7.0063e-02, 1.0381e+00, 5.0484e-01, -8.8854e-02, 3.9800e-01, 3.4168e-01]], grad_fn=&lt;AddmmBackward&gt;) . Pytorch Dataloader versus pl.DataMoudle . A DataModule implements 5 key methods: . prepare_data (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode, e.g. split data). | setup (things to do on every accelerator in distributed mode, e.g. download data). | train_dataloader the training dataloader. | val_dataloader the val dataloader(s). | test_dataloader the test dataloader(s). | . . Note: Why do we need to to setup? It&#8217;s more a design choice, the benefit of doing so is that the framework takes care how to do distributed training in most efficient way. On the other hand, if you only doing local training on 1 GPU, there is not much benefit of doing so. . Trainer.tune() . def tune(self, model, train_dataloader, val_dataloaders, datamodule): # Run auto batch size scaling if self.trainer.auto_scale_batch_size: if isinstance(self.trainer.auto_scale_batch_size, bool): self.trainer.auto_scale_batch_size = &#39;power&#39; self.scale_batch_size( model, mode=self.trainer.auto_scale_batch_size, train_dataloader=train_dataloader, val_dataloaders=val_dataloaders, datamodule=datamodule, ) # Run learning rate finder: if self.trainer.auto_lr_find: self.lr_find(model, update_attr=True) . The main usage of Trainer.tune() is to automatically find the best learning rate and batch size according to your model. . Now Back to our Lab1 (training/run_experiment.py) . I slightly modified the script so it can be run inside a notebook instead of using argparse. We change these arguments to variable instead. . python3 training/run_experiment.py --model_class=MLP --data_class=MNIST --max_epochs=5 --gpus=1 --fc1=4 --fc2=8 . import os, sys sys.path.append(os.path.join(os.path.dirname(os.getcwd()), &quot;text_recognizer&quot;)) . parser = _setup_parser() args = parser.parse_args([ &#39;--model_class&#39;, &#39;MLP&#39;, &#39;--data_class&#39;, &#39;MNIST&#39;, &#39;--max_epochs&#39;, &#39;5&#39;, &#39;--gpus&#39;, &#39;0&#39;, &#39;--fc1&#39;, &#39;4&#39;, &#39;--fc2&#39;, &#39;8&#39;, ]) data_class = _import_class(f&quot;text_recognizer.data.{args.data_class}&quot;) model_class = _import_class(f&quot;text_recognizer.models.{args.model_class}&quot;) data = data_class(args) model = model_class(data_config=data.config(), args=args) if args.loss not in (&#39;ctc&#39;, &#39;transformer&#39;): lit_model_class = lit_models.BaseLitModel if args.load_checkpoint is not None: lit_model = lit_model_class.load_from_checkpoint(args.load_checkpoint, args=args, model=model) else: lit_model = lit_model_class(args=args, model=model) logger = pl.loggers.TensorBoardLogger(&quot;training/logs&quot;) callbacks = [pl.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, mode=&quot;min&quot;, patience=10)] args.weights_summary = &quot;full&quot; # Print full summary of the model trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger, default_root_dir=&quot;training/logs&quot;) trainer.tune(lit_model, datamodule=data) # If passing --auto_lr_find, this will set learning rate trainer.fit(lit_model, datamodule=data) trainer.test(lit_model, datamodule=data) . trainer.tune(lit_model, datamodule=data) # If passing --auto_lr_find, this will set learning rate trainer.fit(lit_model, datamodule=data) trainer.test(lit_model, datamodule=data) . First line try to find the optimal batch size | Second line try to trains 5 epochs | Run test defined in DataModule |",
            "url": "https://noklam.github.io/blog/fsdl/2021/03/21/full-stack-deep-learning-lecture-01.html",
            "relUrl": "/fsdl/2021/03/21/full-stack-deep-learning-lecture-01.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "deepcopy, LGBM and pickle",
            "content": "To start with, let&#39;s look at some code to get some context. . deepcopy or no copy? . import pandas as pd import numpy as np import lightgbm as lgb from copy import deepcopy params = { &#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3 } X = np.random.rand(100,2) Y = np.ravel(np.random.rand(100,1)) lgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1) print(&quot;Parameters of the model: &quot;, lgbm.params) . Parameters of the model: {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None} . new_model = deepcopy(lgbm) . Finished loading model, total used 1 iterations . You would expect new_model.parameters return the same dict right? Not quite. . print(&quot;Parameters of the copied model: &quot;, new_model.params) . Parameters of the copied model: {} . Surprise, surprise. It&#39;s an empty dict, where did the parameters go? To dive deep into the issue, let&#39;s have a look at the source code of deepcopy to understand how does it work. . reference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128 . def deepcopy(x, memo=None, _nil=[]): &quot;&quot;&quot;Deep copy operation on arbitrary Python objects. See the module&#39;s __doc__ string for more info. &quot;&quot;&quot; ... # skip some irrelevant code cls = type(x) copier = _deepcopy_dispatch.get(cls) if copier is not None: y = copier(x, memo) else: if issubclass(cls, type): y = _deepcopy_atomic(x, memo) else: copier = getattr(x, &quot;__deepcopy__&quot;, None) if copier is not None: y = copier(memo) else: ... # skip irrelevant code # If is its own copy, don&#39;t memoize. if y is not x: memo[d] = y _keep_alive(x, memo) # Make sure x lives at least as long as d return y . In particular, line 17 is what we care. copier = getattr(x, &quot;__deepcopy__&quot;, None) . If a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly. . class DummyClass(): def __deepcopy__(self, _): print(&#39;Just hanging around and not copying.&#39;) . o = DummyClass() deepcopy(o) . Just hanging around and not copying. . a lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary. . def __deepcopy__(self, _): model_str = self.model_to_string(num_iteration=-1) booster = Booster(model_str=model_str) return booster . Reference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282 . Okay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub. . https://github.com/microsoft/LightGBM/issues/4085 Their response is . Custom deepcopy is needed to make Booster class picklable. . &#129366;Italian BMT, &#129388;Lettuce &#127813; tomato and some &#129362;pickles please . What does pickle really is? and what makes an object pickable? . Python Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk. . Serialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It&#39;s like ordering a chair from Ikea, they will send you a box, but not a chair. . The process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling. . . What is Pickle . Pickle is a protocol for Python, you and either pickling a Python object to memory or to file. . import pickle . d = {&#39;a&#39;: 1} pickle_d = pickle.dumps(d) pickle_d . b&#39; x80 x04 x95 n x00 x00 x00 x00 x00 x00 x00} x94 x8c x01a x94K x01s.&#39; . The python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict. . binary_str = b&#39; x80 x04 x95 n x00 x00 x00 x00 x00 x00 x00} x94 x8c x01a x94K x01s.&#39; pickle.loads(binary_str) . {&#39;a&#39;: 1} . Reference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk. . What makes something picklable . Finally, we come back to our initial questions. . What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable? . What can be pickled and unpickled? The following types can be pickled: None, True, and False integers, floating point numbers, complex numbers . * strings, bytes, bytearrays * tuples, lists, sets, and dictionaries containing only picklable objects * functions defined at the top level of a module (using def, not lambda) * built-in functions defined at the top level of a module * classes that are defined at the top level of a module . So pretty much common datatype, functions and classes are picklable. Let&#39;s see without __deepcopy__, the Booster class is not serializable as it claims. . import lightgbm from lightgbm import Booster del Booster.__deepcopy__ params = { &#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3 } X = np.random.rand(100,2) Y = np.ravel(np.random.rand(100,1)) lgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1) deepcopy_lgbm = deepcopy(lgbm) lgbm.params, deepcopy_lgbm.params . ({&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}, {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}) . pickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm) . True . unpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm)) unpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm)) . unpickle_model.params, unpickle_deepcopy_model.params . ({&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}, {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}) . unpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string() . True . unpickle_deepcopy_model.predict(X) . array([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803, 0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787, 0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787, 0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787]) . Last Word . Well.... It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary. . I tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298 .",
            "url": "https://noklam.github.io/blog/python/pickle/deepcopy/2021/03/19/deepcopy-lightgbm-and-Pickles.html",
            "relUrl": "/python/pickle/deepcopy/2021/03/19/deepcopy-lightgbm-and-Pickles.html",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Data Test as CI",
            "content": "I am running test with great_expectations that validate data in UAT and production server with CI, so it would be nice if the log can capture this. . I created a custom error class that would do the job, however, pytest truncated my AssertionError since it is quite long. . I am using pytest magic from https://github.com/akaihola/ipython_pytest which allow me to run pytest in a Jupyter notebook cell. . It is quite simple with a few tens of lines. . %%writefile ipython_pytest.py import os import shlex import sys from pathlib import Path import tempfile from IPython.core import magic from pytest import main as pytest_main TEST_MODULE_NAME = &#39;_ipytesttmp&#39; def pytest(line, cell): with tempfile.TemporaryDirectory() as root: oldcwd = os.getcwd() os.chdir(root) tests_module_path = &#39;{}.py&#39;.format(TEST_MODULE_NAME) try: Path(tests_module_path).write_text(cell) args = shlex.split(line) os.environ[&#39;COLUMNS&#39;] = &#39;80&#39; pytest_main(args + [tests_module_path]) if TEST_MODULE_NAME in sys.modules: del sys.modules[TEST_MODULE_NAME] finally: os.chdir(oldcwd) def load_ipython_extension(ipython): magic.register_cell_magic(pytest) . Writing ipython_pytest.py . %load_ext ipython_pytest . The ipython_pytest extension is already loaded. To reload it, use: %reload_ext ipython_pytest . %%pytest def test_long_assertion_error(): x = &quot;placeholder&quot; expect = &quot;abcdefg n&quot;*20 # Long string assert x == expect . ============================= test session starts ============================= platform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 rootdir: C: Users channo AppData Local Temp tmpohw9e_9w collected 1 item _ipytesttmp.py F [100%] ================================== FAILURES =================================== __________________________ test_long_assertion_error __________________________ def test_long_assertion_error(): x = &#34;placeholder&#34; expect = &#34;abcdefg n&#34;*20 # Long string &gt; assert x == expect E AssertionError: assert &#39;placeholder&#39; == &#39;abcdefg nabc...fg nabcdefg n&#39; E + placeholder E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg... E E ...Full output truncated (15 lines hidden), use &#39;-vv&#39; to show _ipytesttmp.py:5: AssertionError =========================== short test summary info =========================== FAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert &#39;pl... ============================== 1 failed in 0.06s ============================== . You can see that pytest truncated my error with ... Here is how I solve ths issue . %%pytest -vv def test_long_assertion_error(): x = &quot;placeholder&quot; expect = &quot;abcdefg n&quot;*20 # Long string assert x == expect . ============================= test session starts ============================= platform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- c: programdata miniconda3 python.exe cachedir: .pytest_cache rootdir: C: Users channo AppData Local Temp tmpyic4vcra collecting ... collected 1 item _ipytesttmp.py::test_long_assertion_error FAILED [100%] ================================== FAILURES =================================== __________________________ test_long_assertion_error __________________________ def test_long_assertion_error(): x = &#34;placeholder&#34; expect = &#34;abcdefg n&#34;*20 # Long string &gt; assert x == expect E AssertionError: assert &#39;placeholder&#39; == (&#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39; n &#39;abcdefg n&#39;) E + placeholder E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg E - abcdefg _ipytesttmp.py:5: AssertionError =========================== short test summary info =========================== FAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert &#39;pl... ============================== 1 failed in 0.06s ============================== .",
            "url": "https://noklam.github.io/blog/python/2021/03/17/pytest-data-test-truncated-error.html",
            "relUrl": "/python/2021/03/17/pytest-data-test-truncated-error.html",
            "date": " • Mar 17, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
            "content": "Introduction . Long story short, connect with Impala is a big headache in Windows. pyhive, impyla are both buggy. At the end, I stick with pyodbc as it works on both Linux and Windows, and seems to have better performance. There are not many steps, but it would be tricky if you try to Google as there are not much guide that just work out of the box . Setup . First, you need to download the ODBC driver from Cloudera. . Then you need to instsall the driver properly. . dpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb . Add this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file. . # /etc/odbcinst.ini [ODBC Drivers] Cloudera Impala ODBC Driver 32-bit=Installed Cloudera Impala ODBC Driver 64-bit=Installed [Cloudera Impala ODBC Driver 32-bit] Description=Cloudera Impala ODBC Driver (32-bit) Driver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so [Cloudera Impala ODBC Driver 64-bit] Description=Cloudera Impala ODBC Driver (64-bit) Driver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so . Then install some additional package. . apt-get update &amp;&amp; apt-get -y install gnupg apt-transport-https apt-get update &amp;&amp; apt-get -y install libssl1.0.0 unixodbc unixodbc-dev &amp;&amp; ACCEPT_EULA=Y apt-get -y install msodbcsql17 apt-get install unixodbc-dev -y . Last, pip install pyodbc and have fun. . To read a database table, you can simply do this. . import pyodbc import pandas as pd conn = pyodbc.connect(f&quot;&quot;&quot; Driver=Cloudera ODBC Driver for Impala 64-bit; PWD=password; UID=username; Database=database &quot;&quot;&quot;) . There are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup. .",
            "url": "https://noklam.github.io/blog/pyodbc/impala/2021/03/05/pyodbc-linux.html",
            "relUrl": "/pyodbc/impala/2021/03/05/pyodbc-linux.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Create python command line in few lines, and use it anywhere as a standalone tool!",
            "content": "The Typer documentation has great example explaining how to use it. This is the example copied from their GitHub homepage. https://github.com/tiangolo/typer. . %%writefile main1.py import typer def main(name: str): typer.echo(f&quot;Hello {name}&quot;) if __name__ == &quot;__main__&quot;: typer.run(main) . Overwriting main1.py . !python main1.py world . Hello world . !python main1.py --help . Usage: main1.py [OPTIONS] NAME Arguments: NAME [required] Options: --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. . Here I write a new file to main1.py and execute it as a command line with just 5 lines of code. It always comes with a help message for free. Let&#39;s see another example. . %%writefile main2.py import typer def main(name: str, age: int = 20, height_meters: float = 1.89, female: bool = True): typer.echo(f&quot;NAME is {name}, of type: {type(name)}&quot;) typer.echo(f&quot;--age is {age}, of type: {type(age)}&quot;) typer.echo(f&quot;--height-meters is {height_meters}, of type: {type(height_meters)}&quot;) typer.echo(f&quot;--female is {female}, of type: {type(female)}&quot;) if __name__ == &quot;__main__&quot;: typer.run(main) . Writing main2.py . !python main2.py --help . Usage: main2.py [OPTIONS] NAME Arguments: NAME [required] Options: --age INTEGER [default: 20] --height-meters FLOAT [default: 1.89] --female / --no-female [default: True] --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. . This time, we can see that the help message even supplement the expected datatype. Typer will validate the type and conevrt it when possible. . !python main2.py Nok --age=3 . NAME is Nok, of type: &lt;class &#39;str&#39;&gt; --age is 3, of type: &lt;class &#39;int&#39;&gt; --height-meters is 1.89, of type: &lt;class &#39;float&#39;&gt; --female is True, of type: &lt;class &#39;bool&#39;&gt; . The command line works file, but it only works in the same directory, and you always have to type the keyword python. With python setuptools, we can actually installed a command line and run it anywhere. It is pretty easy with just 1 trick, let&#39;s go back to the simple Hello command. . %%writefile main3.py import typer def hello(name:str): typer.echo(f&quot;Hello {name}&quot;) def main(): typer.run(hello) . Overwriting main3.py . Here we made a few changes. . The logic is move to a new function named hello | We removed the __main__ part, as we will not call this python file directly anymore. | typer.run(main) is changed to typer.run(hello) and moved inside the main function. | Console Script . We will use setuptool to build console script, which may call the function main. The magic is using console script to install a command line interface (It creates a .exe file) that can be run anywhere. We can name our command line instead of using the filename with a pattern of command_name=file:func_name. Here our function main is inside a file main3.py, so we use hello=main3:main. . %%writefile setup.py from setuptools import setup, find_packages setup( name=&quot;my_library&quot;, version=&quot;1.0&quot;, packages=find_packages(), entry_points = { &#39;console_scripts&#39;: [&#39;hello=main3:main&#39;]} ) . Overwriting setup.py . Then we install the console script . . !python setup.py develop . !hello world . Hello world . We can now call hello anywhere, as it is installed as a executable. . !where hello . C: ProgramData Miniconda3 Scripts hello.exe . It&#39;s time for you to build your own commands. This can be easily extended to support multiple commands. https://github.com/tiangolo/typer .",
            "url": "https://noklam.github.io/blog/python/cli/typer/2020/12/10/Typer-create-command-line-and-use-it-anywhere.html",
            "relUrl": "/python/cli/typer/2020/12/10/Typer-create-command-line-and-use-it-anywhere.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Introduction to Kedro - pipeline for data science",
            "content": "Why we need a pipeline tool . Data Scientist often starts their development with a Jupyter Notebook. As the notebook grows larger, it&#39;s inevitable to convert it to a python script. It starts with one file, then another one, and it accumulates quickly. Converting a notebook could be more than just pasting the code in a script. It involves careful thinking and refactoring. . A pipeline library can be helpful in a few ways: . modular pipeline, it can be executed partially. | easily run in parallel | check for loop dependecies | . What is Kedro . Kedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to make your data science code reproducible, modular and well-documented. For example, you can easily create a template for new projects, build a documentation site, lint your code and always have an expected structure to find your config and data. . Kedro is a lightweight pipeline library without need to setup infracstructure. . In comparison to Airflow or Luigi, Kedro is much more lightweighted. It helps you to write production-ready code, and let data engineer and data scientist work together with the same code base. It also has good Jupyter support, so data scientist can still use the tool that they are familiar with. . Functions and Pipeline . Nodes . def split_data(data: pd.DataFrame, example_test_data_ratio: float): ... return dict( train_x=train_data_x, train_y=train_data_y, test_x=test_data_x, test_y=test_data_y, ) . Node is the core component of kedro Pipeline. For example, we have a python function that split data into train/test set. A node take 4 arguments. func, inputs, outputs, name. To use this function as a node, we would write something like this. . node(split_data, inputs=[&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], outputs= dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), name=&quot;split_data&quot;) . It&#39;s fairly simple, and it resemble the original function. The only significant difference is, split_data takes a df and float, but in our nodes, it becomes a list of strings. I will explain it in Section 3.2. . Pipeline . Pipeline is nothing more than a list of Node, it helps you to reuse nodes for different pipelines . Pipeline([ṅode(), [node(), ...]]) . Here is an simple Pipeline which does splitting data, train a model, make predictions, and report metrics. . def create_pipeline(**kwargs): return Pipeline( [ node( split_data, [&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), ), node( train_model, [&quot;example_train_x&quot;, &quot;example_train_y&quot;, &quot;parameters&quot;], &quot;example_model&quot;, ), node( predict, dict(model=&quot;example_model&quot;, test_x=&quot;example_test_x&quot;), &quot;example_predictions&quot;, ), node(report_accuracy, [&quot;example_predictions&quot;, &quot;example_test_y&quot;], None, name=&#39;report1&#39;), node(report_accuracy, [&quot;example_predictions&quot;, &quot;example_test_y&quot;], None, name=&#39;report2&#39;), ] ) . You can also use node tags or writing different defined pipeline to reuse your node easily. . Kedro Viz . Internally, Kedro always form a graph for your entire pipelines, which can be visaulized with this command. . kedro viz . This starts a web server that visualizes the dependencies of your function, parameters and data,you can also filter some nodes of function with the UI. . . Kedro Run, partial pipeline, parallel execution . . You can execute your pipeline partially with this command. This with execute your pipeline from A to C except the last Node D. . kedro run --from-nodes=&quot;A, B, C&quot; . If you pay attention to this graph, Node B and Node C has no dependency, they only depend on Node A. With kedro, you can parallelize this execution for free by using this command. . kedro run --parallel . Functional programming . Now, you have basic understand of what is Node and Pipeline, you also learnt that you can use kedro run command to execute your pipeline with different options. Before I jump into other kedro features, let me explain a bit more about functional programming. This concept is at the heart of data processing library like spark. . Functional programming, means using functions to program literally. It may sounds silly, but bear with me. . Pure Function has these characteristics: . 1. No side effect, it won&#39;t change state outside of the function scope. 2. If you repeating running the same function with same input(argument), it should give you the same output. 3. Easy to parallel if there is no data dependency . Consider this simple function that add 1 to your input: . def func1(x): x=x+1 def func2(x): return x+1 . var1 = 1 var2 = 1 func1(var1) # var1=2 func2(var2) # var2=2 . They both add 1 to your input, so which version is a better function? . func1(var1) # var1=3 func2(var2) # var2=2 . Now consider if we run this function twice. func1 changes the result to 3, while func2 still give you 2. I argue func2 is better in this case. . Why does this matter? Or how is it going to be useful at all? Well, it makes debugging much easier. It is because you only need to debug code inside a function, not 200 lines of code before it. This greatly reduce the complexity that you have to worried about your data. This fundamental principle is what powering the pipeline, and the reason why you can just use kedro run --parallel to parallelize some computation. . It will also be easier to write test for function. func1 is harder to test, because you need to consider all possible code path. You may end up need to write verbose test cases like this. . def test_case1(): func_A() func_B() def test_case2(): func_A() func_A() func_B() . How does using Kedro helps to achieve this? Think about func1, if it is written as a Node, it will look like this. . Node(func1, inputs=var1, output=None, name=&quot;func1&quot;) . Since it is a Node without any output, it will have no impact to the downstreams. In order to use that variable, you will naturally writing code looks more like func2 instead. . Let&#39;s look at one more example. . k = 10 def func3(x): return x+k . func3(10) . 20 . Now consider func3, it is a valid Python function. You can run it in a notebook or in a script, but it wouldn&#39;t be possible for a Node, sinec a Node only have access to its input. It will just throw an error to you immediately. . pyton node(func3, inputs=&#39;x&#39;, outputs=&#39;some_result&#39;, name=&#39;func3&#39;) . By writing nodes, you limit your function to only access variable within its scope. It helps to prevent a lot of bug. . Decompose program to pipeline is not just copy and paste . I hope the examples demonstrate how writing nodes help transform your code towards functional style. In reality, decoupling your functions from a programming is not straight forward. . Consider this example. . Look at how data np.nan is changed. This wouldn&#39;t be a problem if we have one program, since we will just passing all variable in memroy, without the step that writing and reading from a file. . Error like these are subtle and dangerous, it may not throw error, but ruining our features quality. We have better chance to catch these error in a small program, but it would be much harder to isolate the issue if we have 1000 lines of code. The sooner you integrate it into your pipeline, the easier the integration is. In fact, we can do better. We could introduce test case for validating data, I would explain more in Section 3.5. . Data Catalog &amp; Paramaeters . Data Catalog is an API for Dataset. It includes a Data Model from from raw data, feature, to reporting layer and a standard Data I/O API. It integrates with pandas, spark, SQLAlchemy and Cloud Storage. . To use Data Catalog, you would first need to define your dataset in the catalog.yml. You will have give it a name and type, denoting whether it is a SQL query or a CSV. Optionally, you can pass in any arguments that are supported from the underlying API as well. . example_iris_data: type: pandas.CSVDataSet filepath: data/01_raw/iris.csv . Connect Data Catalog with Node . Let&#39;s reuse our split_data function. When you create a node that using the split_data function, you would pass in the string of the dataset instead of an actual dataframe, the Reading/Writing operation is handled by Kedro, so you don&#39;t have to write to_csv() or read_csv() yourself. . parameters.yml . example_test_data_ratio: 0.2 . A node using the split_data function. . node(split_data, inputs=[&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], outputs= dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), name=&quot;split_data&quot;) . Here the inputs &quot;example_iris_data&quot; is refering to a dataset defined by catalog.yml, kedro will load the csv for you. Same applies for params:example_test_data_ratio. . By using catalog and parmaeters, it already makes your program cleaner. You now have a single file to manager all data source, and a single file contains all parameters, which is configurable. Your functions now is parameterized, you can simply change configuration in a single file without going into every possible script to change a number. . Data Catalog abstract away the Data I/O logic from the data processing function. . It process data and write a file. . def process_data(df): ... # do some processing df.to_csv(&#39;xxx.csv&#39;) . It only process data . def process_data(df): ... #do some processing return df . This applies the single-responsibility principle (SRP), meaning that your function is only doing one thing at a time. There are many benefits from it, for example, it makes data versioning easier. I will explain this in Section 3.3. . Memory Dataset (optional to skip) . Remember our we pass in a string to our node, and it will look for the corresponding dataset? What if we do not define it? It could be a lot of work if we need to define everything. Besides, some variable are not needed to be written out as a file, it could just stay as in memory. . In fact, kedro use MemroyDataset by default. Which means you could simply pass in a string that is not defined, the string will be use as the name of the variable. There are more useful dataset like CacheDataset, you can find more details in this link. . https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html . p.s. When using kedro pipeline, you only define the node&#39;s inputs and outputs, but you never defined the order of execution. From my experience, there are pros and cons. The benefits is, your code is less coupled, and due to this, kedro is able to execute your pipeline in parallel whenever possible to speed up your program. However, it means the order of execution is not guaranteed, this may cause unexpected effect. For example, if you are training a machine learning model, it is common to set a random seed at the beginning. Due to the randomness of execution, you may not get identical result, as the order of execution is different everytime, thus the sequence of the random number used is random too. In general this is not a big problem, but if you have a strong need to make sure you have identical output (e.g. regression test), it may cause some trouble and you need to use dummy input and output to force kedro run your pipeline in a specific order. .",
            "url": "https://noklam.github.io/blog/2020/12/04/kedro-pipeline.html",
            "relUrl": "/2020/12/04/kedro-pipeline.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Hong Kong Elevation map with rayshader (with R)",
            "content": "This blog is mainly reproducing the blog with different data https://www.tylermw.com/a-step-by-step-guide-to-making-3d-maps-with-satellite-imagery-in-r/. My impression is that R is doing so much better for graph compare to Python. (ggplot and now rayshader for 3D plots!) . Data . Two datasets was used for this images. Landset for RGB . LC08_L1TP_122044_20200218_20200225_01_T1.TIF | . SRTM 30M resolution elevation map . n21e113.hgt | n21e114.hgt | n22e113.hgt | n22e114.hgt The USGS explorer is a very nice tool to search data. | . I actually couldn&#39;t find a Landsat image cover entire hong kong (some western part is missing). Further enhancement is needed for stitching together different images. . Setup . conda with R Kernel | Jupyter Notebook | fastpages | rayshader | Use conda install even for R Packages, I spend hours to get the environment going back and forth in Windows and Linux . library(rayshader) library(sp) library(raster) library(scales) library(dplyr) . elevation1 = raster::raster(&quot;../data/rayshader/HongKong/N21E113.hgt&quot;) elevation2 = raster::raster(&quot;../data/rayshader/HongKong/N21E114.hgt&quot;) elevation3 = raster::raster(&quot;../data/rayshader/HongKong/N22E113.hgt&quot;) elevation4 = raster::raster(&quot;../data/rayshader/HongKong/N22E114.hgt&quot;) . Let&#39;s plot the elevation map. The whole image is green-ish because most of the area is ocean, so they are at sea-level. The orange color indicate a higher elevation. . hk_elevation = raster::merge(elevation1,elevation2, elevation3, elevation4) height_shade(raster_to_matrix(hk_elevation)) %&gt;% plot_map(); . . Next, we are going to process the RGB image from Landsat-8 ,The raw jpeg look like this. . . Satellite raw images requries some preprocessing, before they look like what we expected. . hk_r = raster::raster(&quot;../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B4.TIF&quot;) hk_g = raster::raster(&quot;../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B3.TIF&quot;) hk_b = raster::raster(&quot;../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B2.TIF&quot;) hk_rbg_corrected = sqrt(raster::stack(hk_r, hk_g, hk_b)) raster::plotRGB(hk_rbg_corrected); . . The image is quite hazzy, which doesn&#39;t look like the jpeg we saw earlier. We need to improve the contrast. . hk_elevation_utm = raster::projectRaster(hk_elevation, crs = crs(hk_r), method = &quot;bilinear&quot;) crs(hk_elevation_utm) bottom_left = c(y=113.888, x=22.1365) top_right = c(y=114.330, x=22.5493) extent_latlong = sp::SpatialPoints(rbind(bottom_left, top_right), proj4string=sp::CRS(&quot;+proj=longlat +ellps=WGS84 +datum=WGS84&quot;)) extent_utm = sp::spTransform(extent_latlong, raster::crs(hk_elevation_utm)) e = raster::extent(extent_utm) e . CRS arguments: +proj=utm +zone=49 +datum=WGS84 +units=m +no_defs . class : Extent xmin : 797906.6 xmax : 842523 ymin : 2450766 ymax : 2497449 . hk_rgb_cropped = raster::crop(hk_rbg_corrected, e) elevation_cropped = raster::crop(hk_elevation_utm, e) names(hk_rgb_cropped) = c(&quot;r&quot;,&quot;g&quot;,&quot;b&quot;) hk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r) hk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g) hk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b) hkel_matrix = rayshader::raster_to_matrix(elevation_cropped) hk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3)) hk_rgb_array[,,1] = hk_r_cropped/255 #Red layer hk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer hk_rgb_array[,,3] = hk_b_cropped/255 #Green layer hk_rgb_array = aperm(hk_rgb_array, c(2,1,3)) plot_map(hk_rgb_array) . . The whole image is bright because we have some dark pixels in the corner. It&#39;s similiar to taking images in a dark room, any light source will become a bright spot. . We can improve this by stretching the intensity. It&#39;s really no different than how you fine tune your images on Instagram. . hk_rgb_cropped = raster::crop(hk_rbg_corrected, e) elevation_cropped = raster::crop(hk_elevation_utm, e) # Stretch the images hk_rgb_cropped &lt;- raster::stretch(hk_rgb_cropped, minq = .01, maxq = .999, ) names(hk_rgb_cropped) = c(&quot;r&quot;,&quot;g&quot;,&quot;b&quot;) hk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r) hk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g) hk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b) hkel_matrix = rayshader::raster_to_matrix(elevation_cropped) hk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3)) hk_rgb_array[,,1] = hk_r_cropped/255 #Red layer hk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer hk_rgb_array[,,3] = hk_b_cropped/255 #Green layer hk_rgb_array = aperm(hk_rgb_array, c(2,1,3)) hk_rgb_contrast = scales::rescale(hk_rgb_array,to=c(0,1)) plot_map(hk_rgb_contrast) . . Now we get a much better image . plot_3d(hk_rgb_contrast, hkel_matrix, windowsize = c(1100,900), zscale = 15, shadowdepth = -50, zoom=0.5, phi=45,theta=-15,fov=70, background = &quot;#F2E1D0&quot;, shadowcolor = &quot;#523E2B&quot;) render_scalebar(limits=c(0, 5, 10),label_unit = &quot;km&quot;,position = &quot;W&quot;, y=50, scale_length = c(0.33,1)) render_compass(position = &quot;N&quot;) render_snapshot(title_text = &quot;Hong Kong | Imagery: Landsat 8 | DEM: 30m SRTM&quot;, title_bar_color = &quot;#000000&quot;, title_color = &quot;white&quot;, title_bar_alpha = 1, clear=TRUE, ) . .",
            "url": "https://noklam.github.io/blog/r/2020/11/14/Hong-Kong-Elevation-Map-with-rayshader.html",
            "relUrl": "/r/2020/11/14/Hong-Kong-Elevation-Map-with-rayshader.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Optimizing pandas - Reducing 90% memory footprint - updated version",
            "content": "Todo . [ ] TWO options to automatically optimize pandas | . We can check some basic info about the data with pandas .info() function . df_gamelogs.info(memory_usage=&#39;deep&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 171907 entries, 0 to 171906 Columns: 161 entries, date to acquisition_info dtypes: float64(77), int64(6), object(78) memory usage: 860.5 MB . We can see the data has 171907 rows and 161 columns and 859.4 MB memory. Let&#39;s see how much we can optimize dtype_diet. . proposed_df = report_on_dataframe(df_gamelogs, unit=&quot;MB&quot;) proposed_df . Current dtype Proposed dtype Current Memory (MB) Proposed Memory (MB) Ram Usage Improvement (MB) Ram Usage Improvement (%) . Column . date int64 | int32 | 671.574219 | 335.818359 | 335.755859 | 49.995347 | . number_of_game int64 | int8 | 671.574219 | 84.001465 | 587.572754 | 87.491857 | . day_of_week object | category | 5036.400391 | 84.362793 | 4952.037598 | 98.324939 | . v_name object | category | 5036.400391 | 174.776367 | 4861.624023 | 96.529736 | . v_league object | category | 4952.461426 | 84.359375 | 4868.102051 | 98.296617 | . ... ... | ... | ... | ... | ... | ... | . h_player_9_id object | category | 4955.471680 | 412.757324 | 4542.714355 | 91.670675 | . h_player_9_name object | category | 5225.463379 | 421.197266 | 4804.266113 | 91.939523 | . h_player_9_def_pos float64 | float16 | 671.574219 | 167.940430 | 503.633789 | 74.993020 | . additional_info object | category | 2714.671875 | 190.601074 | 2524.070801 | 92.978854 | . acquisition_info object | category | 4749.209961 | 84.070801 | 4665.139160 | 98.229794 | . 161 rows × 6 columns . new_df = optimize_dtypes(df_gamelogs, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 79.04368686676025 MB . df = pd.read_csv(&#39;../data/sell_prices.csv.zip&#39;) . proposed_df = report_on_dataframe(df, unit=&quot;MB&quot;) new_df = optimize_dtypes(df, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 85.09655094146729 MB . ## collapse-hide .",
            "url": "https://noklam.github.io/blog/python/2020/11/10/Pandas-memory-optimization.html",
            "relUrl": "/python/2020/11/10/Pandas-memory-optimization.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Recreating the BBC style graphic in Python - `plotnine` and `altair`",
            "content": "Todo . [ ] Missing Subtitle (plotnine) | [ ] Missing Style | . Difference between plotnine and ggplot . 99% of them are the same, except that in python you have to wrap column names in &#39;&#39;, otherwise it will be treated as variable and caused error. Most of the time you just need to wrap a &#39;&#39; or replaced with _ depends on the function. . I tried to produce the same chart with plotnine and altair, and hopefully you will see their difference. plotnine covers 99% of ggplot2, so if you are coming from R, just go ahead with plotnine! altair is another interesting visualization library that base on vega-lite, therefore it can be integrated with website easily. In addition, it can also produce interactive chart with very simple function, which is a big plus! . Setup . # !pip install plotnine[all] # !pip install altair # !pip install gapminder from gapminder import gapminder from plotnine.data import mtcars from plotnine import * from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_line from plotnine import ggplot # https://plotnine.readthedocs.io/en/stable/ import altair as alt import pandas as pd import plotnine %matplotlib inline . . Collecting plotnine[all] Using cached https://files.pythonhosted.org/packages/19/da/4d2f68e7436e76a3c26ccd804e1bfc5c58fca7a6cba06c71bab68b25e825/plotnine-0.6.0-py3-none-any.whl Collecting descartes&gt;=1.1.0 (from plotnine[all]) Using cached https://files.pythonhosted.org/packages/e5/b6/1ed2eb03989ae574584664985367ba70cd9cf8b32ee8cad0e8aaeac819f3/descartes-1.1.0-py3-none-any.whl Requirement already satisfied: numpy&gt;=1.16.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (1.16.5) Requirement already satisfied: matplotlib&gt;=3.1.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (3.1.1) Requirement already satisfied: statsmodels&gt;=0.9.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (0.10.1) Requirement already satisfied: pandas&gt;=0.25.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (1.0.3) Requirement already satisfied: scipy&gt;=1.2.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (1.3.1) Requirement already satisfied: patsy&gt;=0.4.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (0.5.1) Collecting mizani&gt;=0.6.0 (from plotnine[all]) Using cached https://files.pythonhosted.org/packages/e3/76/7a2c9094547ee592f9f43f651ab824aa6599af5e1456250c3f4cc162aece/mizani-0.6.0-py2.py3-none-any.whl Requirement already satisfied: scikit-learn; extra == &#34;all&#34; in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from plotnine[all]) (0.22.1) Collecting scikit-misc; extra == &#34;all&#34; (from plotnine[all]) Using cached https://files.pythonhosted.org/packages/94/4c/e6c3ba02dc66278317778b5c5df7b372c6c5313fce43615a7ce7fc0b34b8/scikit_misc-0.1.1-cp37-cp37m-win_amd64.whl Requirement already satisfied: cycler&gt;=0.10 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from matplotlib&gt;=3.1.1-&gt;plotnine[all]) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from matplotlib&gt;=3.1.1-&gt;plotnine[all]) (1.1.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from matplotlib&gt;=3.1.1-&gt;plotnine[all]) (2.4.2) Requirement already satisfied: python-dateutil&gt;=2.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from matplotlib&gt;=3.1.1-&gt;plotnine[all]) (2.8.0) Requirement already satisfied: pytz&gt;=2017.2 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas&gt;=0.25.0-&gt;plotnine[all]) (2019.3) Requirement already satisfied: six in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from patsy&gt;=0.4.1-&gt;plotnine[all]) (1.12.0) Requirement already satisfied: palettable in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from mizani&gt;=0.6.0-&gt;plotnine[all]) (3.3.0) Requirement already satisfied: joblib&gt;=0.11 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from scikit-learn; extra == &#34;all&#34;-&gt;plotnine[all]) (0.13.2) Requirement already satisfied: setuptools in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=3.1.1-&gt;plotnine[all]) (41.4.0) Installing collected packages: descartes, mizani, scikit-misc, plotnine Successfully installed descartes-1.1.0 mizani-0.6.0 plotnine-0.6.0 scikit-misc-0.1.1 Requirement already satisfied: altair in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (4.1.0) Requirement already satisfied: jinja2 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (2.10.3) Requirement already satisfied: jsonschema in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (3.0.2) Requirement already satisfied: entrypoints in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (0.3) Requirement already satisfied: numpy in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (1.16.5) Requirement already satisfied: toolz in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (0.10.0) Requirement already satisfied: pandas&gt;=0.18 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from altair) (1.0.3) Requirement already satisfied: MarkupSafe&gt;=0.23 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from jinja2-&gt;altair) (1.1.1) Requirement already satisfied: pyrsistent&gt;=0.14.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from jsonschema-&gt;altair) (0.15.4) Requirement already satisfied: setuptools in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from jsonschema-&gt;altair) (41.4.0) Requirement already satisfied: six&gt;=1.11.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from jsonschema-&gt;altair) (1.12.0) Requirement already satisfied: attrs&gt;=17.4.0 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from jsonschema-&gt;altair) (19.2.0) Requirement already satisfied: pytz&gt;=2017.2 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas&gt;=0.18-&gt;altair) (2019.3) Requirement already satisfied: python-dateutil&gt;=2.6.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas&gt;=0.18-&gt;altair) (2.8.0) Collecting gapminder Downloading https://files.pythonhosted.org/packages/85/83/57293b277ac2990ea1d3d0439183da8a3466be58174f822c69b02e584863/gapminder-0.1-py3-none-any.whl Requirement already satisfied: pandas in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from gapminder) (1.0.3) Requirement already satisfied: numpy&gt;=1.13.3 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas-&gt;gapminder) (1.16.5) Requirement already satisfied: pytz&gt;=2017.2 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas-&gt;gapminder) (2019.3) Requirement already satisfied: python-dateutil&gt;=2.6.1 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from pandas-&gt;gapminder) (2.8.0) Requirement already satisfied: six&gt;=1.5 in c: users channo.oocldm appdata local continuum anaconda3 lib site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas-&gt;gapminder) (1.12.0) Installing collected packages: gapminder Successfully installed gapminder-0.1 . print(f&#39;altair version: {alt.__version__}&#39;) print(f&#39;plotnine version: {plotnine.__version__}&#39;) print(f&#39;pandas version: {pd.__version__}&#39;) . altair version: 4.1.0 plotnine version: 0.6.0 pandas version: 1.0.3 . Plotnine Example . (ggplot(mtcars, aes(&#39;wt&#39;, &#39;mpg&#39;, color=&#39;factor(gear)&#39;)) + geom_point() + stat_smooth(method=&#39;lm&#39;) + facet_wrap(&#39;~gear&#39;)) . &lt;ggplot: (-9223371941312347920)&gt; . Make a Line Chart . ggplot . line_df &lt;- gapminder %&gt;% filter(country == &quot;Malawi&quot;) #Make plot line &lt;- ggplot(line_df, aes(x = year, y = lifeExp)) + geom_line(colour = &quot;#1380A1&quot;, size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in Malawi 1952-2007&quot;) . plotnine . (ggplot(line_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;)) + geom_line(colour=&#39;#1380A1&#39;, size=1) + geom_hline(yintercept = 0, size = 1, colour=&#39;#333333&#39;) + labs(title=&#39;Living longer&#39;, subtitle = &#39;Life expectancy in Malawi 1952-2007&#39;) ) . &lt;ggplot: (-9223371941310406772)&gt; . line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in Malawi 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;y&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;y:Q&#39;) line + hline . The BBC style . function () { font &lt;- &quot;Helvetica&quot; ggplot2::theme(plot.title = ggplot2::element_text(family = font, size = 28, face = &quot;bold&quot;, color = &quot;#222222&quot;), plot.subtitle = ggplot2::element_text(family = font, size = 22, margin = ggplot2::margin(9, 0, 9, 0)), plot.caption = ggplot2::element_blank(), legend.position = &quot;top&quot;, legend.text.align = 0, legend.background = ggplot2::element_blank(), legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), legend.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.title = ggplot2::element_blank(), axis.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)), axis.ticks = ggplot2::element_blank(), axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major.y = ggplot2::element_line(color = &quot;#cbcbcb&quot;), panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), strip.background = ggplot2::element_rect(fill = &quot;white&quot;), strip.text = ggplot2::element_text(size = 22, hjust = 0)) } &lt;environment: namespace:bbplot&gt; . legend_text_align . - NameErrorTraceback (most recent call last) &lt;ipython-input-54-6b903dfad290&gt; in &lt;module&gt; -&gt; 1 legend_text_align NameError: name &#39;legend_text_align&#39; is not defined . def bbc_style(): font = &quot;Helvetica&quot; t = theme(plot_title=element_text(family=font, size=28, face=&quot;bold&quot;, color=&quot;#222222&quot;), # plot_subtitle=element_text(family=font, # size=22, plot_margin=(9, 0, 9, 0)), plot_caption=element_blank(), legend_position=&quot;top&quot;, legend_title_align=0, legend_background=element_blank(), legend_title=element_blank(), legend_key=element_blank(), legend_text=element_text(family=font, size=18, color=&quot;#222222&quot;), axis_title=element_blank(), axis_text=element_text(family=font, size=18, color=&quot;#222222&quot;), axis_text_x=element_text(margin={&#39;t&#39;: 5, &#39;b&#39;: 10}), axis_ticks=element_blank(), axis_line=element_blank(), panel_grid_minor=element_blank(), panel_grid_major_y=element_line(color=&quot;#cbcbcb&quot;), panel_grid_major_x=element_blank(), panel_background=element_blank(), strip_background=element_rect(fill=&quot;white&quot;), strip_text=element_text(size=22, hjust=0) ) return t . font = &quot;Helvetica&quot; theme(plot_title=element_text(family=font, size=28, face=&quot;bold&quot;, color=&quot;#222222&quot;), # plot_subtitle=element_text(family=font, # size=22, plot_margin=(9, 0, 9, 0)), plot_caption=element_blank(), legend_position=&quot;top&quot;, legend_title_align=0, legend_background=element_blank(), legend_title=element_blank(), legend_key=element_blank(), legend_text=element_text(family=font, size=18, color=&quot;#222222&quot;), axis_title=element_blank(), axis_text=element_text(family=font, size=18, color=&quot;#222222&quot;), axis_text_x=element_text(margin={&#39;t&#39;: 5, &#39;b&#39;: 10}), axis_ticks=element_blank(), axis_line=element_blank(), panel_grid_minor=element_blank(), panel_grid_major_y=element_line(color=&quot;#cbcbcb&quot;), panel_grid_major_x=element_blank(), panel_background=element_blank(), strip_background=element_rect(fill=&quot;white&quot;), strip_text=element_text(size=22, hjust=0) ) . &lt;plotnine.themes.theme.theme at 0x163f0ca1508&gt; . The finalise_plot() function does more than just save out your chart, it also left-aligns the title and subtitle as is standard for BBC graphics, adds a footer with the logo on the right side and lets you input source text on the left side. . altair . line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) line + hline . Make a multiple line chart . ggplot . #Prepare data multiple_line_df &lt;- gapminder %&gt;% filter(country == &quot;China&quot; | country == &quot;United States&quot;) #Make plot multiple_line &lt;- ggplot(multiple_line_df, aes(x = year, y = lifeExp, colour = country)) + geom_line(size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + scale_colour_manual(values = c(&quot;#FAAB18&quot;, &quot;#1380A1&quot;)) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in China and the US&quot;) . plotnine . multiline = ( ggplot(multiline_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;, colour=&#39;country&#39;)) + geom_line(colour=&quot;#1380A1&quot;, size=1) + geom_hline(yintercept=0, size=1, color=&quot;#333333&quot;) + scale_colour_manual(values=[&quot;#FAAB18&quot;, &quot;#1380A1&quot;]) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle=&quot;Life expectancy in China 1952-2007&quot;)) multiline . findfont: Font family [&#39;Helvetica&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;Helvetica&#39;] not found. Falling back to DejaVu Sans. . &lt;ggplot: (-9223371941310014864)&gt; . altair . multiline_altair = (alt.Chart(multiline_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;, color=&#39;country&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) multiline_altair + hline . Make a bar chart . ggplot . #Prepare data bar_df &lt;- gapminder %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(desc(lifeExp)) %&gt;% head(5) #Make plot bars &lt;- ggplot(bar_df, aes(x = country, y = lifeExp)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle = &quot;Highest African life expectancy, 2007&quot;) . bar_df = gapminder.query(&#39; year == 2007 &amp; continent == &quot;Africa&quot; &#39;).nlargest(5, &#39;lifeExp&#39;) . plotnine . bars_ggplot = (ggplot(bar_df, aes(x=&#39;country&#39;, y=&#39;lifeExp&#39;)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + # bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle=&quot;Highest African life expectancy, 2007&quot;)) bars_ggplot . &lt;ggplot: (-9223371941310355340)&gt; . altair . bars_altair = (alt.Chart(bar_df).mark_bar().encode( x=&#39;country&#39;, y=&#39;lifeExp&#39;, # color=&#39;country&#39; ) .properties(title={&#39;text&#39;: &#39;Reunion is highest&#39;, &#39;subtitle&#39;: &#39;Highest African life expectancy, 2007&#39;}) ) bars_altair . Make a stacked bar chart . Data preprocessing . stacked_bar_df = ( gapminder.query(&#39; year == 2007&#39;) .assign( lifeExpGrouped=lambda x: pd.cut( x[&#39;lifeExp&#39;], bins=[0, 50, 65, 80, 90], labels=[&quot;under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;])) .groupby( [&#39;continent&#39;, &#39;lifeExpGrouped&#39;], as_index=True) .agg({&#39;pop&#39;: &#39;sum&#39;}) .rename(columns={&#39;pop&#39;: &#39;continentPop&#39;}) .reset_index() ) stacked_bar_df[&#39;lifeExpGrouped&#39;] = pd.Categorical(stacked_bar_df[&#39;lifeExpGrouped&#39;], ordered=True) stacked_bar_df.head(6) . continent lifeExpGrouped continentPop . 0 Africa | under 50 | 376100713.0 | . 1 Africa | 50-65 | 386811458.0 | . 2 Africa | 65-80 | 166627521.0 | . 3 Africa | 80+ | NaN | . 4 Americas | under 50 | NaN | . 5 Americas | 50-65 | 8502814.0 | . ggplot . #prepare data stacked_df &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% mutate(lifeExpGrouped = cut(lifeExp, breaks = c(0, 50, 65, 80, 90), labels = c(&quot;Under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;))) %&gt;% group_by(continent, lifeExpGrouped) %&gt;% summarise(continentPop = sum(as.numeric(pop))) #set order of stacks by changing factor levels stacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped))) #create plot stacked_bars &lt;- ggplot(data = stacked_df, aes(x = continent, y = continentPop, fill = lifeExpGrouped)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + bbc_style() + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_d(direction = -1) + geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;% of population by life expectancy band, 2007&quot;) + theme(legend.position = &quot;top&quot;, legend.justification = &quot;left&quot;) + guides(fill = guide_legend(reverse = TRUE)) . plotnine . stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . C: Users CHANNO.OOCLDM AppData Local Continuum anaconda3 lib site-packages plotnine scales scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction` warn(msg.format(self.__class__.__name__, k), PlotnineWarning) C: Users CHANNO.OOCLDM AppData Local Continuum anaconda3 lib site-packages plotnine layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values. data = self.position.setup_data(self.data, params) . &lt;ggplot: (-9223371941310320660)&gt; . stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . C: Users CHANNO.OOCLDM AppData Local Continuum anaconda3 lib site-packages plotnine scales scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction` warn(msg.format(self.__class__.__name__, k), PlotnineWarning) C: Users CHANNO.OOCLDM AppData Local Continuum anaconda3 lib site-packages plotnine layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values. data = self.position.setup_data(self.data, params) . &lt;ggplot: (-9223371941310406808)&gt; . altair . stacked_bar_altair = ( alt.Chart(stacked_bar_df) .mark_bar() .encode(x=&#39;continent&#39;, y=alt.Y(&#39;continentPop&#39;, stack=&#39;normalize&#39;, axis=alt.Axis(format=&#39;%&#39;)), fill=alt.Fill(&#39;lifeExpGrouped&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;))) .properties(title={&#39;text&#39;: &#39;How life expectancy varies&#39;, &#39;subtitle&#39;: &#39;% of population by life expectancy band, 2007&#39;} ) ) overlay = overlay = pd.DataFrame({&#39;continentPop&#39;: [0]}) hline = alt.Chart(overlay).mark_rule( color=&#39;#333333&#39;, strokeWidth=2).encode(y=&#39;continentPop:Q&#39;) stacked_bar_altair + hline . Make a grouped bar chart . ggplot . #Prepare data grouped_bar_df &lt;- gapminder %&gt;% filter(year == 1967 | year == 2007) %&gt;% select(country, year, lifeExp) %&gt;% spread(year, lifeExp) %&gt;% mutate(gap = `2007` - `1967`) %&gt;% arrange(desc(gap)) %&gt;% head(5) %&gt;% gather(key = year, value = lifeExp, -country, -gap) #Make plot grouped_bars &lt;- ggplot(grouped_bar_df, aes(x = country, y = lifeExp, fill = as.factor(year))) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_fill_manual(values = c(&quot;#1380A1&quot;, &quot;#FAAB18&quot;)) + labs(title=&quot;We&#39;re living longer&quot;, subtitle = &quot;Biggest life expectancy rise, 1967-2007&quot;) . plotnine . grouped_bars_ggplot = (ggplot(grouped_bar_df, aes(x=&#39;country&#39;, y=&#39;lifeExp&#39;, fill=&#39;year&#39;)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + # bbc_style() + scale_fill_manual(values=(&quot;#1380A1&quot;, &quot;#FAAB18&quot;)) + labs(title=&quot;We&#39;re living longer&quot;, subtitle=&quot;Biggest life expectancy rise, 1967-2007&quot;)) grouped_bars_ggplot . &lt;ggplot: (-9223371941310352504)&gt; . altair . grouped_bars_altair = ( alt.Chart(grouped_bar_df) .mark_bar() .encode(x=&#39;year:N&#39;, y=&#39;lifeExp&#39;, color=alt.Color(&#39;year:N&#39;, scale=alt.Scale(range=[&quot;#1380A1&quot;, &quot;#FAAB18&quot;])), column=&#39;country:N&#39;) .properties(title={&#39;text&#39;: &quot;We&#39;re living longe&quot;, &#39;subtitle&#39;: &#39;Biggest life expectancy rise, 1967-2007&#39;} ) ) grouped_bars_altair . Make changes to the legend . . plotnine . multiline + guides(colour=False) . &lt;ggplot: (-9223371941310211164)&gt; . multiline + theme(legend_position = &quot;none&quot;) . &lt;ggplot: (-9223371941308840576)&gt; . from plotnine import unit . - ImportErrorTraceback (most recent call last) &lt;ipython-input-43-628fddfc6297&gt; in &lt;module&gt; -&gt; 1 from plotnine import unit ImportError: cannot import name &#39;unit&#39; from &#39;plotnine&#39; (C: Users CHANNO.OOCLDM AppData Local Continuum anaconda3 lib site-packages plotnine __init__.py) . x=multiline + theme( axis_ticks_major_x = element_line(color = &quot;#333333&quot;), axis_ticks_length = 0.26) x . &lt;ggplot: (-9223371941310025320)&gt; . altair . Make changes to the axes . . ggplot . . plotnine . altair . Add annotations . . ggplot . . plotnine . altair . Work with small multiples . . ggplot . . plotnine . altair . Do something else entirely . . ggplot . . plotnine . altair . Make a dumbbell chart . ggplot . #Prepare data dumbbell_df &lt;- gapminder %&gt;% filter(year == 1967 | year == 2007) %&gt;% select(country, year, lifeExp) %&gt;% spread(year, lifeExp) %&gt;% mutate(gap = `2007` - `1967`) %&gt;% arrange(desc(gap)) %&gt;% head(10) ggplot(hist_df, aes(lifeExp)) + geom_histogram(binwidth = 5, colour = &quot;white&quot;, fill = &quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, by = 10), labels = c(&quot;40&quot;, &quot;50&quot;, &quot;60&quot;, &quot;70&quot;, &quot;80&quot;, &quot;90 years&quot;)) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;Distribution of life expectancy in 2007&quot;) . plotnine . Not available with plotnine. . altair . dumbbell_chart_altair = ( alt.Chart(dumbbell_chart_df). mark_rule() ) dumbbell_chart_altair . () .",
            "url": "https://noklam.github.io/blog/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "relUrl": "/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Beyond Unit Testing - What is Property-based Testing?",
            "content": "# https://hypothesis.readthedocs.io/en/latest/quickstart.html !pip install hypothesis %load_ext ipython_pytest . . Requirement already satisfied: hypothesis in c: programdata anaconda3 lib site-packages (5.8.1) Requirement already satisfied: sortedcontainers&lt;3.0.0,&gt;=2.1.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (2.1.0) Requirement already satisfied: attrs&gt;=19.2.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (19.2.0) The ipython_pytest extension is already loaded. To reload it, use: %reload_ext ipython_pytest . Unit Testing is a common technique for software engineering. Even if you are not writing a unit test explicitly, you are still doing unit testing, as your function should at least works for what you intended. You give an input x to a function, it should return y, simple as that. . For example, imagine we have a function like this. . def add_ints(x1, x2): return x1 + x2 . add_ints(1,1) . 2 . add_ints(1,&#39;2&#39;) . TypeError Traceback (most recent call last) &lt;ipython-input-59-99ea9d9c8984&gt; in &lt;module&gt; 1 # Case 2 -&gt; 2 add_ints(1,&#39;2&#39;) &lt;ipython-input-57-d4599be2ffda&gt; in add_ints(x1, x2) 1 def add_ints(x1, x2): -&gt; 2 return x1 + x2 TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . add_ints(&#39;2&#39;, &#39;2&#39;) . The first two cases are expected behaviors, but the last case is a side-effect of how Python works. We should probably checks the input are numbers, otherwise we should throw error explicitly. Now, checking function behave properly with intend use is easy, to test the opposite is much harder. You have to test a lot of edge case, which is much harder and make your test verbose. . In this article, I will introduce a library called Hypothesis that does property-based testing. If none of this make sense to you, please bare with me, I will explain with simple examples. I found the name of Hypothesis and property-based testing isn&#39;t adding a lot of information, but they are useful. . Hypothesis comes in handy that it generated artificial input to make your test fails. Instead of specifying an input, you specify what kind of input you want to test loosely. For example, if you expect your input is number, often you may want to test when the value is negative, positive, a floating point number, or if it exceeds certain range. This list of condition can expands quickly, and Hypothesis make this easier. . Start with a simple function . Let&#39;s stick with our simple add_ints function above. To keep it simple, let test for this 3 cases first. . Adding two number -&gt; Expect Pass | Adding number and string -&gt; Expect Fail | Adding two number -&gt; Expect Fail | %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): return x1 + x2 def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmp79u2a6x6 plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xF [100%] ================================== FAILURES =================================== ____________________________ test_add_ints_string _____________________________ [XPASS(strict)] =================== 1 failed, 1 passed, 1 xfailed in 0.12s ==================== . In pytest, you can use a mark @pytest.mark.xfail to annotate a function is expected to fail the test. We have 1 pass, 1xfailed, 1 failed. . _ipytesttmp.py .xF indicates the last test is failed. Let&#39;s try to fix it by throwing an error is input type is not a number. . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpfrh2uipy plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xx [100%] ======================== 1 passed, 2 xfailed in 0.10s ========================= . Okay, now we checks if input are integers. In reality, this if often an iterative process. You start with coming up with test cases, then every now and then, you hit some edge cases and you add that into your collections of test cases. . How can we make out test cases more robust to input? Hypothesis is exactly the tool you need. . strategy, your auto-genenerated input for unit test . strategy is your input for unit test. Instead of specify a number, or a string, you specify what kind of input you want, and Hypothesis wouuld take care the rest of it. You can even composite different strategies to form more complicated input. . But let&#39;s keep it simple, we would just use integer for this demo. . from hypothesis import strategies as st . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpmdbi_h6f plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py F [100%] ================================== FAILURES =================================== ________________________________ test_add_ints ________________________________ @given(st.integers(), st.integers()) &gt; def test_add_ints(x1, x2): _ipytesttmp.py:15: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ x1 = 0, x2 = 0 @given(st.integers(), st.integers()) def test_add_ints(x1, x2): &gt; assert add_ints(x1, x2) E assert 0 E + where 0 = add_ints(0, 0) _ipytesttmp.py:16: AssertionError Hypothesis - Falsifying example: test_add_ints( x1=0, x2=0, ) ============================== 1 failed in 0.30s ============================== . The test was simple, as should pass as long as no error was thrown. Look what Hypothesis found, it found when both x1, x2=0, the assertion will fail, because we are asserting 0 + 0 = 0, thus evaluated as False in Python. . Hence, I modified my test to not assert anything, it should just keep silent as long as no error is thrown. . @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpwicd08ny plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.25s ============================== . Yes, now our test finally pass. .",
            "url": "https://noklam.github.io/blog/python/2020/04/12/Property-based-testing-in-Python.html",
            "relUrl": "/python/2020/04/12/Property-based-testing-in-Python.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Making Powerpoint Ready Chart with matplotlib",
            "content": ". In reality, you probably don&#39;t need a title as big as this one. But using library defautls often is not the best choice. . import matplotlib from matplotlib import pyplot as plt import numpy as np matplotlib.matplotlib_fname() %load_ext autoreload %autoreload 2 . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . def make_scatter_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.scatter(x, y) ax.set_title(&#39;A Simple Scatter Plot&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() def make_line_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.plot(x, y, &#39;-&#39;) ax.set_title(&#39;A Simple Line Chart&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() . The Problem of library defaults . make_scatter_plot() make_line_plot() . Your insight is as best as your audience understand. Data Scientist spends a lot of time to drill insight from data, but not enough time to present their insight. Unfortunately, human perception is largely based on visual, a easy-to-read chart is much more likely to sell your idea with a custom matplotlib pyplot chart. There is nothing wrong with matplotlib, it is custom for a user sit in front of a monitor. When it comes to presentation, you really should make some adjustment for your audience. Luckily, it is easy to do with the following tips. . Apply matplotlib theme . with plt.style.context(&#39;ggplot&#39;): # Or plt.style.use(&#39;presentation&#39;) for global setting make_scatter_plot() make_line_plot() . Much better right? . There is nothing wrong with the chart if you are viewing it in front of your monitor. However, you may not want to put it directly into your PowerPoint. . Make PowerPoint-ready charts . Luckily, there is some easy way to prepare PowerPoint-ready charts. I created a presentation.mplstyle file as follow. . Custom presentation theme . axes.titlesize : 24 axes.labelsize : 24 axes.location: &#39;left&#39; lines.linewidth : 3 lines.markersize : 10 xtick.labelsize : 18 ytick.labelsize : 18 figure.figsize : 10, 6 figure.titlesize: 24 . with plt.style.context([&#39;presentation&#39;, &#39;ggplot&#39;]): make_scatter_plot() make_line_plot() . If you are careful enough, you will notice the font size of the title is not correct. This is because ggplot theme overwrite my theme. To make it right, you just need to switch the order so that your theme will overwrite conflict settings. . with plt.style.context([&#39;ggplot&#39;, &#39;presentation&#39;]): make_scatter_plot() make_line_plot() . I actually disable the grid in my presentation theme, which conflicts with fivethirtyeight configuration. If conflict configs exist, it resolved base on your order. See the same plot with &#39;presentation&#39;,&#39;fivethirtyeight&#39; in reverse order. . To give you a sense how this affect your presenation, I put it into a Powerpoint, see if you feel the difference. . . . Avoid Low Resolution Chart . . Note: Believe it or not, a low resolution chart looks much less conviencing. Taking screenshot with larger charts helps you to preserve the resolution. . Resolution of the chart is much better | More obvious Title &amp; Label (Try take a few step back from your monitor, see if you can read it) | . Define Once, Use Everywhere . It could be troublesome if you need to define the same file over and over in different computer/environment. You can actually use a URL. I have put my own theme in GitHub so I can always access it from anywhere. . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Bad key &#34;font.name&#34; on line 9 in https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template or from the matplotlib source distribution . Conclusion . I hope this blog helps you to prepare Powerpoint-ready charts better, happy coding! .",
            "url": "https://noklam.github.io/blog/python/2020/04/10/Presentation-Ready-Chart.html",
            "relUrl": "/python/2020/04/10/Presentation-Ready-Chart.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
            "content": "I have teamed up with a friend to participate in the Bengali Image Classification Competition. We struggled to get a high rank in the Public leaderboard throughout the competition. In the end, the result is a big surprise to everyone as the leaderboard shook a lot. . . . The final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard. . The Classification Task . This is an image classification competition. We need to predict 3 parts of Bengali characters root, consonant and vowel. It is a typical classification tasks like the MNIST dataset. . . Evaluation Metrics . The competition use macro-recall as the evaluation metric. In general, people get &gt;96% recall in training, the tops are even getting &gt;99% recall. . # collapse-hide python import numpy as np import sklearn.metrics scores = [] for component in [&#39;grapheme_root&#39;, &#39;consonant_diacritic&#39;, &#39;vowel_diacritic&#39;]: y_true_subset = solution[solution[component] == component][&#39;target&#39;].values y_pred_subset = submission[submission[component] == component][&#39;target&#39;].values scores.append(sklearn.metrics.recall_score( y_true_subset, y_pred_subset, average=&#39;macro&#39;)) final_score = np.average(scores, weights=[2,1,1]) . . Model (Bigger still better) . We start with xresnet50, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast. . Near the end of the competition, we switch to a larger model se-resnext101. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with ~0.3-0.5% recall. It is a big improvement as the recall is very high (~0.97), in other words, it reduces ~10% error solely by just using a better model, not bad! . Augmentation . There are never &quot;enough&quot; data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation. We start with rotation + scale. We also find MixUp and CutMix is very effective to boost the performance. It also gives us roughly 10% boost initially from 0.96 -&gt; 0.964 recall. . CutMix &amp; MixUp . . Mixup is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2. . CutMix shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one. . It always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically. . Logging of Experiment . I normally just log my experiment with a simple CSV and some printing message. This start to get tedious when there are more than 1 people to work. It is important to communicate the results of experiments. I explore Hydra and wandb in this competition and they are very useful. . Hydra . It is often a good idea to make your experiment configurable. We use Hydra for this purpose and it is useful to compose different configuration group. By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model. . I have written an short example for how to use Hydra. . Wandb . wandb (Weight &amp; Biases) does a few things. It provides built-in functions that automatically log all your model statistics, you can also log your custom metrics with simple functions. . Compare the configuration of different experiments to find out the model with the best performance. | Built-in function for logging model weights and gradient for debugging purpose. | Log any metrics that you want | . All of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier. . . Stochastic Weight Averaging . This is a simple yet effective technique which gives about 0.3-0.4% boost to my model. In simple words, it takes snapshots of the model weights during training and takes an average at the end. It provides a cheap way to do models ensemble while you are only training 1 model. This is important for this competition as it allows me to keep training time short enough to allow feedback within hours and reduce over-fitting.) . . Larger is better (image size) . We downsample our image size to 128x128 throughout the competition, as it makes the model train faster and we believe most technique should be transferable to larger image size. It is important to keep your feedback loop short enough (hours if not days). You want your training data as small as possible while keeping them transferable to your full dataset. Once we scale our image to full size, it takes almost 20 hours to train a single model, and we only have little chance to tune the hyper-parameters before the competition end. . Debug &amp; Checkpoint . There was a time we develop our model separately and we didn&#39;t sync our code for a while. We refactor our code during the time and it was a huge mistake. It turns out our pre-refactor code trains much better model and we introduce some unknown bug. It is almost impossible to find out as we change multiple things. It is so hard to debug a neural network and testing it thoroughly is important. Injecting a large amount of code may help you to run an experiment earlier, but you may pay much more time to debug it afterwards. . I think this is applicable even if you are working alone. . Keep your changes small. | Establish a baseline early, always do a regression test after a new feature introduced (especially after code refactoring) | Create checkpoint to rollback anytime, especially if you are not working on it every day. | . Implementation is the key of Kaggle competition (in real life too). It does not matter how great your model is, a tiny little bug could have damaged your model silently . Use auxiliary label . As mentioned earlier, this competition requires to predict the root, vowel and the consonant part. In the training data, they actually provide the grapheme too. Lots of people saying that if you train with the grapheme, it improves the model greatly and get the recall &gt;98% easily. . This is something we could not reproduce throughout the competition, we tried it in the very last minute but it does not seem to improve our model. It turns out lots of people are overfitting the data, as the testing dataset has much more unseen character. . But it is still a great remark that training with labels that is not your final desired output could still be very useful. . Weight loss . The distribution of the training dataset is very imbalance, but to get a good result, we need to predict every single class accurately (macro recall). To deal with this issue, we choose to use class weights, where a higher weight would be applied to rare samples. We don&#39;t have an ablation study for this, but it seems to help close the gap between accuracy &amp; recall and allows us to train the model slightly better. . Find a teammate! . Lastly, please go and find a teammate if you can. It is very common to start a Kaggle competition, but not so easy to finish them. I have stopped for a month during the competition due to my job. It is really hard to get back to the competition after you stopped for so long. Getting a teammate helps to motivate you and in the end, it is a great learning experience for both of us. . Pretrain Model . We also tried to use a pretrained model, as it allows shorter training and gives better performance by transfer learning (Using weights learn from a large dataset to as initial weight). It also gives our model a bit of improvement. . Finetune the model head, while keeping other layers freeze (except BatchNorm layer). | Unfreeze the model, train all the layers together. | . I also tried training the model directly with discriminating learning rate while not freezing any layer at all. It performs similarly to freezing fine-tuning , so I end up just start training the entire model from the beginning. . If the code works, don&#39;t touch it . This is probably not a good habit usually, but I suggest not to do it for a competition. We spent lots of time for debugging our code after code refactoring and end up just rolling back to an older commit and cherry-picks new features. In a competition, you don&#39;t have enough time to test everything. You do not need a nice abstract class for all your features, some refactoring to keep your function/class clean is probably needed, but do not overspend your time on it. It is even common to jump between frameworks (you may find other&#39;s Kernel useful), so it is not possible to structure your code perfectly. . If someone has create a working submission script, use it! | If someone has create a working pre-processing function, use it! | . Don&#39;t spend time on trying to optimize these code unless it is necessary, it is often not worth it in a competition context. You should focus on adding new features, trying out new model, testing with new augmentation technique instead. . Summary . This is a great learning experience and refreshes some of my outdated computer vision model knowledge. If you have never joined a competition, find a friend and get started. If you have just finished one, try writing it out and share your experience. 😉 .",
            "url": "https://noklam.github.io/blog/ml/python/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "relUrl": "/ml/python/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "date": " • Mar 21, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "The missing piece in Python tutorial - What is dispatch why you should care",
            "content": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want. . Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean. . In python, you cannot overload a normal function twice for different behavior base on the arguments. For example: . def foo(number:int ): print(&#39;it is a integer&#39;) def foo(number: float): print(&#39;it is a float&#39;) . foo(1) . it is a float . The definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument. . from functools import singledispatch @singledispatch def foo(number ): print(f&#39;{type(number)}, {number}&#39;) . foo(1) . &lt;class &#39;int&#39;&gt;, 1 . We can now register the function for different argument type. . @foo.register(int) def _(data): print(&#39;It is a integer!&#39;) @foo.register(float) def _(data): print(&#39;It is a float!&#39;) @foo.register(dict) def _(data): print(&#39;It is a dict!&#39;) . foo(1.0) foo(1) foo({&#39;1&#39;:1}) . It is a float! It is a integer! It is a dict! . How is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument. . It will fallback to the most generic function if the type of argument is not registered. . foo([1,2,3]) . &lt;class &#39;list&#39;&gt;, [1, 2, 3] . I hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that. . In next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too. . Fastai @typedispatch . Single Dispatch is great, but what if we can do multi dispatch for more than 1 argument? . from fastcore.dispatch import typedispatch, TypeDispatch . Let us first try if this work as expected . @typedispatch def add(x:int, y:int): return x+y @typedispatch def add(x:int, y:str): return x + int(y) . print(add(1,2)) print(add(1,&#39;2&#39;)) print(add(&#39;a&#39;,&#39;a&#39;)) . 3 3 a . add(1,2) . 3 . add(1,&#39;2&#39;) . 3 . But what if we added something does not define? . add(&#39;2&#39;,1) . &#39;2&#39; . &#39;2&#39;? where does it come from? Let&#39;s have a look at the definition of typedispatch and understand how it works. . ??typedispatch class DispatchReg: &quot;A global registry for `TypeDispatch` objects keyed by function name&quot; def __init__(self): self.d = defaultdict(TypeDispatch) def __call__(self, f): nm = f&#39;{f.__qualname__}&#39; self.d[nm].add(f) return self.d[nm] . In fact, typedispatch is not even a function, it&#39;s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg . type(typedispatch) . fastcore.dispatch.DispatchReg . typedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you. . def foo(): return &#39;foo&#39; a = foo def foo(): return &#39;not foo&#39; b = foo . foo() . &#39;not foo&#39; . foo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable. . a() . &#39;foo&#39; . b() . &#39;not foo&#39; . hex(id(a)), hex(id(b)) . (&#39;0x2b9d28bb5e8&#39;, &#39;0x2b9d2ebe048&#39;) . The two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition. . typedispatch.d . defaultdict(fastcore.dispatch.TypeDispatch, {&#39;cast&#39;: (object,object) -&gt; cast, &#39;add&#39;: (int,str) -&gt; add (int,int) -&gt; add}) . So back to our question, why does add(&#39;a&#39;,1) return &#39;a&#39;? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument. . def __call__(self, *args, **kwargs): ts = L(args).map(type)[:2] f = self[tuple(ts)] if not f: return args[0] if self.inst is not None: f = MethodType(f, self.inst) return f(*args, **kwargs) .",
            "url": "https://noklam.github.io/blog/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "relUrl": "/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 | The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 | First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 | I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 | Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 | Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![123](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://noklam.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "data augmentation - Understand MixUp and Beta Distribution",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta . Understand Mixup Augmentation &amp; Beta Distribution . Implementation In the original article, the authors suggested three things: . Create two separate dataloaders and draw a batch from each at every iteration to mix them up | Draw a t value following a beta distribution with a parameter alpha (0.4 is suggested in their article) | Mix up the two batches with the same value t. | Use one-hot encoded targets | Source: https://forums.fast.ai/t/mixup-data-augmentation/22764 (Sylvain Gugger) . Beta Distribution . Beta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat . To get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect . import math import torch import matplotlib.pyplot as plt from torch import tensor . # PyTorch has a log-gamma but not a gamma, so we&#39;ll create one Γ = lambda x: x.lgamma().exp() facts = [math.factorial(i) for i in range(7)] plt.plot(range(7), facts, &#39;ro&#39;) plt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1)) plt.legend([&#39;factorial&#39;,&#39;Γ&#39;]); . . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [5.0,1.0,0.4, 1.0] b_ls = [1.0,5.0,0.4, 1.0] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α != β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . A few observations from this graph. . α and β control the curve symmetrically, the blue line is symmetric with the orange line. | when α and β = 1, it reduce to uniform distribution | when α = β, the distribution is a symmetric distribution | . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [0.1, 0.4, 0.6, 0.9] b_ls = [0.1, 0.4, 0.6, 0.9] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α = β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . As we remember, when α = β =1, it is an uniform distribution. When α = β , when α is small, most density is concentrated around 0 and 1, and when α increase, the distribution get more evenly distributed. . The default for α suggested by the paper is 0.4 .",
            "url": "https://noklam.github.io/blog/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "relUrl": "/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Hydra - Config Composition for Machine Learning Project",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example . Machine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations. . Assume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure. . Folder Structure . config.yaml demo.py run_mode - train.yaml - test.yaml hyperparmeter - base.yaml . config.yaml . defaults: - run_mode: train - hyperparameter: base . The benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance. . import hydra from omegaconf import DictConfig @hydra.main(config_path=&quot;config.yaml&quot;) def my_app(cfg : DictConfig) -&gt; None: print(cfg.pretty()) if __name__ == &quot;__main__&quot;: my_app() . python demo.py . gamma: 0.01 learning_rate: 0.01 run_mode: train week: 8 . For example, with a simple example with 4 parameters only, you can simply run the experiment with default . Override default parameters . You can easily overrite the learning rate with an argument, it would be very clear that learning rate is the only changing parameter with this approach . python demo.py learning_rate=0.1 . gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 . In somecase, you may only need to test a model instead of changing it. . python demo.py learning_rate=0.1 run_mode=test . gamma: 0.01 learning_rate: 0.1 run_mode: test week: 8 . It also safeguard your experiment if you pass in some parameters that is not exist . !python demo.py typo=0.2 . Traceback (most recent call last): File &quot;demo.py&quot;, line 7, in &lt;module&gt; my_app() &quot;C: ProgramData Anaconda3 lib site-packages omegaconf dictconfig.py&quot;, line 41, in __setitem__ &quot;Accessing unknown key in a struct : {}&quot;.format(self.get_full_key(key)) KeyError: &#39;Accessing unknown key in a struct : typo&#39; . –Multirun, Combination of parameters . In case you want to gridsearch paramters, which is very common in machine learning, you can use an additional argument multirun to do that easily. . !python demo.py --multirun learning_rate=0.1,0.01,0.001 gamma=0.1,0.01 . [2020-02-08 19:28:46,095][HYDRA] Sweep output dir : multirun/2020-02-08/19-28-46 [2020-02-08 19:28:46,102][HYDRA] Launching 6 jobs locally [2020-02-08 19:28:46,103][HYDRA] #0 : learning_rate=0.1 gamma=0.1 gamma: 0.1 learning_rate: 0.1 run_mode: train week: 8 [2020-02-08 19:28:46,192][HYDRA] #1 : learning_rate=0.1 gamma=0.01 gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 ... SKIPPED .",
            "url": "https://noklam.github.io/blog/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "relUrl": "/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://noklam.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "plyer - Desktop Notification with Python",
            "content": "from plyer import notification import random class DesktopNotification: @staticmethod def notify(title=&#39;Hey~&#39;, message=&#39;Done!&#39;, timeout=10): ls = [&#39;👍&#39;,&#39;✔&#39;,&#39;✌&#39;,&#39;👌&#39;,&#39;👍&#39;,&#39;😎&#39;] notification.notify( title = title , message = random.choice(ls) * 3 + &#39; &#39; + message, timeout = timeout # seconds ) if __name__ == &#39;__main__&#39;: DesktopNotification.notify() . You could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window. .",
            "url": "https://noklam.github.io/blog/python/2019/10/19/Deskto-Notification.html",
            "relUrl": "/python/2019/10/19/Deskto-Notification.html",
            "date": " • Oct 19, 2019"
        }
        
    
  
    
        ,"post37": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://noklam.github.io/blog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Jan 1, 2019"
        }
        
    
  

  
  
      ,"page0": {
          "title": "",
          "content": "1792. Maximum Average Pass Ratio . Complexity O(N * LogN) + O(K * LogN) . We need to sort the list once | Then for each update, we need to insert the tuple in an ordered list (Log N) * k | Learning point . A heap would simplify the implementation a lot, as I am basically implementing heap here. | list.pop(0) is O(n) while deque.popleft() is O(1) | User HEAP! | . from typing import List from bisect import insort def average(classes): ratios = [] for i in classes: ratios.append(i[0] / i[1]) return sum(ratios) / len(ratios) def delta(x, y): return -(y - x) / (y ** 2 + y) class Solution: def maxAverageRatio(self, classes: List[List[int]], extraStudents: int) -&gt; float: order = [] for x in classes: order.append(delta(x[0], x[1])) d = [] for i, v in enumerate(order): t = tuple([v,i]) d.append(t) order = sorted(d) while extraStudents &gt; 0 and order: selected_tuple = order.pop(0) value,index = selected_tuple c = classes[index] pass_, total_ = c if pass_ == total_: continue c[0] = pass_ + 1 c[1] = total_ + 1 extraStudents = extraStudents - 1 new_value = delta(c[0], c[1]) new_tuple = (new_value, index, ) insort(order, new_tuple) if extraStudents &lt;= 0: break return average(classes) . Reference: . https://leetcode.com/problems/maximum-average-pass-ratio/discuss/1108686/Python-Time-exceed .",
          "url": "https://noklam.github.io/blog/leetcode/1792.-maximum-average-pass-ratio.html",
          "relUrl": "/leetcode/1792.-maximum-average-pass-ratio.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "",
          "content": "README . Introduction . . https://noklam.ml . All things data . I am a data scientist. Recently, I find myself studying database, data structure, data pipeline way more than machine learning. To build a good model, I found the importance of writing good code to produce data with quality often triumphs a SOTA model. . Delivering the model is the job of a data scientist. Inevitably, every data scientist should somewhat be a “full-stack” data scientist. . This is a central repository for my blogs and notes . Blog: https://noklam.ml (Github Page) - Usually blog or notes with code with shorter articles | Blog: Medium (https://medium.com/@nokknocknok) | GitBook (Study notes mainly, I use Joplin to keep notes in markdown, am considering sync to Gitbook from time to time. I haven’t figured out what’s the best way to do so.) | . Resource . I am generally interested in tools that increase productivity, please let me know if you have any recommendations. Here is a list of software/topics that I found useful. . Uncertainty Estimation . Uncertainty Quantification in Deep Learning . Visualization . Visualization (University of Washington) . Custom Matplotlib style for Presentation (Larger font size) . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Useful Python Tools . pyinstructment: for profiling python process, which is useful for optimization | torchsnooper -&gt; pytorch profiling, another profiling tool which is for PyTorch, no more print x.shape anymore. | knockknock notification: A single line of code that get you notifications when your 10 hours model training finally done. No more starring at the progress bar. | colorama: Colored printing in terminal (cross platform) | Hypoehsis - Property-based testing, autogenerated input for unit-test. . Reviewing (any suggestions for code metric report/analysis library are welcome!) . | coala - coala provides a unified command-line interface for linting and fixing all your code, regardless of the programming languages you use. | radon - Radon is a Python tool that computes various metrics from the source code | great_expectations - A data validation library for python integrated with Pandas/Spark/SQL | . Syntax Highlight . lunr.js | . A catalog of various machine learning topics. . Graph Neural Network Basics Understand What is the weird D-1/2LD-1/2 | Supplement Chinese Reading | . | Time Series Forecast Motivation | Forecasting Methods Statistical Method | Machine Learning | Deep Neural Network | . | . | Prediction Interval Python Time Series Forecasting Library | . | Contribution | Under Review | . Graph Neural Network Basics . Understand What is the weird D-1/2LD-1/2 . spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | What’s the intuition behind a Laplacian matrix? I’m not so much interested in mathematical details or technical applications. I’m trying to grasp what a laplacian matrix actually represents, and what aspects of a graph it makes accessible. - Quora | Supplement Chinese Reading . Heat Diffusion | GCN use edge to agg node information | How to do batch training with GCN | Time Series Forecast . Motivation . While neural network has gain a lot of success in NLP and computer vision, there are relatively less changes for traditional time series forecasting. This repository aims to study the lastest practical technique for time series prediction, with either statistical method, machine learning, or deep neural network. . Forecasting Methods . Statistical Method . Machine Learning . Deep Neural Network . Gramian Angular Field : Transform time series into an image and use transfer learning with CNN . Prediction Interval . While forecasting accuracy is important, the prediction interval is also important and it is an area that the machine learning world has less focus on. . Traditional statistical forecast (ARIMA, ETS etc) | Bayesian Neural Network | Random Forest jackknife approximation | MCDropout (Use Dropout at inference time as variation inference) | Quantile Regression | VOGN (Optimizer weight perturbation) | Random Forest jackknife approximation | . Python Time Series Forecasting Library . Prophet (Facebook): Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth. It has build-in modeling for the Holiday effect. . pyts : state-of-the-art algorithms for time-series transformation and classification . Contribution . Feel free to send a PR or discuss by starting an issue.😁 . powered by fastpages . fastpages allow me to blog directly in Notebook, so I don’t have to worry how to convert into markdown anymore. I simple code and write. .",
          "url": "https://noklam.github.io/blog/README.html",
          "relUrl": "/README.html",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About Me",
          "content": "Qualities are often lost by an accident, but are never accidentally acquired. A Data Scientist located in Hong Kong, solving data scinece problem with code. . What I thought I should be doing: Create fancy machine learning models What I am actually doing: Messing with Excel and CSV, occassionally work with model. . Contact: mediumnok@gmail.com .",
          "url": "https://noklam.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "Explainable AI . Introduction . Personal collecions of model interpretation utilities. . This repository include general model interpretation methods. Most articles focus on library, but the method are actually general than that. For example, we can use partial indepedence for deep learning model too. You will find most tutorials out there are only using it on Tree. . Same applied on SHAP, many tutorials use Tree as example, but the library actually support much more general algorithm. Your feature don’t even have to be a column. . Partial Depedence Plot | SHAP | Tensorflow (What-if tools)[https://pair-code.github.io/what-if-tool/age.html] | Counterfactual (Most simliar data point, but very different inference value) | Model interpretation . Model interpretation can be divided into local or global. Different method are complementary instead of replacement. For example, SHAP give you an idea what features are important for a particular prediction. But Partial Dependence plot supplement the “What-if” condition, namely, how will your prediction changes when a dependent variable changes. These are two different information which are often overlooked. . Partial Dependence . Local | Global | . Partial depedence can be applied in row level or dataset level. It gives you a sense that how Change of a feature will change the model output accordingly. . We can also “zoom in” if we want, say using a subset of data (e.g. at country level) or even at row level to dig into the model. . The what-if tool allows you to change the value of a feature and run inference. Partial dependence is doing the exact same thing except it run multiple prediction by changing one features to different values to obtain a continuous plot. . . SHAP . Squashing function (e.g. log transformation of Target variable) | . It can affect the “feature importance” as it will change the order of feature importance even with a monotonic transformation .",
          "url": "https://noklam.github.io/blog/explainable-ai.html",
          "relUrl": "/explainable-ai.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://noklam.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}