{
  
    
        "post0": {
            "title": "deepcopy, LGBM and pickle",
            "content": "To start with, let&#39;s look at some code to get some context. . deepcopy or no copy? . import pandas as pd import numpy as np import lightgbm as lgb from copy import deepcopy params = { &#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3 } X = np.random.rand(100,2) Y = np.ravel(np.random.rand(100,1)) lgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1) print(&quot;Parameters of the model: &quot;, lgbm.params) . Parameters of the model: {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None} . new_model = deepcopy(lgbm) . Finished loading model, total used 1 iterations . You would expect new_model.parameters return the same dict right? Not quite. . print(&quot;Parameters of the copied model: &quot;, new_model.params) . Parameters of the copied model: {} . Surprise, surprise. It&#39;s an empty dict, where did the parameters go? To dive deep into the issue, let&#39;s have a look at the source code of deepcopy to understand how does it work. . reference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128 . def deepcopy(x, memo=None, _nil=[]): &quot;&quot;&quot;Deep copy operation on arbitrary Python objects. See the module&#39;s __doc__ string for more info. &quot;&quot;&quot; ... # skip some irrelevant code cls = type(x) copier = _deepcopy_dispatch.get(cls) if copier is not None: y = copier(x, memo) else: if issubclass(cls, type): y = _deepcopy_atomic(x, memo) else: copier = getattr(x, &quot;__deepcopy__&quot;, None) if copier is not None: y = copier(memo) else: ... # skip irrelevant code # If is its own copy, don&#39;t memoize. if y is not x: memo[d] = y _keep_alive(x, memo) # Make sure x lives at least as long as d return y . In particular, line 17 is what we care. copier = getattr(x, &quot;__deepcopy__&quot;, None) . If a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly. . class DummyClass(): def __deepcopy__(self, _): print(&#39;Just hanging around and not copying.&#39;) . o = DummyClass() deepcopy(o) . Just hanging around and not copying. . a lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary. . def __deepcopy__(self, _): model_str = self.model_to_string(num_iteration=-1) booster = Booster(model_str=model_str) return booster . Reference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282 . Okay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub. . https://github.com/microsoft/LightGBM/issues/4085 Their response is . Custom deepcopy is needed to make Booster class picklable. . &#129366;Italian BMT, &#129388;Lettuce &#127813; tomato and some &#129362;pickles please . What does pickle really is? and what makes an object pickable? . Python Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk. . Serialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It&#39;s like ordering a chair from Ikea, they will send you a box, but not a chair. . The process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling. . . What is Pickle . Pickle is a protocol for Python, you and either pickling a Python object to memory or to file. . import pickle . d = {&#39;a&#39;: 1} pickle_d = pickle.dumps(d) pickle_d . b&#39; x80 x04 x95 n x00 x00 x00 x00 x00 x00 x00} x94 x8c x01a x94K x01s.&#39; . The python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict. . binary_str = b&#39; x80 x04 x95 n x00 x00 x00 x00 x00 x00 x00} x94 x8c x01a x94K x01s.&#39; pickle.loads(binary_str) . {&#39;a&#39;: 1} . Reference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk. . What makes something picklable . Finally, we come back to our initial questions. . What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable? . What can be pickled and unpickled? The following types can be pickled: None, True, and False integers, floating point numbers, complex numbers . * strings, bytes, bytearrays * tuples, lists, sets, and dictionaries containing only picklable objects * functions defined at the top level of a module (using def, not lambda) * built-in functions defined at the top level of a module * classes that are defined at the top level of a module . So pretty much common datatype, functions and classes are picklable. Let&#39;s see without __deepcopy__, the Booster class is not serializable as it claims. . import lightgbm from lightgbm import Booster del Booster.__deepcopy__ params = { &#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3 } X = np.random.rand(100,2) Y = np.ravel(np.random.rand(100,1)) lgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1) deepcopy_lgbm = deepcopy(lgbm) lgbm.params, deepcopy_lgbm.params . ({&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}, {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}) . pickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm) . True . unpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm)) unpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm)) . unpickle_model.params, unpickle_deepcopy_model.params . ({&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}, {&#39;objective&#39;: &#39;regression&#39;, &#39;verbose&#39;: -1, &#39;num_leaves&#39;: 3, &#39;num_iterations&#39;: 1, &#39;early_stopping_round&#39;: None}) . unpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string() . True . unpickle_deepcopy_model.predict(X) . array([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803, 0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787, 0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787, 0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787, 0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787]) . Last Word . Well.... It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary. . I tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298 .",
            "url": "https://noklam.ml/python/pickle/deepcopy/2021/03/19/deepcopy-lightgbm-and-Pickles.html",
            "relUrl": "/python/pickle/deepcopy/2021/03/19/deepcopy-lightgbm-and-Pickles.html",
            "date": " • Mar 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
            "content": "Introduction . Long story short, connect with Impala is a big headache in Windows. pyhive, impyla are both buggy. At the end, I stick with pyodbc as it works on both Linux and Windows, and seems to have better performance. There are not many steps, but it would be tricky if you try to Google as there are not much guide that just work out of the box . Setup . First, you need to download the ODBC driver from Cloudera. . Then you need to instsall the driver properly. . dpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb . Add this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file. . # /etc/odbcinst.ini [ODBC Drivers] Cloudera Impala ODBC Driver 32-bit=Installed Cloudera Impala ODBC Driver 64-bit=Installed [Cloudera Impala ODBC Driver 32-bit] Description=Cloudera Impala ODBC Driver (32-bit) Driver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so [Cloudera Impala ODBC Driver 64-bit] Description=Cloudera Impala ODBC Driver (64-bit) Driver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so . Then install some additional package. . apt-get update &amp;&amp; apt-get -y install gnupg apt-transport-https apt-get update &amp;&amp; apt-get -y install libssl1.0.0 unixodbc unixodbc-dev &amp;&amp; ACCEPT_EULA=Y apt-get -y install msodbcsql17 apt-get install unixodbc-dev -y . Last, pip install pyodbc and have fun. . To read a database table, you can simply do this. . import pyodbc import pandas as pd conn = pyodbc.connect(f&quot;&quot;&quot; Driver=Cloudera ODBC Driver for Impala 64-bit; PWD=password; UID=username; Database=database &quot;&quot;&quot;) . There are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup. .",
            "url": "https://noklam.ml/pyodbc/impala/2021/03/05/pyodbc-linux.html",
            "relUrl": "/pyodbc/impala/2021/03/05/pyodbc-linux.html",
            "date": " • Mar 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "What is Web Server Really?",
            "content": "",
            "url": "https://noklam.ml/python/web/wsgi/2021/03/02/python-web-server-wsgi.html",
            "relUrl": "/python/web/wsgi/2021/03/02/python-web-server-wsgi.html",
            "date": " • Mar 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The missing piece in Python tutorial - What is dispatch why you should care",
            "content": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want. . Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean. . In python, you cannot overload a normal function twice for different behavior base on the arguments. For example: . def foo(number:int ): print(&#39;it is a integer&#39;) def foo(number: float): print(&#39;it is a float&#39;) . foo(1) . it is a float . The definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument. . from functools import singledispatch @singledispatch def foo(number ): print(f&#39;{type(number)}, {number}&#39;) . foo(1) . &lt;class &#39;int&#39;&gt;, 1 . We can now register the function for different argument type. . @foo.register(int) def _(data): print(&#39;It is a integer!&#39;) @foo.register(float) def _(data): print(&#39;It is a float!&#39;) @foo.register(dict) def _(data): print(&#39;It is a dict!&#39;) . foo(1.0) foo(1) foo({&#39;1&#39;:1}) . It is a float! It is a integer! It is a dict! . How is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument. . It will fallback to the most generic function if the type of argument is not registered. . foo([1,2,3]) . &lt;class &#39;list&#39;&gt;, [1, 2, 3] . I hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that. . In next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too. . Fastai @typedispatch . Single Dispatch is great, but what if we can do multi dispatch for more than 1 argument? . from fastcore.dispatch import typedispatch, TypeDispatch . Let us first try if this work as expected . @typedispatch def add(x:int, y:int): return x+y @typedispatch def add(x:int, y:str): return x + int(y) . print(add(1,2)) print(add(1,&#39;2&#39;)) print(add(&#39;a&#39;,&#39;a&#39;)) . 3 3 a . add(1,2) . 3 . add(1,&#39;2&#39;) . 3 . But what if we added something does not define? . add(&#39;2&#39;,1) . &#39;2&#39; . &#39;2&#39;? where does it come from? Let&#39;s have a look at the definition of typedispatch and understand how it works. . ??typedispatch class DispatchReg: &quot;A global registry for `TypeDispatch` objects keyed by function name&quot; def __init__(self): self.d = defaultdict(TypeDispatch) def __call__(self, f): nm = f&#39;{f.__qualname__}&#39; self.d[nm].add(f) return self.d[nm] . In fact, typedispatch is not even a function, it&#39;s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg . type(typedispatch) . fastcore.dispatch.DispatchReg . typedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you. . def foo(): return &#39;foo&#39; a = foo def foo(): return &#39;not foo&#39; b = foo . foo() . &#39;not foo&#39; . foo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable. . a() . &#39;foo&#39; . b() . &#39;not foo&#39; . hex(id(a)), hex(id(b)) . (&#39;0x2b9d28bb5e8&#39;, &#39;0x2b9d2ebe048&#39;) . The two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition. . typedispatch.d . defaultdict(fastcore.dispatch.TypeDispatch, {&#39;cast&#39;: (object,object) -&gt; cast, &#39;add&#39;: (int,str) -&gt; add (int,int) -&gt; add}) . So back to our question, why does add(&#39;a&#39;,1) return &#39;a&#39;? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument. . def __call__(self, *args, **kwargs): ts = L(args).map(type)[:2] f = self[tuple(ts)] if not f: return args[0] if self.inst is not None: f = MethodType(f, self.inst) return f(*args, **kwargs) .",
            "url": "https://noklam.ml/python/fastai/2021/02/06/leetcode.html",
            "relUrl": "/python/fastai/2021/02/06/leetcode.html",
            "date": " • Feb 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "The missing piece in Python tutorial - What is dispatch why you should care",
            "content": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want. . Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean. . In python, you cannot overload a normal function twice for different behavior base on the arguments. For example: . def foo(number:int ): print(&#39;it is a integer&#39;) def foo(number: float): print(&#39;it is a float&#39;) . foo(1) . it is a float . The definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument. . from functools import singledispatch @singledispatch def foo(number ): print(f&#39;{type(number)}, {number}&#39;) . foo(1) . &lt;class &#39;int&#39;&gt;, 1 . We can now register the function for different argument type. . @foo.register(int) def _(data): print(&#39;It is a integer!&#39;) @foo.register(float) def _(data): print(&#39;It is a float!&#39;) @foo.register(dict) def _(data): print(&#39;It is a dict!&#39;) . foo(1.0) foo(1) foo({&#39;1&#39;:1}) . It is a float! It is a integer! It is a dict! . How is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument. . It will fallback to the most generic function if the type of argument is not registered. . foo([1,2,3]) . &lt;class &#39;list&#39;&gt;, [1, 2, 3] . I hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that. . In next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too. . Fastai @typedispatch . Single Dispatch is great, but what if we can do multi dispatch for more than 1 argument? . from fastcore.dispatch import typedispatch, TypeDispatch . Let us first try if this work as expected . @typedispatch def add(x:int, y:int): return x+y @typedispatch def add(x:int, y:str): return x + int(y) . print(add(1,2)) print(add(1,&#39;2&#39;)) print(add(&#39;a&#39;,&#39;a&#39;)) . 3 3 a . add(1,2) . 3 . add(1,&#39;2&#39;) . 3 . But what if we added something does not define? . add(&#39;2&#39;,1) . &#39;2&#39; . &#39;2&#39;? where does it come from? Let&#39;s have a look at the definition of typedispatch and understand how it works. . ??typedispatch class DispatchReg: &quot;A global registry for `TypeDispatch` objects keyed by function name&quot; def __init__(self): self.d = defaultdict(TypeDispatch) def __call__(self, f): nm = f&#39;{f.__qualname__}&#39; self.d[nm].add(f) return self.d[nm] . In fact, typedispatch is not even a function, it&#39;s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg . type(typedispatch) . fastcore.dispatch.DispatchReg . typedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you. . def foo(): return &#39;foo&#39; a = foo def foo(): return &#39;not foo&#39; b = foo . foo() . &#39;not foo&#39; . foo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable. . a() . &#39;foo&#39; . b() . &#39;not foo&#39; . hex(id(a)), hex(id(b)) . (&#39;0x2b9d28bb5e8&#39;, &#39;0x2b9d2ebe048&#39;) . The two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition. . typedispatch.d . defaultdict(fastcore.dispatch.TypeDispatch, {&#39;cast&#39;: (object,object) -&gt; cast, &#39;add&#39;: (int,str) -&gt; add (int,int) -&gt; add}) . So back to our question, why does add(&#39;a&#39;,1) return &#39;a&#39;? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument. . def __call__(self, *args, **kwargs): ts = L(args).map(type)[:2] f = self[tuple(ts)] if not f: return args[0] if self.inst is not None: f = MethodType(f, self.inst) return f(*args, **kwargs) .",
            "url": "https://noklam.ml/python/fastai/2021/02/06/leetcode-Copy1.html",
            "relUrl": "/python/fastai/2021/02/06/leetcode-Copy1.html",
            "date": " • Feb 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Create python command line in few lines, and use it anywhere as a standalone tool!",
            "content": "The Typer documentation has great example explaining how to use it. This is the example copied from their GitHub homepage. https://github.com/tiangolo/typer. . %%writefile main1.py import typer def main(name: str): typer.echo(f&quot;Hello {name}&quot;) if __name__ == &quot;__main__&quot;: typer.run(main) . Overwriting main1.py . !python main1.py world . Hello world . !python main1.py --help . Usage: main1.py [OPTIONS] NAME Arguments: NAME [required] Options: --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. . Here I write a new file to main1.py and execute it as a command line with just 5 lines of code. It always comes with a help message for free. Let&#39;s see another example. . %%writefile main2.py import typer def main(name: str, age: int = 20, height_meters: float = 1.89, female: bool = True): typer.echo(f&quot;NAME is {name}, of type: {type(name)}&quot;) typer.echo(f&quot;--age is {age}, of type: {type(age)}&quot;) typer.echo(f&quot;--height-meters is {height_meters}, of type: {type(height_meters)}&quot;) typer.echo(f&quot;--female is {female}, of type: {type(female)}&quot;) if __name__ == &quot;__main__&quot;: typer.run(main) . Writing main2.py . !python main2.py --help . Usage: main2.py [OPTIONS] NAME Arguments: NAME [required] Options: --age INTEGER [default: 20] --height-meters FLOAT [default: 1.89] --female / --no-female [default: True] --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the installation. --help Show this message and exit. . This time, we can see that the help message even supplement the expected datatype. Typer will validate the type and conevrt it when possible. . !python main2.py Nok --age=3 . NAME is Nok, of type: &lt;class &#39;str&#39;&gt; --age is 3, of type: &lt;class &#39;int&#39;&gt; --height-meters is 1.89, of type: &lt;class &#39;float&#39;&gt; --female is True, of type: &lt;class &#39;bool&#39;&gt; . The command line works file, but it only works in the same directory, and you always have to type the keyword python. With python setuptools, we can actually installed a command line and run it anywhere. It is pretty easy with just 1 trick, let&#39;s go back to the simple Hello command. . %%writefile main3.py import typer def hello(name:str): typer.echo(f&quot;Hello {name}&quot;) def main(): typer.run(hello) . Overwriting main3.py . Here we made a few changes. . The logic is move to a new function named hello | We removed the __main__ part, as we will not call this python file directly anymore. | typer.run(main) is changed to typer.run(hello) and moved inside the main function. | Console Script . We will use setuptool to build console script, which may call the function main. The magic is using console script to install a command line interface (It creates a .exe file) that can be run anywhere. We can name our command line instead of using the filename with a pattern of command_name=file:func_name. Here our function main is inside a file main3.py, so we use hello=main3:main. . %%writefile setup.py from setuptools import setup, find_packages setup( name=&quot;my_library&quot;, version=&quot;1.0&quot;, packages=find_packages(), entry_points = { &#39;console_scripts&#39;: [&#39;hello=main3:main&#39;]} ) . Overwriting setup.py . Then we install the console script . . !python setup.py develop . !hello world . Hello world . We can now call hello anywhere, as it is installed as a executable. . !where hello . C: ProgramData Miniconda3 Scripts hello.exe . It&#39;s time for you to build your own commands. This can be easily extended to support multiple commands. https://github.com/tiangolo/typer .",
            "url": "https://noklam.ml/python/cli/typer/2020/12/10/Typer-create-command-line-and-use-it-anywhere.html",
            "relUrl": "/python/cli/typer/2020/12/10/Typer-create-command-line-and-use-it-anywhere.html",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction to Kedro - pipeline for data science",
            "content": "Why we need a pipeline tool . Data Scientist often starts their development with a Jupyter Notebook. As the notebook grows larger, it&#39;s inevitable to convert it to a python script. It starts with one file, then another one, and it accumulates quickly. Converting a notebook could be more than just pasting the code in a script. It involves careful thinking and refactoring. . A pipeline library can be helpful in a few ways: . modular pipeline, it can be executed partially. | easily run in parallel | check for loop dependecies | . What is Kedro . Kedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to make your data science code reproducible, modular and well-documented. For example, you can easily create a template for new projects, build a documentation site, lint your code and always have an expected structure to find your config and data. . Kedro is a lightweight pipeline library without need to setup infracstructure. . In comparison to Airflow or Luigi, Kedro is much more lightweighted. It helps you to write production-ready code, and let data engineer and data scientist work together with the same code base. It also has good Jupyter support, so data scientist can still use the tool that they are familiar with. . Functions and Pipeline . Nodes . def split_data(data: pd.DataFrame, example_test_data_ratio: float): ... return dict( train_x=train_data_x, train_y=train_data_y, test_x=test_data_x, test_y=test_data_y, ) . Node is the core component of kedro Pipeline. For example, we have a python function that split data into train/test set. A node take 4 arguments. func, inputs, outputs, name. To use this function as a node, we would write something like this. . node(split_data, inputs=[&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], outputs= dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), name=&quot;split_data&quot;) . It&#39;s fairly simple, and it resemble the original function. The only significant difference is, split_data takes a df and float, but in our nodes, it becomes a list of strings. I will explain it in Section 3.2. . Pipeline . Pipeline is nothing more than a list of Node, it helps you to reuse nodes for different pipelines . Pipeline([ṅode(), [node(), ...]]) . Here is an simple Pipeline which does splitting data, train a model, make predictions, and report metrics. . def create_pipeline(**kwargs): return Pipeline( [ node( split_data, [&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), ), node( train_model, [&quot;example_train_x&quot;, &quot;example_train_y&quot;, &quot;parameters&quot;], &quot;example_model&quot;, ), node( predict, dict(model=&quot;example_model&quot;, test_x=&quot;example_test_x&quot;), &quot;example_predictions&quot;, ), node(report_accuracy, [&quot;example_predictions&quot;, &quot;example_test_y&quot;], None, name=&#39;report1&#39;), node(report_accuracy, [&quot;example_predictions&quot;, &quot;example_test_y&quot;], None, name=&#39;report2&#39;), ] ) . You can also use node tags or writing different defined pipeline to reuse your node easily. . Kedro Viz . Internally, Kedro always form a graph for your entire pipelines, which can be visaulized with this command. . kedro viz . This starts a web server that visualizes the dependencies of your function, parameters and data,you can also filter some nodes of function with the UI. . . Kedro Run, partial pipeline, parallel execution . . You can execute your pipeline partially with this command. This with execute your pipeline from A to C except the last Node D. . kedro run --from-nodes=&quot;A, B, C&quot; . If you pay attention to this graph, Node B and Node C has no dependency, they only depend on Node A. With kedro, you can parallelize this execution for free by using this command. . kedro run --parallel . Functional programming . Now, you have basic understand of what is Node and Pipeline, you also learnt that you can use kedro run command to execute your pipeline with different options. Before I jump into other kedro features, let me explain a bit more about functional programming. This concept is at the heart of data processing library like spark. . Functional programming, means using functions to program literally. It may sounds silly, but bear with me. . Pure Function has these characteristics: . 1. No side effect, it won&#39;t change state outside of the function scope. 2. If you repeating running the same function with same input(argument), it should give you the same output. 3. Easy to parallel if there is no data dependency . Consider this simple function that add 1 to your input: . def func1(x): x=x+1 def func2(x): return x+1 . var1 = 1 var2 = 1 func1(var1) # var1=2 func2(var2) # var2=2 . They both add 1 to your input, so which version is a better function? . func1(var1) # var1=3 func2(var2) # var2=2 . Now consider if we run this function twice. func1 changes the result to 3, while func2 still give you 2. I argue func2 is better in this case. . Why does this matter? Or how is it going to be useful at all? Well, it makes debugging much easier. It is because you only need to debug code inside a function, not 200 lines of code before it. This greatly reduce the complexity that you have to worried about your data. This fundamental principle is what powering the pipeline, and the reason why you can just use kedro run --parallel to parallelize some computation. . It will also be easier to write test for function. func1 is harder to test, because you need to consider all possible code path. You may end up need to write verbose test cases like this. . def test_case1(): func_A() func_B() def test_case2(): func_A() func_A() func_B() . How does using Kedro helps to achieve this? Think about func1, if it is written as a Node, it will look like this. . Node(func1, inputs=var1, output=None, name=&quot;func1&quot;) . Since it is a Node without any output, it will have no impact to the downstreams. In order to use that variable, you will naturally writing code looks more like func2 instead. . Let&#39;s look at one more example. . k = 10 def func3(x): return x+k . func3(10) . 20 . Now consider func3, it is a valid Python function. You can run it in a notebook or in a script, but it wouldn&#39;t be possible for a Node, sinec a Node only have access to its input. It will just throw an error to you immediately. . pyton node(func3, inputs=&#39;x&#39;, outputs=&#39;some_result&#39;, name=&#39;func3&#39;) . By writing nodes, you limit your function to only access variable within its scope. It helps to prevent a lot of bug. . Decompose program to pipeline is not just copy and paste . I hope the examples demonstrate how writing nodes help transform your code towards functional style. In reality, decoupling your functions from a programming is not straight forward. . Consider this example. . Look at how data np.nan is changed. This wouldn&#39;t be a problem if we have one program, since we will just passing all variable in memroy, without the step that writing and reading from a file. . Error like these are subtle and dangerous, it may not throw error, but ruining our features quality. We have better chance to catch these error in a small program, but it would be much harder to isolate the issue if we have 1000 lines of code. The sooner you integrate it into your pipeline, the easier the integration is. In fact, we can do better. We could introduce test case for validating data, I would explain more in Section 3.5. . Data Catalog &amp; Paramaeters . Data Catalog is an API for Dataset. It includes a Data Model from from raw data, feature, to reporting layer and a standard Data I/O API. It integrates with pandas, spark, SQLAlchemy and Cloud Storage. . To use Data Catalog, you would first need to define your dataset in the catalog.yml. You will have give it a name and type, denoting whether it is a SQL query or a CSV. Optionally, you can pass in any arguments that are supported from the underlying API as well. . example_iris_data: type: pandas.CSVDataSet filepath: data/01_raw/iris.csv . Connect Data Catalog with Node . Let&#39;s reuse our split_data function. When you create a node that using the split_data function, you would pass in the string of the dataset instead of an actual dataframe, the Reading/Writing operation is handled by Kedro, so you don&#39;t have to write to_csv() or read_csv() yourself. . parameters.yml . example_test_data_ratio: 0.2 . A node using the split_data function. . node(split_data, inputs=[&quot;example_iris_data&quot;, &quot;params:example_test_data_ratio&quot;], outputs= dict( train_x=&quot;example_train_x&quot;, train_y=&quot;example_train_y&quot;, test_x=&quot;example_test_x&quot;, test_y=&quot;example_test_y&quot;, ), name=&quot;split_data&quot;) . Here the inputs &quot;example_iris_data&quot; is refering to a dataset defined by catalog.yml, kedro will load the csv for you. Same applies for params:example_test_data_ratio. . By using catalog and parmaeters, it already makes your program cleaner. You now have a single file to manager all data source, and a single file contains all parameters, which is configurable. Your functions now is parameterized, you can simply change configuration in a single file without going into every possible script to change a number. . Data Catalog abstract away the Data I/O logic from the data processing function. . It process data and write a file. . def process_data(df): ... # do some processing df.to_csv(&#39;xxx.csv&#39;) . It only process data . def process_data(df): ... #do some processing return df . This applies the single-responsibility principle (SRP), meaning that your function is only doing one thing at a time. There are many benefits from it, for example, it makes data versioning easier. I will explain this in Section 3.3. . Memory Dataset (optional to skip) . Remember our we pass in a string to our node, and it will look for the corresponding dataset? What if we do not define it? It could be a lot of work if we need to define everything. Besides, some variable are not needed to be written out as a file, it could just stay as in memory. . In fact, kedro use MemroyDataset by default. Which means you could simply pass in a string that is not defined, the string will be use as the name of the variable. There are more useful dataset like CacheDataset, you can find more details in this link. . https://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html . p.s. When using kedro pipeline, you only define the node&#39;s inputs and outputs, but you never defined the order of execution. From my experience, there are pros and cons. The benefits is, your code is less coupled, and due to this, kedro is able to execute your pipeline in parallel whenever possible to speed up your program. However, it means the order of execution is not guaranteed, this may cause unexpected effect. For example, if you are training a machine learning model, it is common to set a random seed at the beginning. Due to the randomness of execution, you may not get identical result, as the order of execution is different everytime, thus the sequence of the random number used is random too. In general this is not a big problem, but if you have a strong need to make sure you have identical output (e.g. regression test), it may cause some trouble and you need to use dummy input and output to force kedro run your pipeline in a specific order. .",
            "url": "https://noklam.ml/2020/12/04/kedro-pipeline.html",
            "relUrl": "/2020/12/04/kedro-pipeline.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "[Drafted] -Optimizing pandas - Reducing 90% memory footprint - updated version",
            "content": "Todo . [ ] TWO options to automatically optimize pandas | . We can check some basic info about the data with pandas .info() function . df_gamelogs.info(memory_usage=&#39;deep&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 171907 entries, 0 to 171906 Columns: 161 entries, date to acquisition_info dtypes: float64(77), int64(6), object(78) memory usage: 860.5 MB . We can see the data has 171907 rows and 161 columns and 859.4 MB memory. Let&#39;s see how much we can optimize dtype_diet. . proposed_df = report_on_dataframe(df_gamelogs, unit=&quot;MB&quot;) proposed_df . Current dtype Proposed dtype Current Memory (MB) Proposed Memory (MB) Ram Usage Improvement (MB) Ram Usage Improvement (%) . Column . date int64 | int32 | 671.574219 | 335.818359 | 335.755859 | 49.995347 | . number_of_game int64 | int8 | 671.574219 | 84.001465 | 587.572754 | 87.491857 | . day_of_week object | category | 5036.400391 | 84.362793 | 4952.037598 | 98.324939 | . v_name object | category | 5036.400391 | 174.776367 | 4861.624023 | 96.529736 | . v_league object | category | 4952.461426 | 84.359375 | 4868.102051 | 98.296617 | . ... ... | ... | ... | ... | ... | ... | . h_player_9_id object | category | 4955.471680 | 412.757324 | 4542.714355 | 91.670675 | . h_player_9_name object | category | 5225.463379 | 421.197266 | 4804.266113 | 91.939523 | . h_player_9_def_pos float64 | float16 | 671.574219 | 167.940430 | 503.633789 | 74.993020 | . additional_info object | category | 2714.671875 | 190.601074 | 2524.070801 | 92.978854 | . acquisition_info object | category | 4749.209961 | 84.070801 | 4665.139160 | 98.229794 | . 161 rows × 6 columns . new_df = optimize_dtypes(df_gamelogs, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 79.04368686676025 MB . df = pd.read_csv(&#39;../data/sell_prices.csv.zip&#39;) . proposed_df = report_on_dataframe(df, unit=&quot;MB&quot;) new_df = optimize_dtypes(df, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 85.09655094146729 MB . ## collapse-hide .",
            "url": "https://noklam.ml/python/pandas/optimization/2020/11/10/Pandas-memory-optimization.html",
            "relUrl": "/python/pandas/optimization/2020/11/10/Pandas-memory-optimization.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Recreating the BBC style graphic in Python - `plotnine` and `altair`",
            "content": "Todo . [ ] Missing Subtitle | [ ] Missing Style | . Difference between plotnine and ggplot . 99% of them are the same, except that in python you have to wrap column names in &#39;&#39;, otherwise it will be treated as variable and caused error. Most of the time you just need to wrap a &#39;&#39; or replaced with _ depends on the function. . I tried to produce the same chart with plotnine and altair, and hopefully you will see their difference. plotnine covers 99% of ggplot2, so if you are coming from R, just go ahead with plotnine! altair is another interesting visualization library that base on vega-lite, therefore it can be integrated with website easily. In addition, it can also produce interactive chart with very simple function, which is a big plus! . Setup . #collapse-hide !pip install plotnine[all] !pip install altair !pip install gapminder %matplotlib inline import plotnine import pandas as pd import altair as alt from plotnine import ggplot # https://plotnine.readthedocs.io/en/stable/ from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_line from plotnine import * from plotnine.data import mtcars from gapminder import gapminder . . print(f&#39;altair version: {alt.__version__}&#39;) print(f&#39;plotnine version: {plotnine.__version__}&#39;) print(f&#39;pandas version: {pd.__version__}&#39;) . altair version: 4.0.1 plotnine version: 0.6.0 pandas version: 0.25.1 . Plotnine Example . (ggplot(mtcars, aes(&#39;wt&#39;, &#39;mpg&#39;, color=&#39;factor(gear)&#39;)) + geom_point() + stat_smooth(method=&#39;lm&#39;) + facet_wrap(&#39;~gear&#39;)) . C: ProgramData Anaconda3 lib site-packages numpy core fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) . &lt;ggplot: (-9223371916406847624)&gt; . Make a Line Chart . ggplot . line_df &lt;- gapminder %&gt;% filter(country == &quot;Malawi&quot;) #Make plot line &lt;- ggplot(line_df, aes(x = year, y = lifeExp)) + geom_line(colour = &quot;#1380A1&quot;, size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in Malawi 1952-2007&quot;) . plotnine . (ggplot(line_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;)) + geom_line(colour=&#39;#1380A1&#39;, size=1) + geom_hline(yintercept = 0, size = 1, colour=&#39;#333333&#39;) + labs(title=&#39;Living longer&#39;, subtitle = &#39;Life expectancy in Malawi 1952-2007&#39;) ) . &lt;ggplot: (-9223371916406567792)&gt; . ## altair line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in Malawi 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;y&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;y:Q&#39;) line + hline . The BBC style . function () { font &lt;- &quot;Helvetica&quot; ggplot2::theme(plot.title = ggplot2::element_text(family = font, size = 28, face = &quot;bold&quot;, color = &quot;#222222&quot;), plot.subtitle = ggplot2::element_text(family = font, size = 22, margin = ggplot2::margin(9, 0, 9, 0)), plot.caption = ggplot2::element_blank(), legend.position = &quot;top&quot;, legend.text.align = 0, legend.background = ggplot2::element_blank(), legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), legend.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.title = ggplot2::element_blank(), axis.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)), axis.ticks = ggplot2::element_blank(), axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major.y = ggplot2::element_line(color = &quot;#cbcbcb&quot;), panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), strip.background = ggplot2::element_rect(fill = &quot;white&quot;), strip.text = ggplot2::element_text(size = 22, hjust = 0)) } &lt;environment: namespace:bbplot&gt; . The finalise_plot() function does more than just save out your chart, it also left-aligns the title and subtitle as is standard for BBC graphics, adds a footer with the logo on the right side and lets you input source text on the left side. . altair . ## altair line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) line + hline . Make a multiple line chart . ggplot . #Prepare data multiple_line_df &lt;- gapminder %&gt;% filter(country == &quot;China&quot; | country == &quot;United States&quot;) #Make plot multiple_line &lt;- ggplot(multiple_line_df, aes(x = year, y = lifeExp, colour = country)) + geom_line(size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + scale_colour_manual(values = c(&quot;#FAAB18&quot;, &quot;#1380A1&quot;)) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in China and the US&quot;) . plotnine . # Make plot multiline = ( ggplot(multiline_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;, fill=&#39;country&#39;)) + geom_line(colour=&quot;#1380A1&quot;, size=1) + geom_hline(yintercept=0, size=1, color=&quot;#333333&quot;) + scale_colour_manual(values=[&quot;#FAAB18&quot;, &quot;#1380A1&quot;]) + # bbc_style() + labs(title=&quot;Living longer&quot;, subtitle=&quot;Life expectancy in China 1952-2007&quot;)) multiline . C: ProgramData Anaconda3 lib site-packages plotnine guides guides.py:200: PlotnineWarning: Cannot generate legend for the &#39;color&#39; aesthetic. Make sure you have mapped a variable to it PlotnineWarning) . &lt;ggplot: (-9223371916406562192)&gt; . altair . multiline_altair = (alt.Chart(multiline_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;, color=&#39;country&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) multiline_altair + hline . Make a bar chart . ggplot . #Prepare data bar_df &lt;- gapminder %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(desc(lifeExp)) %&gt;% head(5) #Make plot bars &lt;- ggplot(bar_df, aes(x = country, y = lifeExp)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle = &quot;Highest African life expectancy, 2007&quot;) . ## hide bar_df = gapminder.query(&#39; year == 2007 &amp; continent == &quot;Africa&quot; &#39;).nlargest(5, &#39;lifeExp&#39;) . plotnine . bars_ggplot = (ggplot(bar_df, aes(x=&#39;country&#39;, y=&#39;lifeExp&#39;)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + # bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle=&quot;Highest African life expectancy, 2007&quot;)) bars_ggplot . &lt;ggplot: (-9223371916405111540)&gt; . altair . bars_altair = (alt.Chart(bar_df).mark_bar().encode( x=&#39;country&#39;, y=&#39;lifeExp&#39;, # color=&#39;country&#39; ) .properties(title={&#39;text&#39;: &#39;Reunion is highest&#39;, &#39;subtitle&#39;: &#39;Highest African life expectancy, 2007&#39;}) ) bars_altair . Make a stacked bar chart . Data preprocessing . ## collapse-hide stacked_bar_df = ( gapminder.query(&#39; year == 2007&#39;) .assign( lifeExpGrouped=lambda x: pd.cut( x[&#39;lifeExp&#39;], bins=[0, 50, 65, 80, 90], labels=[&quot;under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;])) .groupby( [&#39;continent&#39;, &#39;lifeExpGrouped&#39;], as_index=True) .agg({&#39;pop&#39;: &#39;sum&#39;}) .rename(columns={&#39;pop&#39;: &#39;continentPop&#39;}) .reset_index() ) stacked_bar_df[&#39;lifeExpGrouped&#39;] = pd.Categorical(stacked_bar_df[&#39;lifeExpGrouped&#39;], ordered=True) stacked_bar_df.head(6) . continent lifeExpGrouped continentPop . 0 | Africa | under 50 | 376100713.0 | . 1 | Africa | 50-65 | 386811458.0 | . 2 | Africa | 65-80 | 166627521.0 | . 3 | Africa | 80+ | NaN | . 4 | Americas | under 50 | NaN | . 5 | Americas | 50-65 | 8502814.0 | . ggplot . #prepare data stacked_df &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% mutate(lifeExpGrouped = cut(lifeExp, breaks = c(0, 50, 65, 80, 90), labels = c(&quot;Under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;))) %&gt;% group_by(continent, lifeExpGrouped) %&gt;% summarise(continentPop = sum(as.numeric(pop))) #set order of stacks by changing factor levels stacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped))) #create plot stacked_bars &lt;- ggplot(data = stacked_df, aes(x = continent, y = continentPop, fill = lifeExpGrouped)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + bbc_style() + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_d(direction = -1) + geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;% of population by life expectancy band, 2007&quot;) + theme(legend.position = &quot;top&quot;, legend.justification = &quot;left&quot;) + guides(fill = guide_legend(reverse = TRUE)) . plotnine . # create plot stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . C: ProgramData Anaconda3 lib site-packages plotnine scales scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction` warn(msg.format(self.__class__.__name__, k), PlotnineWarning) C: ProgramData Anaconda3 lib site-packages plotnine layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values. data = self.position.setup_data(self.data, params) . &lt;ggplot: (-9223371916391151364)&gt; . # create plot stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . altair . stacked_bar_altair = ( alt.Chart(stacked_bar_df) .mark_bar() .encode(x=&#39;continent&#39;, y=alt.Y(&#39;continentPop&#39;, stack=&#39;normalize&#39;, axis=alt.Axis(format=&#39;%&#39;)), fill=alt.Fill(&#39;lifeExpGrouped&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;))) .properties(title={&#39;text&#39;: &#39;How life expectancy varies&#39;, &#39;subtitle&#39;: &#39;% of population by life expectancy band, 2007&#39;} ) ) overlay = overlay = pd.DataFrame({&#39;continentPop&#39;: [0]}) hline = alt.Chart(overlay).mark_rule( color=&#39;#333333&#39;, strokeWidth=2).encode(y=&#39;continentPop:Q&#39;) stacked_bar_altair + hline . Make a grouped bar chart . ggplot . #Prepare data grouped_bar_df &lt;- gapminder %&gt;% filter(year == 1967 | year == 2007) %&gt;% select(country, year, lifeExp) %&gt;% spread(year, lifeExp) %&gt;% mutate(gap = `2007` - `1967`) %&gt;% arrange(desc(gap)) %&gt;% head(5) %&gt;% gather(key = year, value = lifeExp, -country, -gap) #Make plot grouped_bars &lt;- ggplot(grouped_bar_df, aes(x = country, y = lifeExp, fill = as.factor(year))) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_fill_manual(values = c(&quot;#1380A1&quot;, &quot;#FAAB18&quot;)) + labs(title=&quot;We&#39;re living longer&quot;, subtitle = &quot;Biggest life expectancy rise, 1967-2007&quot;) . plotnine . altair . Make a dumbbell chart . ## hide . ggplot . hist_df &lt;- gapminder %&gt;% filter(year == 2007) ggplot(hist_df, aes(lifeExp)) + geom_histogram(binwidth = 5, colour = &quot;white&quot;, fill = &quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, by = 10), labels = c(&quot;40&quot;, &quot;50&quot;, &quot;60&quot;, &quot;70&quot;, &quot;80&quot;, &quot;90 years&quot;)) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;Distribution of life expectancy in 2007&quot;) . plotnine . altair . Make changes to the legend . ## hide . ggplot . . plotnine . altair . Make changes to the axes . ## hide . ggplot . . plotnine . altair . Add annotations . ## hide . ggplot . . plotnine . altair . Work with small multiples . ## hide . ggplot . . plotnine . altair . Do something else entirely . ## hide . ggplot . . plotnine . altair .",
            "url": "https://noklam.ml/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "relUrl": "/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Beyond Unit Testing - What is Property-based Testing?",
            "content": "#collapse-hide # https://hypothesis.readthedocs.io/en/latest/quickstart.html !pip install hypothesis %load_ext ipython_pytest . . Requirement already satisfied: hypothesis in c: programdata anaconda3 lib site-packages (5.8.1) Requirement already satisfied: sortedcontainers&lt;3.0.0,&gt;=2.1.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (2.1.0) Requirement already satisfied: attrs&gt;=19.2.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (19.2.0) The ipython_pytest extension is already loaded. To reload it, use: %reload_ext ipython_pytest . Unit Testing is a common technique for software engineering. Even if you are not writing a unit test explicitly, you are still doing unit testing, as your function should at least works for what you intended. You give an input x to a function, it should return y, simple as that. . For example, imagine we have a function like this. . def add_ints(x1, x2): return x1 + x2 . # Case 1 add_ints(1,1) . 2 . # Case 2 add_ints(1,&#39;2&#39;) . TypeError Traceback (most recent call last) &lt;ipython-input-59-99ea9d9c8984&gt; in &lt;module&gt; 1 # Case 2 -&gt; 2 add_ints(1,&#39;2&#39;) &lt;ipython-input-57-d4599be2ffda&gt; in add_ints(x1, x2) 1 def add_ints(x1, x2): -&gt; 2 return x1 + x2 TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . # Case 3 add_ints(&#39;2&#39;, &#39;2&#39;) . The first two cases are expected behaviors, but the last case is a side-effect of how Python works. We should probably checks the input are numbers, otherwise we should throw error explicitly. Now, checking function behave properly with intend use is easy, to test the opposite is much harder. You have to test a lot of edge case, which is much harder and make your test verbose. . In this article, I will introduce a library called Hypothesis that does property-based testing. If none of this make sense to you, please bare with me, I will explain with simple examples. I found the name of Hypothesis and property-based testing isn&#39;t adding a lot of information, but they are useful. . Hypothesis comes in handy that it generated artificial input to make your test fails. Instead of specifying an input, you specify what kind of input you want to test loosely. For example, if you expect your input is number, often you may want to test when the value is negative, positive, a floating point number, or if it exceeds certain range. This list of condition can expands quickly, and Hypothesis make this easier. . Start with a simple function . Let&#39;s stick with our simple add_ints function above. To keep it simple, let test for this 3 cases first. . Adding two number -&gt; Expect Pass | Adding number and string -&gt; Expect Fail | Adding two number -&gt; Expect Fail | %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): return x1 + x2 def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmp79u2a6x6 plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xF [100%] ================================== FAILURES =================================== ____________________________ test_add_ints_string _____________________________ [XPASS(strict)] =================== 1 failed, 1 passed, 1 xfailed in 0.12s ==================== . In pytest, you can use a mark @pytest.mark.xfail to annotate a function is expected to fail the test. We have 1 pass, 1xfailed, 1 failed. . _ipytesttmp.py .xF indicates the last test is failed. Let&#39;s try to fix it by throwing an error is input type is not a number. . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpfrh2uipy plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xx [100%] ======================== 1 passed, 2 xfailed in 0.10s ========================= . Okay, now we checks if input are integers. In reality, this if often an iterative process. You start with coming up with test cases, then every now and then, you hit some edge cases and you add that into your collections of test cases. . How can we make out test cases more robust to input? Hypothesis is exactly the tool you need. . strategy, your auto-genenerated input for unit test . strategy is your input for unit test. Instead of specify a number, or a string, you specify what kind of input you want, and Hypothesis wouuld take care the rest of it. You can even composite different strategies to form more complicated input. . But let&#39;s keep it simple, we would just use integer for this demo. . from hypothesis import strategies as st . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpmdbi_h6f plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py F [100%] ================================== FAILURES =================================== ________________________________ test_add_ints ________________________________ @given(st.integers(), st.integers()) &gt; def test_add_ints(x1, x2): _ipytesttmp.py:15: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ x1 = 0, x2 = 0 @given(st.integers(), st.integers()) def test_add_ints(x1, x2): &gt; assert add_ints(x1, x2) E assert 0 E + where 0 = add_ints(0, 0) _ipytesttmp.py:16: AssertionError Hypothesis - Falsifying example: test_add_ints( x1=0, x2=0, ) ============================== 1 failed in 0.30s ============================== . The test was simple, as should pass as long as no error was thrown. Look what Hypothesis found, it found when both x1, x2=0, the assertion will fail, because we are asserting 0 + 0 = 0, thus evaluated as False in Python. . Hence, I modified my test to not assert anything, it should just keep silent as long as no error is thrown. . @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpwicd08ny plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.25s ============================== . Yes, now our test finally pass. .",
            "url": "https://noklam.ml/python/2020/04/12/Property-based-testing-in-Python.html",
            "relUrl": "/python/2020/04/12/Property-based-testing-in-Python.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Hey Data Scientist, Use a Larger Font Size",
            "content": ". In reality, you probably don&#39;t need a title as big as this one. But using library defautls often is not the best choice. . #collapse-hide def make_scatter_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.scatter(x, y) ax.set_title(&#39;A Simple Scatter Plot&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() def make_line_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.plot(x, y, &#39;-&#39;) ax.set_title(&#39;A Simple Line Chart&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() . . The Problem of library defaults . make_scatter_plot() make_line_plot() . Your insight is as best as your audience understand. Data Scientist spends a lot of time to drill insight from data, but not enough time to present their insight. Unfortunately, human perception is largely based on visual, a easy-to-read chart is much more likely to sell your idea with a custom matplotlib pyplot chart. There is nothing wrong with matplotlib, it is custom for a user sit in front of a monitor. When it comes to presentation, you really should make some adjustment for your audience. Luckily, it is easy to do with the following tips. . Apply matplotlib theme . with plt.style.context(&#39;ggplot&#39;): # Or plt.style.use(&#39;presentation&#39;) for global setting make_scatter_plot() make_line_plot() . Much better right? . There is nothing wrong with the chart if you are viewing it in front of your monitor. However, you may not want to put it directly into your PowerPoint. . Make PowerPoint-ready charts . Luckily, there is some easy way to prepare PowerPoint-ready charts. I created a presentation.mplstyle file as follow. . Custom presentation theme . axes.titlesize : 24 axes.labelsize : 24 axes.location: &#39;left&#39; lines.linewidth : 3 lines.markersize : 10 xtick.labelsize : 18 ytick.labelsize : 18 figure.figsize : 10, 6 figure.titlesize: 24 . with plt.style.context([&#39;presentation&#39;, &#39;ggplot&#39;]): make_scatter_plot() make_line_plot() . If you are careful enough, you will notice the font size of the title is not correct. This is because ggplot theme overwrite my theme. To make it right, you just need to switch the order so that your theme will overwrite conflict settings. . with plt.style.context([&#39;ggplot&#39;, &#39;presentation&#39;]): make_scatter_plot() make_line_plot() . I actually disable the grid in my presentation theme, which conflicts with fivethirtyeight configuration. If conflict configs exist, it resolved base on your order. See the same plot with &#39;presentation&#39;,&#39;fivethirtyeight&#39; in reverse order. . To give you a sense how this affect your presenation, I put it into a Powerpoint, see if you feel the difference. . . . Avoid Low Resolution Chart . . Note: Believe it or not, a low resolution chart looks much less conviencing. Taking screenshot with larger charts helps you to preserve the resolution. . Resolution of the chart is much better | More obvious Title &amp; Label (Try take a few step back from your monitor, see if you can read it) | . Define Once, Use Everywhere . It could be troublesome if you need to define the same file over and over in different computer/environment. You can actually use a URL. I have put my own theme in GitHub so I can always access it from anywhere. . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Bad key &#34;font.name&#34; on line 9 in https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template or from the matplotlib source distribution . Conclusion . I hope this blog helps you to prepare Powerpoint-ready charts better, happy coding! .",
            "url": "https://noklam.ml/python/2020/04/10/Presentation-Ready-Chart.html",
            "relUrl": "/python/2020/04/10/Presentation-Ready-Chart.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
            "content": "I have teamed up with a friend to participate in the Bengali Image Classification Competition. We struggled to get a high rank in the Public leaderboard throughout the competition. In the end, the result is a big surprise to everyone as the leaderboard shook a lot. . . . The final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard. . The Classification Task . This is an image classification competition. We need to predict 3 parts of Bengali characters root, consonant and vowel. It is a typical classification tasks like the MNIST dataset. . . Evaluation Metrics . The competition use macro-recall as the evaluation metric. In general, people get &gt;96% recall in training, the tops are even getting &gt;99% recall. . # collapse-hide python import numpy as np import sklearn.metrics scores = [] for component in [&#39;grapheme_root&#39;, &#39;consonant_diacritic&#39;, &#39;vowel_diacritic&#39;]: y_true_subset = solution[solution[component] == component][&#39;target&#39;].values y_pred_subset = submission[submission[component] == component][&#39;target&#39;].values scores.append(sklearn.metrics.recall_score( y_true_subset, y_pred_subset, average=&#39;macro&#39;)) final_score = np.average(scores, weights=[2,1,1]) . . Model (Bigger still better) . We start with xresnet50, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast. . Near the end of the competition, we switch to a larger model se-resnext101. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with ~0.3-0.5% recall. It is a big improvement as the recall is very high (~0.97), in other words, it reduces ~10% error solely by just using a better model, not bad! . Augmentation . There are never &quot;enough&quot; data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation. We start with rotation + scale. We also find MixUp and CutMix is very effective to boost the performance. It also gives us roughly 10% boost initially from 0.96 -&gt; 0.964 recall. . CutMix &amp; MixUp . . Mixup is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2. . CutMix shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one. . It always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically. . Logging of Experiment . I normally just log my experiment with a simple CSV and some printing message. This start to get tedious when there are more than 1 people to work. It is important to communicate the results of experiments. I explore Hydra and wandb in this competition and they are very useful. . Hydra . It is often a good idea to make your experiment configurable. We use Hydra for this purpose and it is useful to compose different configuration group. By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model. . I have written an short example for how to use Hydra. . Wandb . wandb (Weight &amp; Biases) does a few things. It provides built-in functions that automatically log all your model statistics, you can also log your custom metrics with simple functions. . Compare the configuration of different experiments to find out the model with the best performance. | Built-in function for logging model weights and gradient for debugging purpose. | Log any metrics that you want | . All of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier. . . Stochastic Weight Averaging . This is a simple yet effective technique which gives about 0.3-0.4% boost to my model. In simple words, it takes snapshots of the model weights during training and takes an average at the end. It provides a cheap way to do models ensemble while you are only training 1 model. This is important for this competition as it allows me to keep training time short enough to allow feedback within hours and reduce over-fitting.) . . Larger is better (image size) . We downsample our image size to 128x128 throughout the competition, as it makes the model train faster and we believe most technique should be transferable to larger image size. It is important to keep your feedback loop short enough (hours if not days). You want your training data as small as possible while keeping them transferable to your full dataset. Once we scale our image to full size, it takes almost 20 hours to train a single model, and we only have little chance to tune the hyper-parameters before the competition end. . Debug &amp; Checkpoint . There was a time we develop our model separately and we didn&#39;t sync our code for a while. We refactor our code during the time and it was a huge mistake. It turns out our pre-refactor code trains much better model and we introduce some unknown bug. It is almost impossible to find out as we change multiple things. It is so hard to debug a neural network and testing it thoroughly is important. Injecting a large amount of code may help you to run an experiment earlier, but you may pay much more time to debug it afterwards. . I think this is applicable even if you are working alone. . Keep your changes small. | Establish a baseline early, always do a regression test after a new feature introduced (especially after code refactoring) | Create checkpoint to rollback anytime, especially if you are not working on it every day. | . Implementation is the key of Kaggle competition (in real life too). It does not matter how great your model is, a tiny little bug could have damaged your model silently . Use auxiliary label . As mentioned earlier, this competition requires to predict the root, vowel and the consonant part. In the training data, they actually provide the grapheme too. Lots of people saying that if you train with the grapheme, it improves the model greatly and get the recall &gt;98% easily. . This is something we could not reproduce throughout the competition, we tried it in the very last minute but it does not seem to improve our model. It turns out lots of people are overfitting the data, as the testing dataset has much more unseen character. . But it is still a great remark that training with labels that is not your final desired output could still be very useful. . Weight loss . The distribution of the training dataset is very imbalance, but to get a good result, we need to predict every single class accurately (macro recall). To deal with this issue, we choose to use class weights, where a higher weight would be applied to rare samples. We don&#39;t have an ablation study for this, but it seems to help close the gap between accuracy &amp; recall and allows us to train the model slightly better. . Find a teammate! . Lastly, please go and find a teammate if you can. It is very common to start a Kaggle competition, but not so easy to finish them. I have stopped for a month during the competition due to my job. It is really hard to get back to the competition after you stopped for so long. Getting a teammate helps to motivate you and in the end, it is a great learning experience for both of us. . Pretrain Model . We also tried to use a pretrained model, as it allows shorter training and gives better performance by transfer learning (Using weights learn from a large dataset to as initial weight). It also gives our model a bit of improvement. . Finetune the model head, while keeping other layers freeze (except BatchNorm layer). | Unfreeze the model, train all the layers together. | . I also tried training the model directly with discriminating learning rate while not freezing any layer at all. It performs similarly to freezing fine-tuning , so I end up just start training the entire model from the beginning. . If the code works, don&#39;t touch it . This is probably not a good habit usually, but I suggest not to do it for a competition. We spent lots of time for debugging our code after code refactoring and end up just rolling back to an older commit and cherry-picks new features. In a competition, you don&#39;t have enough time to test everything. You do not need a nice abstract class for all your features, some refactoring to keep your function/class clean is probably needed, but do not overspend your time on it. It is even common to jump between frameworks (you may find other&#39;s Kernel useful), so it is not possible to structure your code perfectly. . If someone has create a working submission script, use it! | If someone has create a working pre-processing function, use it! | . Don&#39;t spend time on trying to optimize these code unless it is necessary, it is often not worth it in a competition context. You should focus on adding new features, trying out new model, testing with new augmentation technique instead. . Summary . This is a great learning experience and refreshes some of my outdated computer vision model knowledge. If you have never joined a competition, find a friend and get started. If you have just finished one, try writing it out and share your experience. 😉 .",
            "url": "https://noklam.ml/ml/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "relUrl": "/ml/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "date": " • Mar 21, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "The missing piece in Python tutorial - What is dispatch why you should care",
            "content": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want. . Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean. . In python, you cannot overload a normal function twice for different behavior base on the arguments. For example: . def foo(number:int ): print(&#39;it is a integer&#39;) def foo(number: float): print(&#39;it is a float&#39;) . foo(1) . it is a float . The definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument. . from functools import singledispatch @singledispatch def foo(number ): print(f&#39;{type(number)}, {number}&#39;) . foo(1) . &lt;class &#39;int&#39;&gt;, 1 . We can now register the function for different argument type. . @foo.register(int) def _(data): print(&#39;It is a integer!&#39;) @foo.register(float) def _(data): print(&#39;It is a float!&#39;) @foo.register(dict) def _(data): print(&#39;It is a dict!&#39;) . foo(1.0) foo(1) foo({&#39;1&#39;:1}) . It is a float! It is a integer! It is a dict! . How is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument. . It will fallback to the most generic function if the type of argument is not registered. . foo([1,2,3]) . &lt;class &#39;list&#39;&gt;, [1, 2, 3] . I hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that. . In next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too. . Fastai @typedispatch . Single Dispatch is great, but what if we can do multi dispatch for more than 1 argument? . from fastcore.dispatch import typedispatch, TypeDispatch . Let us first try if this work as expected . @typedispatch def add(x:int, y:int): return x+y @typedispatch def add(x:int, y:str): return x + int(y) . print(add(1,2)) print(add(1,&#39;2&#39;)) print(add(&#39;a&#39;,&#39;a&#39;)) . 3 3 a . add(1,2) . 3 . add(1,&#39;2&#39;) . 3 . But what if we added something does not define? . add(&#39;2&#39;,1) . &#39;2&#39; . &#39;2&#39;? where does it come from? Let&#39;s have a look at the definition of typedispatch and understand how it works. . ??typedispatch class DispatchReg: &quot;A global registry for `TypeDispatch` objects keyed by function name&quot; def __init__(self): self.d = defaultdict(TypeDispatch) def __call__(self, f): nm = f&#39;{f.__qualname__}&#39; self.d[nm].add(f) return self.d[nm] . In fact, typedispatch is not even a function, it&#39;s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg . type(typedispatch) . fastcore.dispatch.DispatchReg . typedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you. . def foo(): return &#39;foo&#39; a = foo def foo(): return &#39;not foo&#39; b = foo . foo() . &#39;not foo&#39; . foo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable. . a() . &#39;foo&#39; . b() . &#39;not foo&#39; . hex(id(a)), hex(id(b)) . (&#39;0x2b9d28bb5e8&#39;, &#39;0x2b9d2ebe048&#39;) . The two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition. . typedispatch.d . defaultdict(fastcore.dispatch.TypeDispatch, {&#39;cast&#39;: (object,object) -&gt; cast, &#39;add&#39;: (int,str) -&gt; add (int,int) -&gt; add}) . So back to our question, why does add(&#39;a&#39;,1) return &#39;a&#39;? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument. . def __call__(self, *args, **kwargs): ts = L(args).map(type)[:2] f = self[tuple(ts)] if not f: return args[0] if self.inst is not None: f = MethodType(f, self.inst) return f(*args, **kwargs) .",
            "url": "https://noklam.ml/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "relUrl": "/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 | The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 | First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 | I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 | Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 | Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![123](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://noklam.ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "data augmentation - Understand MixUp and Beta Distribution",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta . Understand Mixup Augmentation &amp; Beta Distribution . Implementation In the original article, the authors suggested three things: . Create two separate dataloaders and draw a batch from each at every iteration to mix them up | Draw a t value following a beta distribution with a parameter alpha (0.4 is suggested in their article) | Mix up the two batches with the same value t. | Use one-hot encoded targets | Source: https://forums.fast.ai/t/mixup-data-augmentation/22764 (Sylvain Gugger) . Beta Distribution . Beta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat . To get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect . import math import torch import matplotlib.pyplot as plt from torch import tensor . # PyTorch has a log-gamma but not a gamma, so we&#39;ll create one Γ = lambda x: x.lgamma().exp() facts = [math.factorial(i) for i in range(7)] plt.plot(range(7), facts, &#39;ro&#39;) plt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1)) plt.legend([&#39;factorial&#39;,&#39;Γ&#39;]); . . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [5.0,1.0,0.4, 1.0] b_ls = [1.0,5.0,0.4, 1.0] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α != β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . A few observations from this graph. . α and β control the curve symmetrically, the blue line is symmetric with the orange line. | when α and β = 1, it reduce to uniform distribution | when α = β, the distribution is a symmetric distribution | . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [0.1, 0.4, 0.6, 0.9] b_ls = [0.1, 0.4, 0.6, 0.9] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α = β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . As we remember, when α = β =1, it is an uniform distribution. When α = β , when α is small, most density is concentrated around 0 and 1, and when α increase, the distribution get more evenly distributed. . The default for α suggested by the paper is 0.4 .",
            "url": "https://noklam.ml/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "relUrl": "/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Hydra - Config Composition for Machine Learning Project",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example . Machine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations. . Assume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure. . Folder Structure . config.yaml demo.py run_mode - train.yaml - test.yaml hyperparmeter - base.yaml . config.yaml . defaults: - run_mode: train - hyperparameter: base . The benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance. . import hydra from omegaconf import DictConfig @hydra.main(config_path=&quot;config.yaml&quot;) def my_app(cfg : DictConfig) -&gt; None: print(cfg.pretty()) if __name__ == &quot;__main__&quot;: my_app() . python demo.py . gamma: 0.01 learning_rate: 0.01 run_mode: train week: 8 . For example, with a simple example with 4 parameters only, you can simply run the experiment with default . Override default parameters . You can easily overrite the learning rate with an argument, it would be very clear that learning rate is the only changing parameter with this approach . python demo.py learning_rate=0.1 . gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 . In somecase, you may only need to test a model instead of changing it. . python demo.py learning_rate=0.1 run_mode=test . gamma: 0.01 learning_rate: 0.1 run_mode: test week: 8 . It also safeguard your experiment if you pass in some parameters that is not exist . !python demo.py typo=0.2 . Traceback (most recent call last): File &quot;demo.py&quot;, line 7, in &lt;module&gt; my_app() &quot;C: ProgramData Anaconda3 lib site-packages omegaconf dictconfig.py&quot;, line 41, in __setitem__ &quot;Accessing unknown key in a struct : {}&quot;.format(self.get_full_key(key)) KeyError: &#39;Accessing unknown key in a struct : typo&#39; . –Multirun, Combination of parameters . In case you want to gridsearch paramters, which is very common in machine learning, you can use an additional argument multirun to do that easily. . !python demo.py --multirun learning_rate=0.1,0.01,0.001 gamma=0.1,0.01 . [2020-02-08 19:28:46,095][HYDRA] Sweep output dir : multirun/2020-02-08/19-28-46 [2020-02-08 19:28:46,102][HYDRA] Launching 6 jobs locally [2020-02-08 19:28:46,103][HYDRA] #0 : learning_rate=0.1 gamma=0.1 gamma: 0.1 learning_rate: 0.1 run_mode: train week: 8 [2020-02-08 19:28:46,192][HYDRA] #1 : learning_rate=0.1 gamma=0.01 gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 ... SKIPPED .",
            "url": "https://noklam.ml/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "relUrl": "/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://noklam.ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "plyer - Desktop Notification with Python",
            "content": "from plyer import notification import random class DesktopNotification: @staticmethod def notify(title=&#39;Hey~&#39;, message=&#39;Done!&#39;, timeout=10): ls = [&#39;👍&#39;,&#39;✔&#39;,&#39;✌&#39;,&#39;👌&#39;,&#39;👍&#39;,&#39;😎&#39;] notification.notify( title = title , message = random.choice(ls) * 3 + &#39; &#39; + message, timeout = timeout # seconds ) if __name__ == &#39;__main__&#39;: DesktopNotification.notify() . You could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window. .",
            "url": "https://noklam.ml/python/2019/10/19/Deskto-Notification.html",
            "relUrl": "/python/2019/10/19/Deskto-Notification.html",
            "date": " • Oct 19, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://noklam.ml/codespaces",
            "relUrl": "/codespaces",
            "date": " • Jan 1, 2019"
        }
        
    
  

  
  
      ,"page0": {
          "title": "",
          "content": "1792. Maximum Average Pass Ratio . Complexity O(N * LogN) + O(K * LogN) . We need to sort the list once | Then for each update, we need to insert the tuple in an ordered list (Log N) * k | Learning point . A heap would simplify the implementation a lot, as I am basically implementing heap here. | list.pop(0) is O(n) while deque.popleft() is O(1) | User HEAP! | . from typing import List from bisect import insort def average(classes): ratios = [] for i in classes: ratios.append(i[0] / i[1]) return sum(ratios) / len(ratios) def delta(x, y): return -(y - x) / (y ** 2 + y) class Solution: def maxAverageRatio(self, classes: List[List[int]], extraStudents: int) -&gt; float: order = [] for x in classes: order.append(delta(x[0], x[1])) d = [] for i, v in enumerate(order): t = tuple([v,i]) d.append(t) order = sorted(d) while extraStudents &gt; 0 and order: selected_tuple = order.pop(0) value,index = selected_tuple c = classes[index] pass_, total_ = c if pass_ == total_: continue c[0] = pass_ + 1 c[1] = total_ + 1 extraStudents = extraStudents - 1 new_value = delta(c[0], c[1]) new_tuple = (new_value, index, ) insort(order, new_tuple) if extraStudents &lt;= 0: break return average(classes) . Reference: . https://leetcode.com/problems/maximum-average-pass-ratio/discuss/1108686/Python-Time-exceed .",
          "url": "https://noklam.ml/leetcode/1792.-maximum-average-pass-ratio.html",
          "relUrl": "/leetcode/1792.-maximum-average-pass-ratio.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "",
          "content": "README . Introduction . . https://noklam.ml . All things data . I am a data scientist. Recently, I find myself studying database, data structure, data pipeline way more than machine learning. To build a good model, I found the importance of writing good code to produce data with quality often triumphs a SOTA model. . Delivering the model is the job of a data scientist. Inevitably, every data scientist should somewhat be a “full-stack” data scientist. . This is a central repository for my blogs and notes . Blog: https://noklam.ml (Github Page) - Usually blog or notes with code with shorter articles | Blog: Medium (https://medium.com/@nokknocknok) | GitBook (Study notes mainly, I use Joplin to keep notes in markdown, am considering sync to Gitbook from time to time. I haven’t figured out what’s the best way to do so.) | . Resource . I am generally interested in tools that increase productivity, please let me know if you have any recommendations. Here is a list of software/topics that I found useful. . Uncertainty Estimation . Uncertainty Quantification in Deep Learning . Visualization . Visualization (University of Washington) . Custom Matplotlib style for Presentation (Larger font size) . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Useful Python Tools . pyinstructment: for profiling python process, which is useful for optimization | torchsnooper -&gt; pytorch profiling, another profiling tool which is for PyTorch, no more print x.shape anymore. | knockknock notification: A single line of code that get you notifications when your 10 hours model training finally done. No more starring at the progress bar. | colorama: Colored printing in terminal (cross platform) | Hypoehsis - Property-based testing, autogenerated input for unit-test. . Reviewing (any suggestions for code metric report/analysis library are welcome!) . | coala - coala provides a unified command-line interface for linting and fixing all your code, regardless of the programming languages you use. | radon - Radon is a Python tool that computes various metrics from the source code | great_expectations - A data validation library for python integrated with Pandas/Spark/SQL | . Syntax Highlight . lunr.js | . A catalog of various machine learning topics. . Graph Neural Network Basics Understand What is the weird D-1/2LD-1/2 | Supplement Chinese Reading | . | Time Series Forecast Motivation | Forecasting Methods Statistical Method | Machine Learning | Deep Neural Network | . | . | Prediction Interval Python Time Series Forecasting Library | . | Contribution | Under Review | . Graph Neural Network Basics . Understand What is the weird D-1/2LD-1/2 . spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | What’s the intuition behind a Laplacian matrix? I’m not so much interested in mathematical details or technical applications. I’m trying to grasp what a laplacian matrix actually represents, and what aspects of a graph it makes accessible. - Quora | Supplement Chinese Reading . Heat Diffusion | GCN use edge to agg node information | How to do batch training with GCN | Time Series Forecast . Motivation . While neural network has gain a lot of success in NLP and computer vision, there are relatively less changes for traditional time series forecasting. This repository aims to study the lastest practical technique for time series prediction, with either statistical method, machine learning, or deep neural network. . Forecasting Methods . Statistical Method . Machine Learning . Deep Neural Network . Gramian Angular Field : Transform time series into an image and use transfer learning with CNN . Prediction Interval . While forecasting accuracy is important, the prediction interval is also important and it is an area that the machine learning world has less focus on. . Traditional statistical forecast (ARIMA, ETS etc) | Bayesian Neural Network | Random Forest jackknife approximation | MCDropout (Use Dropout at inference time as variation inference) | Quantile Regression | VOGN (Optimizer weight perturbation) | Random Forest jackknife approximation | . Python Time Series Forecasting Library . Prophet (Facebook): Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth. It has build-in modeling for the Holiday effect. . pyts : state-of-the-art algorithms for time-series transformation and classification . Contribution . Feel free to send a PR or discuss by starting an issue.😁 . powered by fastpages . fastpages allow me to blog directly in Notebook, so I don’t have to worry how to convert into markdown anymore. I simple code and write. .",
          "url": "https://noklam.ml/README.html",
          "relUrl": "/README.html",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://noklam.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "Explainable AI . Introduction . Personal collecions of model interpretation utilities. . This repository include general model interpretation methods. Most articles focus on library, but the method are actually general than that. For example, we can use partial indepedence for deep learning model too. You will find most tutorials out there are only using it on Tree. . Same applied on SHAP, many tutorials use Tree as example, but the library actually support much more general algorithm. Your feature don’t even have to be a column. . Partial Depedence Plot | SHAP | Tensorflow (What-if tools)[https://pair-code.github.io/what-if-tool/age.html] | Counterfactual (Most simliar data point, but very different inference value) | Model interpretation . Model interpretation can be divided into local or global. Different method are complementary instead of replacement. For example, SHAP give you an idea what features are important for a particular prediction. But Partial Dependence plot supplement the “What-if” condition, namely, how will your prediction changes when a dependent variable changes. These are two different information which are often overlooked. . Partial Dependence . Local | Global | . Partial depedence can be applied in row level or dataset level. It gives you a sense that how Change of a feature will change the model output accordingly. . We can also “zoom in” if we want, say using a subset of data (e.g. at country level) or even at row level to dig into the model. . The what-if tool allows you to change the value of a feature and run inference. Partial dependence is doing the exact same thing except it run multiple prediction by changing one features to different values to obtain a continuous plot. . . SHAP . Squashing function (e.g. log transformation of Target variable) | . It can affect the “feature importance” as it will change the order of feature importance even with a monotonic transformation .",
          "url": "https://noklam.ml/explainable-ai.html",
          "relUrl": "/explainable-ai.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://noklam.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}