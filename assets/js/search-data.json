{
  
    
        "post0": {
            "title": "[Drafted] -Optimizing pandas - Reducing 90% memory footprint - updated version",
            "content": "Todo . [ ] TWO options to automatically optimize pandas | . We can check some basic info about the data with pandas .info() function . df_gamelogs.info(memory_usage=&#39;deep&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 171907 entries, 0 to 171906 Columns: 161 entries, date to acquisition_info dtypes: float64(77), int64(6), object(78) memory usage: 860.5 MB . We can see the data has 171907 rows and 161 columns and 859.4 MB memory. Let&#39;s see how much we can optimize dtype_diet. . proposed_df = report_on_dataframe(df_gamelogs, unit=&quot;MB&quot;) proposed_df . Current dtype Proposed dtype Current Memory (MB) Proposed Memory (MB) Ram Usage Improvement (MB) Ram Usage Improvement (%) . Column . date int64 | int32 | 671.574219 | 335.818359 | 335.755859 | 49.995347 | . number_of_game int64 | int8 | 671.574219 | 84.001465 | 587.572754 | 87.491857 | . day_of_week object | category | 5036.400391 | 84.362793 | 4952.037598 | 98.324939 | . v_name object | category | 5036.400391 | 174.776367 | 4861.624023 | 96.529736 | . v_league object | category | 4952.461426 | 84.359375 | 4868.102051 | 98.296617 | . ... ... | ... | ... | ... | ... | ... | . h_player_9_id object | category | 4955.471680 | 412.757324 | 4542.714355 | 91.670675 | . h_player_9_name object | category | 5225.463379 | 421.197266 | 4804.266113 | 91.939523 | . h_player_9_def_pos float64 | float16 | 671.574219 | 167.940430 | 503.633789 | 74.993020 | . additional_info object | category | 2714.671875 | 190.601074 | 2524.070801 | 92.978854 | . acquisition_info object | category | 4749.209961 | 84.070801 | 4665.139160 | 98.229794 | . 161 rows × 6 columns . new_df = optimize_dtypes(df_gamelogs, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 79.04368686676025 MB . # sell_prices.csv.zip Source data: https://www.kaggle.com/c/m5-forecasting-uncertainty/ df = pd.read_csv(&#39;../data/sell_prices.csv.zip&#39;) . proposed_df = report_on_dataframe(df, unit=&quot;MB&quot;) new_df = optimize_dtypes(df, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA . print(f&#39;Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB&#39;) print(f&#39;Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB&#39;) . Original df memory: 860.500262260437 MB Propsed df memory: 85.09655094146729 MB . ## hide ## collapse-hide .",
            "url": "https://mediumnok.ml/python/pandas/optimization/2020/11/10/Pandas-memory-optimization.html",
            "relUrl": "/python/pandas/optimization/2020/11/10/Pandas-memory-optimization.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Recreating the BBC style graphic in Python - `plotnine` and `altair`",
            "content": "Todo . [ ] Missing Subtitle | [ ] Missing Style | . Difference between plotnine and ggplot . 99% of them are the same, except that in python you have to wrap column names in &#39;&#39;, otherwise it will be treated as variable and caused error. Most of the time you just need to wrap a &#39;&#39; or replaced with _ depends on the function. . I tried to produce the same chart with plotnine and altair, and hopefully you will see their difference. plotnine covers 99% of ggplot2, so if you are coming from R, just go ahead with plotnine! altair is another interesting visualization library that base on vega-lite, therefore it can be integrated with website easily. In addition, it can also produce interactive chart with very simple function, which is a big plus! . Setup . #collapse-hide !pip install plotnine[all] !pip install altair !pip install gapminder %matplotlib inline import plotnine import pandas as pd import altair as alt from plotnine import ggplot # https://plotnine.readthedocs.io/en/stable/ from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap, geom_line from plotnine import * from plotnine.data import mtcars from gapminder import gapminder . . print(f&#39;altair version: {alt.__version__}&#39;) print(f&#39;plotnine version: {plotnine.__version__}&#39;) print(f&#39;pandas version: {pd.__version__}&#39;) . altair version: 4.0.1 plotnine version: 0.6.0 pandas version: 0.25.1 . Plotnine Example . (ggplot(mtcars, aes(&#39;wt&#39;, &#39;mpg&#39;, color=&#39;factor(gear)&#39;)) + geom_point() + stat_smooth(method=&#39;lm&#39;) + facet_wrap(&#39;~gear&#39;)) . C: ProgramData Anaconda3 lib site-packages numpy core fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) . &lt;ggplot: (-9223371916406847624)&gt; . Make a Line Chart . ggplot . line_df &lt;- gapminder %&gt;% filter(country == &quot;Malawi&quot;) #Make plot line &lt;- ggplot(line_df, aes(x = year, y = lifeExp)) + geom_line(colour = &quot;#1380A1&quot;, size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in Malawi 1952-2007&quot;) . plotnine . (ggplot(line_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;)) + geom_line(colour=&#39;#1380A1&#39;, size=1) + geom_hline(yintercept = 0, size = 1, colour=&#39;#333333&#39;) + labs(title=&#39;Living longer&#39;, subtitle = &#39;Life expectancy in Malawi 1952-2007&#39;) ) . &lt;ggplot: (-9223371916406567792)&gt; . ## altair line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in Malawi 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;y&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;y:Q&#39;) line + hline . The BBC style . function () { font &lt;- &quot;Helvetica&quot; ggplot2::theme(plot.title = ggplot2::element_text(family = font, size = 28, face = &quot;bold&quot;, color = &quot;#222222&quot;), plot.subtitle = ggplot2::element_text(family = font, size = 22, margin = ggplot2::margin(9, 0, 9, 0)), plot.caption = ggplot2::element_blank(), legend.position = &quot;top&quot;, legend.text.align = 0, legend.background = ggplot2::element_blank(), legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), legend.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.title = ggplot2::element_blank(), axis.text = ggplot2::element_text(family = font, size = 18, color = &quot;#222222&quot;), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)), axis.ticks = ggplot2::element_blank(), axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major.y = ggplot2::element_line(color = &quot;#cbcbcb&quot;), panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), strip.background = ggplot2::element_rect(fill = &quot;white&quot;), strip.text = ggplot2::element_text(size = 22, hjust = 0)) } &lt;environment: namespace:bbplot&gt; . The finalise_plot() function does more than just save out your chart, it also left-aligns the title and subtitle as is standard for BBC graphics, adds a footer with the logo on the right side and lets you input source text on the left side. . altair . ## altair line = (alt.Chart(line_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) line + hline . Make a multiple line chart . ggplot . #Prepare data multiple_line_df &lt;- gapminder %&gt;% filter(country == &quot;China&quot; | country == &quot;United States&quot;) #Make plot multiple_line &lt;- ggplot(multiple_line_df, aes(x = year, y = lifeExp, colour = country)) + geom_line(size = 1) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + scale_colour_manual(values = c(&quot;#FAAB18&quot;, &quot;#1380A1&quot;)) + bbc_style() + labs(title=&quot;Living longer&quot;, subtitle = &quot;Life expectancy in China and the US&quot;) . plotnine . # Make plot multiline = ( ggplot(multiline_df, aes(x=&#39;year&#39;, y=&#39;lifeExp&#39;, fill=&#39;country&#39;)) + geom_line(colour=&quot;#1380A1&quot;, size=1) + geom_hline(yintercept=0, size=1, color=&quot;#333333&quot;) + scale_colour_manual(values=[&quot;#FAAB18&quot;, &quot;#1380A1&quot;]) + # bbc_style() + labs(title=&quot;Living longer&quot;, subtitle=&quot;Life expectancy in China 1952-2007&quot;)) multiline . C: ProgramData Anaconda3 lib site-packages plotnine guides guides.py:200: PlotnineWarning: Cannot generate legend for the &#39;color&#39; aesthetic. Make sure you have mapped a variable to it PlotnineWarning) . &lt;ggplot: (-9223371916406562192)&gt; . altair . multiline_altair = (alt.Chart(multiline_df).mark_line().encode( x=&#39;year&#39;, y=&#39;lifeExp&#39;, color=&#39;country&#39;) .properties(title={&#39;text&#39;: &#39;Living Longer&#39;, &#39;subtitle&#39;: &#39;Life expectancy in China 1952-2007&#39;}) ) # hline overlay = overlay = pd.DataFrame({&#39;lifeExp&#39;: [0]}) hline = alt.Chart(overlay).mark_rule(color=&#39;#333333&#39;, strokeWidth=3).encode(y=&#39;lifeExp:Q&#39;) multiline_altair + hline . Make a bar chart . ggplot . #Prepare data bar_df &lt;- gapminder %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(desc(lifeExp)) %&gt;% head(5) #Make plot bars &lt;- ggplot(bar_df, aes(x = country, y = lifeExp)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle = &quot;Highest African life expectancy, 2007&quot;) . ## hide bar_df = gapminder.query(&#39; year == 2007 &amp; continent == &quot;Africa&quot; &#39;).nlargest(5, &#39;lifeExp&#39;) . plotnine . bars_ggplot = (ggplot(bar_df, aes(x=&#39;country&#39;, y=&#39;lifeExp&#39;)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;, fill=&quot;#1380A1&quot;) + geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + # bbc_style() + labs(title=&quot;Reunion is highest&quot;, subtitle=&quot;Highest African life expectancy, 2007&quot;)) bars_ggplot . &lt;ggplot: (-9223371916405111540)&gt; . altair . bars_altair = (alt.Chart(bar_df).mark_bar().encode( x=&#39;country&#39;, y=&#39;lifeExp&#39;, # color=&#39;country&#39; ) .properties(title={&#39;text&#39;: &#39;Reunion is highest&#39;, &#39;subtitle&#39;: &#39;Highest African life expectancy, 2007&#39;}) ) bars_altair . Make a stacked bar chart . Data preprocessing . ## collapse-hide stacked_bar_df = ( gapminder.query(&#39; year == 2007&#39;) .assign( lifeExpGrouped=lambda x: pd.cut( x[&#39;lifeExp&#39;], bins=[0, 50, 65, 80, 90], labels=[&quot;under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;])) .groupby( [&#39;continent&#39;, &#39;lifeExpGrouped&#39;], as_index=True) .agg({&#39;pop&#39;: &#39;sum&#39;}) .rename(columns={&#39;pop&#39;: &#39;continentPop&#39;}) .reset_index() ) stacked_bar_df[&#39;lifeExpGrouped&#39;] = pd.Categorical(stacked_bar_df[&#39;lifeExpGrouped&#39;], ordered=True) stacked_bar_df.head(6) . continent lifeExpGrouped continentPop . 0 | Africa | under 50 | 376100713.0 | . 1 | Africa | 50-65 | 386811458.0 | . 2 | Africa | 65-80 | 166627521.0 | . 3 | Africa | 80+ | NaN | . 4 | Americas | under 50 | NaN | . 5 | Americas | 50-65 | 8502814.0 | . ggplot . #prepare data stacked_df &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% mutate(lifeExpGrouped = cut(lifeExp, breaks = c(0, 50, 65, 80, 90), labels = c(&quot;Under 50&quot;, &quot;50-65&quot;, &quot;65-80&quot;, &quot;80+&quot;))) %&gt;% group_by(continent, lifeExpGrouped) %&gt;% summarise(continentPop = sum(as.numeric(pop))) #set order of stacks by changing factor levels stacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped))) #create plot stacked_bars &lt;- ggplot(data = stacked_df, aes(x = continent, y = continentPop, fill = lifeExpGrouped)) + geom_bar(stat = &quot;identity&quot;, position = &quot;fill&quot;) + bbc_style() + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_d(direction = -1) + geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;% of population by life expectancy band, 2007&quot;) + theme(legend.position = &quot;top&quot;, legend.justification = &quot;left&quot;) + guides(fill = guide_legend(reverse = TRUE)) . plotnine . # create plot stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . C: ProgramData Anaconda3 lib site-packages plotnine scales scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction` warn(msg.format(self.__class__.__name__, k), PlotnineWarning) C: ProgramData Anaconda3 lib site-packages plotnine layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values. data = self.position.setup_data(self.data, params) . &lt;ggplot: (-9223371916391151364)&gt; . # create plot stacked_bar_ggplot = ( ggplot(stacked_bar_df, aes(x=&#39;continent&#39;, y=&#39;continentPop&#39;, fill=&#39;lifeExpGrouped&#39;) ) + geom_bar(stat=&quot;identity&quot;, position=&quot;fill&quot;) + # bbc_style() + scale_y_continuous(labels=lambda l: [&quot;%d%%&quot; % (v * 100) for v in l]) + scale_fill_cmap_d(direction=-1) + # scale_fill_viridis_d geom_hline(yintercept=0, size=1, colour=&quot;#333333&quot;) + labs(title=&quot;How life expectancy varies&quot;, subtitle=&quot;% of population by life expectancy band, 2007&quot;) + guides(fill=guide_legend(reverse=True))) stacked_bar_ggplot . altair . stacked_bar_altair = ( alt.Chart(stacked_bar_df) .mark_bar() .encode(x=&#39;continent&#39;, y=alt.Y(&#39;continentPop&#39;, stack=&#39;normalize&#39;, axis=alt.Axis(format=&#39;%&#39;)), fill=alt.Fill(&#39;lifeExpGrouped&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;))) .properties(title={&#39;text&#39;: &#39;How life expectancy varies&#39;, &#39;subtitle&#39;: &#39;% of population by life expectancy band, 2007&#39;} ) ) overlay = overlay = pd.DataFrame({&#39;continentPop&#39;: [0]}) hline = alt.Chart(overlay).mark_rule( color=&#39;#333333&#39;, strokeWidth=2).encode(y=&#39;continentPop:Q&#39;) stacked_bar_altair + hline . Make a grouped bar chart . ggplot . #Prepare data grouped_bar_df &lt;- gapminder %&gt;% filter(year == 1967 | year == 2007) %&gt;% select(country, year, lifeExp) %&gt;% spread(year, lifeExp) %&gt;% mutate(gap = `2007` - `1967`) %&gt;% arrange(desc(gap)) %&gt;% head(5) %&gt;% gather(key = year, value = lifeExp, -country, -gap) #Make plot grouped_bars &lt;- ggplot(grouped_bar_df, aes(x = country, y = lifeExp, fill = as.factor(year))) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_fill_manual(values = c(&quot;#1380A1&quot;, &quot;#FAAB18&quot;)) + labs(title=&quot;We&#39;re living longer&quot;, subtitle = &quot;Biggest life expectancy rise, 1967-2007&quot;) . plotnine . altair . Make a dumbbell chart . ## hide . ggplot . hist_df &lt;- gapminder %&gt;% filter(year == 2007) ggplot(hist_df, aes(lifeExp)) + geom_histogram(binwidth = 5, colour = &quot;white&quot;, fill = &quot;#1380A1&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + bbc_style() + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, by = 10), labels = c(&quot;40&quot;, &quot;50&quot;, &quot;60&quot;, &quot;70&quot;, &quot;80&quot;, &quot;90 years&quot;)) + labs(title = &quot;How life expectancy varies&quot;, subtitle = &quot;Distribution of life expectancy in 2007&quot;) . plotnine . altair . Make changes to the legend . ## hide . ggplot . . plotnine . altair . Make changes to the axes . ## hide . ggplot . . plotnine . altair . Add annotations . ## hide . ggplot . . plotnine . altair . Work with small multiples . ## hide . ggplot . . plotnine . altair . Do something else entirely . ## hide . ggplot . . plotnine . altair .",
            "url": "https://mediumnok.ml/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "relUrl": "/python/2020/04/13/Recreating-the-BBC-graphs-in-Python-plotnine-altair.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Beyond Unit Testing - What is Property-based Testing?",
            "content": "#collapse-hide # https://hypothesis.readthedocs.io/en/latest/quickstart.html !pip install hypothesis %load_ext ipython_pytest . . Requirement already satisfied: hypothesis in c: programdata anaconda3 lib site-packages (5.8.1) Requirement already satisfied: sortedcontainers&lt;3.0.0,&gt;=2.1.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (2.1.0) Requirement already satisfied: attrs&gt;=19.2.0 in c: programdata anaconda3 lib site-packages (from hypothesis) (19.2.0) The ipython_pytest extension is already loaded. To reload it, use: %reload_ext ipython_pytest . Unit Testing is a common technique for software engineering. Even if you are not writing a unit test explicitly, you are still doing unit testing, as your function should at least works for what you intended. You give an input x to a function, it should return y, simple as that. . For example, imagine we have a function like this. . def add_ints(x1, x2): return x1 + x2 . # Case 1 add_ints(1,1) . 2 . # Case 2 add_ints(1,&#39;2&#39;) . TypeError Traceback (most recent call last) &lt;ipython-input-59-99ea9d9c8984&gt; in &lt;module&gt; 1 # Case 2 -&gt; 2 add_ints(1,&#39;2&#39;) &lt;ipython-input-57-d4599be2ffda&gt; in add_ints(x1, x2) 1 def add_ints(x1, x2): -&gt; 2 return x1 + x2 TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;str&#39; . # Case 3 add_ints(&#39;2&#39;, &#39;2&#39;) . The first two cases are expected behaviors, but the last case is a side-effect of how Python works. We should probably checks the input are numbers, otherwise we should throw error explicitly. Now, checking function behave properly with intend use is easy, to test the opposite is much harder. You have to test a lot of edge case, which is much harder and make your test verbose. . In this article, I will introduce a library called Hypothesis that does property-based testing. If none of this make sense to you, please bare with me, I will explain with simple examples. I found the name of Hypothesis and property-based testing isn&#39;t adding a lot of information, but they are useful. . Hypothesis comes in handy that it generated artificial input to make your test fails. Instead of specifying an input, you specify what kind of input you want to test loosely. For example, if you expect your input is number, often you may want to test when the value is negative, positive, a floating point number, or if it exceeds certain range. This list of condition can expands quickly, and Hypothesis make this easier. . Start with a simple function . Let&#39;s stick with our simple add_ints function above. To keep it simple, let test for this 3 cases first. . Adding two number -&gt; Expect Pass | Adding number and string -&gt; Expect Fail | Adding two number -&gt; Expect Fail | %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): return x1 + x2 def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmp79u2a6x6 plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xF [100%] ================================== FAILURES =================================== ____________________________ test_add_ints_string _____________________________ [XPASS(strict)] =================== 1 failed, 1 passed, 1 xfailed in 0.12s ==================== . In pytest, you can use a mark @pytest.mark.xfail to annotate a function is expected to fail the test. We have 1 pass, 1xfailed, 1 failed. . _ipytesttmp.py .xF indicates the last test is failed. Let&#39;s try to fix it by throwing an error is input type is not a number. . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) def test_add_ints(): assert add_ints(1,1) == 2 @pytest.mark.xfail() def test_add_ints_fail(): assert add_ints(1,&#39;2&#39;) @pytest.mark.xfail(strict=True) def test_add_ints_string(): assert add_ints(&#39;2&#39;, &#39;2&#39;) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpfrh2uipy plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 3 items _ipytesttmp.py .xx [100%] ======================== 1 passed, 2 xfailed in 0.10s ========================= . Okay, now we checks if input are integers. In reality, this if often an iterative process. You start with coming up with test cases, then every now and then, you hit some edge cases and you add that into your collections of test cases. . How can we make out test cases more robust to input? Hypothesis is exactly the tool you need. . strategy, your auto-genenerated input for unit test . strategy is your input for unit test. Instead of specify a number, or a string, you specify what kind of input you want, and Hypothesis wouuld take care the rest of it. You can even composite different strategies to form more complicated input. . But let&#39;s keep it simple, we would just use integer for this demo. . from hypothesis import strategies as st . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpmdbi_h6f plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py F [100%] ================================== FAILURES =================================== ________________________________ test_add_ints ________________________________ @given(st.integers(), st.integers()) &gt; def test_add_ints(x1, x2): _ipytesttmp.py:15: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ x1 = 0, x2 = 0 @given(st.integers(), st.integers()) def test_add_ints(x1, x2): &gt; assert add_ints(x1, x2) E assert 0 E + where 0 = add_ints(0, 0) _ipytesttmp.py:16: AssertionError Hypothesis - Falsifying example: test_add_ints( x1=0, x2=0, ) ============================== 1 failed in 0.30s ============================== . The test was simple, as should pass as long as no error was thrown. Look what Hypothesis found, it found when both x1, x2=0, the assertion will fail, because we are asserting 0 + 0 = 0, thus evaluated as False in Python. . Hence, I modified my test to not assert anything, it should just keep silent as long as no error is thrown. . @given(st.integers(), st.integers()) def test_add_ints(x1, x2): assert add_ints(x1, x2) . %%pytest # ipython magic to run pytest within a cell. This whole blog is written in a Jupyter Notebook! # https://github.com/akaihola/ipython_pytest/blob/master/ipython_pytest.py import pytest from hypothesis import given from hypothesis import strategies as st def add_ints(x1, x2): if isinstance(x1, int) and isinstance(x2, int): return x1 + x2 else: raise TypeError(f&#39;Make sure your input is a number x1 {type(x1)}, x2 {type(x2)}&#39;) @given(st.integers(), st.integers()) def test_add_ints(x1, x2): add_ints(x1, x2) . ============================= test session starts ============================= platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0 rootdir: C: Users CHANNO AppData Local Temp tmpwicd08ny plugins: hypothesis-5.8.1, arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2 collected 1 item _ipytesttmp.py . [100%] ============================== 1 passed in 0.25s ============================== . Yes, now our test finally pass. .",
            "url": "https://mediumnok.ml/python/2020/04/12/Property-based-testing-in-Python.html",
            "relUrl": "/python/2020/04/12/Property-based-testing-in-Python.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Hey Data Scientist, Use a Larger Font Size",
            "content": ". In reality, you probably don&#39;t need a title as big as this one. But using library defautls often is not the best choice. . #collapse-hide def make_scatter_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.scatter(x, y) ax.set_title(&#39;A Simple Scatter Plot&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() def make_line_plot(): num_points = 100 gradient = 0.5 x = np.array(range(num_points)) y = np.random.randn(num_points) * 10 + x * gradient fig, ax = plt.subplots() ax.plot(x, y, &#39;-&#39;) ax.set_title(&#39;A Simple Line Chart&#39;) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) plt.show() . . The Problem of library defaults . make_scatter_plot() make_line_plot() . Your insight is as best as your audience understand. Data Scientist spends a lot of time to drill insight from data, but not enough time to present their insight. Unfortunately, human perception is largely based on visual, a easy-to-read chart is much more likely to sell your idea with a custom matplotlib pyplot chart. There is nothing wrong with matplotlib, it is custom for a user sit in front of a monitor. When it comes to presentation, you really should make some adjustment for your audience. Luckily, it is easy to do with the following tips. . Apply matplotlib theme . with plt.style.context(&#39;ggplot&#39;): # Or plt.style.use(&#39;presentation&#39;) for global setting make_scatter_plot() make_line_plot() . Much better right? . There is nothing wrong with the chart if you are viewing it in front of your monitor. However, you may not want to put it directly into your PowerPoint. . Make PowerPoint-ready charts . Luckily, there is some easy way to prepare PowerPoint-ready charts. I created a presentation.mplstyle file as follow. . Custom presentation theme . axes.titlesize : 24 axes.labelsize : 24 axes.location: &#39;left&#39; lines.linewidth : 3 lines.markersize : 10 xtick.labelsize : 18 ytick.labelsize : 18 figure.figsize : 10, 6 figure.titlesize: 24 . with plt.style.context([&#39;presentation&#39;, &#39;ggplot&#39;]): make_scatter_plot() make_line_plot() . If you are careful enough, you will notice the font size of the title is not correct. This is because ggplot theme overwrite my theme. To make it right, you just need to switch the order so that your theme will overwrite conflict settings. . with plt.style.context([&#39;ggplot&#39;, &#39;presentation&#39;]): make_scatter_plot() make_line_plot() . I actually disable the grid in my presentation theme, which conflicts with fivethirtyeight configuration. If conflict configs exist, it resolved base on your order. See the same plot with &#39;presentation&#39;,&#39;fivethirtyeight&#39; in reverse order. . To give you a sense how this affect your presenation, I put it into a Powerpoint, see if you feel the difference. . . . Avoid Low Resolution Chart . . Note: Believe it or not, a low resolution chart looks much less conviencing. Taking screenshot with larger charts helps you to preserve the resolution. . Resolution of the chart is much better | More obvious Title &amp; Label (Try take a few step back from your monitor, see if you can read it) | . Define Once, Use Everywhere . It could be troublesome if you need to define the same file over and over in different computer/environment. You can actually use a URL. I have put my own theme in GitHub so I can always access it from anywhere. . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Bad key &#34;font.name&#34; on line 9 in https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template or from the matplotlib source distribution . Conclusion . I hope this blog helps you to prepare Powerpoint-ready charts better, happy coding! .",
            "url": "https://mediumnok.ml/python/2020/04/10/Presentation-Ready-Chart.html",
            "relUrl": "/python/2020/04/10/Presentation-Ready-Chart.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
            "content": "I have teamed up with a friend to participate in the Bengali Image Classification Competition. We struggled to get a high rank in the Public leaderboard throughout the competition. In the end, the result is a big surprise to everyone as the leaderboard shook a lot. . . . The final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard. . The Classification Task . This is an image classification competition. We need to predict 3 parts of Bengali characters root, consonant and vowel. It is a typical classification tasks like the MNIST dataset. . . Evaluation Metrics . The competition use macro-recall as the evaluation metric. In general, people get &gt;96% recall in training, the tops are even getting &gt;99% recall. . # collapse-hide python import numpy as np import sklearn.metrics scores = [] for component in [&#39;grapheme_root&#39;, &#39;consonant_diacritic&#39;, &#39;vowel_diacritic&#39;]: y_true_subset = solution[solution[component] == component][&#39;target&#39;].values y_pred_subset = submission[submission[component] == component][&#39;target&#39;].values scores.append(sklearn.metrics.recall_score( y_true_subset, y_pred_subset, average=&#39;macro&#39;)) final_score = np.average(scores, weights=[2,1,1]) . . Model (Bigger still better) . We start with xresnet50, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast. . Near the end of the competition, we switch to a larger model se-resnext101. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with ~0.3-0.5% recall. It is a big improvement as the recall is very high (~0.97), in other words, it reduces ~10% error solely by just using a better model, not bad! . Augmentation . There are never &quot;enough&quot; data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation. We start with rotation + scale. We also find MixUp and CutMix is very effective to boost the performance. It also gives us roughly 10% boost initially from 0.96 -&gt; 0.964 recall. . CutMix &amp; MixUp . . Mixup is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2. . CutMix shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one. . It always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically. . Logging of Experiment . I normally just log my experiment with a simple CSV and some printing message. This start to get tedious when there are more than 1 people to work. It is important to communicate the results of experiments. I explore Hydra and wandb in this competition and they are very useful. . Hydra . It is often a good idea to make your experiment configurable. We use Hydra for this purpose and it is useful to compose different configuration group. By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model. . I have written an short example for how to use Hydra. . Wandb . wandb (Weight &amp; Biases) does a few things. It provides built-in functions that automatically log all your model statistics, you can also log your custom metrics with simple functions. . Compare the configuration of different experiments to find out the model with the best performance. | Built-in function for logging model weights and gradient for debugging purpose. | Log any metrics that you want | . All of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier. . . Stochastic Weight Averaging . This is a simple yet effective technique which gives about 0.3-0.4% boost to my model. In simple words, it takes snapshots of the model weights during training and takes an average at the end. It provides a cheap way to do models ensemble while you are only training 1 model. This is important for this competition as it allows me to keep training time short enough to allow feedback within hours and reduce over-fitting.) . . Larger is better (image size) . We downsample our image size to 128x128 throughout the competition, as it makes the model train faster and we believe most technique should be transferable to larger image size. It is important to keep your feedback loop short enough (hours if not days). You want your training data as small as possible while keeping them transferable to your full dataset. Once we scale our image to full size, it takes almost 20 hours to train a single model, and we only have little chance to tune the hyper-parameters before the competition end. . Debug &amp; Checkpoint . There was a time we develop our model separately and we didn&#39;t sync our code for a while. We refactor our code during the time and it was a huge mistake. It turns out our pre-refactor code trains much better model and we introduce some unknown bug. It is almost impossible to find out as we change multiple things. It is so hard to debug a neural network and testing it thoroughly is important. Injecting a large amount of code may help you to run an experiment earlier, but you may pay much more time to debug it afterwards. . I think this is applicable even if you are working alone. . Keep your changes small. | Establish a baseline early, always do a regression test after a new feature introduced (especially after code refactoring) | Create checkpoint to rollback anytime, especially if you are not working on it every day. | . Implementation is the key of Kaggle competition (in real life too). It does not matter how great your model is, a tiny little bug could have damaged your model silently . Use auxiliary label . As mentioned earlier, this competition requires to predict the root, vowel and the consonant part. In the training data, they actually provide the grapheme too. Lots of people saying that if you train with the grapheme, it improves the model greatly and get the recall &gt;98% easily. . This is something we could not reproduce throughout the competition, we tried it in the very last minute but it does not seem to improve our model. It turns out lots of people are overfitting the data, as the testing dataset has much more unseen character. . But it is still a great remark that training with labels that is not your final desired output could still be very useful. . Weight loss . The distribution of the training dataset is very imbalance, but to get a good result, we need to predict every single class accurately (macro recall). To deal with this issue, we choose to use class weights, where a higher weight would be applied to rare samples. We don&#39;t have an ablation study for this, but it seems to help close the gap between accuracy &amp; recall and allows us to train the model slightly better. . Find a teammate! . Lastly, please go and find a teammate if you can. It is very common to start a Kaggle competition, but not so easy to finish them. I have stopped for a month during the competition due to my job. It is really hard to get back to the competition after you stopped for so long. Getting a teammate helps to motivate you and in the end, it is a great learning experience for both of us. . Pretrain Model . We also tried to use a pretrained model, as it allows shorter training and gives better performance by transfer learning (Using weights learn from a large dataset to as initial weight). It also gives our model a bit of improvement. . Finetune the model head, while keeping other layers freeze (except BatchNorm layer). | Unfreeze the model, train all the layers together. | . I also tried training the model directly with discriminating learning rate while not freezing any layer at all. It performs similarly to freezing fine-tuning , so I end up just start training the entire model from the beginning. . If the code works, don&#39;t touch it . This is probably not a good habit usually, but I suggest not to do it for a competition. We spent lots of time for debugging our code after code refactoring and end up just rolling back to an older commit and cherry-picks new features. In a competition, you don&#39;t have enough time to test everything. You do not need a nice abstract class for all your features, some refactoring to keep your function/class clean is probably needed, but do not overspend your time on it. It is even common to jump between frameworks (you may find other&#39;s Kernel useful), so it is not possible to structure your code perfectly. . If someone has create a working submission script, use it! | If someone has create a working pre-processing function, use it! | . Don&#39;t spend time on trying to optimize these code unless it is necessary, it is often not worth it in a competition context. You should focus on adding new features, trying out new model, testing with new augmentation technique instead. . Summary . This is a great learning experience and refreshes some of my outdated computer vision model knowledge. If you have never joined a competition, find a friend and get started. If you have just finished one, try writing it out and share your experience. 😉 .",
            "url": "https://mediumnok.ml/ml/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "relUrl": "/ml/2020/03/21/10-lessons-learnt-from-Kaggle-competition.html",
            "date": " • Mar 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The missing piece in Python tutorial - What is dispatch why you should care",
            "content": "In python, we often think of it as a dynamic language, and type is barely noticed in Python as you can change the type of a variable whenever you want. . Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean. . In python, you cannot overload a normal function twice for different behavior base on the arguments. For example: . def foo(number:int ): print(&#39;it is a integer&#39;) def foo(number: float): print(&#39;it is a float&#39;) . foo(1) . it is a float . The definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument. . from functools import singledispatch @singledispatch def foo(number ): print(f&#39;{type(number)}, {number}&#39;) . foo(1) . &lt;class &#39;int&#39;&gt;, 1 . We can now register the function for different argument type. . @foo.register(int) def _(data): print(&#39;It is a integer!&#39;) @foo.register(float) def _(data): print(&#39;It is a float!&#39;) @foo.register(dict) def _(data): print(&#39;It is a dict!&#39;) . foo(1.0) foo(1) foo({&#39;1&#39;:1}) . It is a float! It is a integer! It is a dict! . How is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument. . It will fallback to the most generic function if the type of argument is not registered. . foo([1,2,3]) . &lt;class &#39;list&#39;&gt;, [1, 2, 3] . I hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that. . In next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too. . Fastai @typedispatch . Single Dispatch is great, but what if we can do multi dispatch for more than 1 argument? . from fastcore.dispatch import typedispatch, TypeDispatch . Let us first try if this work as expected . @typedispatch def add(x:int, y:int): return x+y @typedispatch def add(x:int, y:str): return x + int(y) . print(add(1,2)) print(add(1,&#39;2&#39;)) print(add(&#39;a&#39;,&#39;a&#39;)) . 3 3 a . add(1,2) . 3 . add(1,&#39;2&#39;) . 3 . But what if we added something does not define? . add(&#39;2&#39;,1) . &#39;2&#39; . &#39;2&#39;? where does it come from? Let&#39;s have a look at the definition of typedispatch and understand how it works. . ??typedispatch class DispatchReg: &quot;A global registry for `TypeDispatch` objects keyed by function name&quot; def __init__(self): self.d = defaultdict(TypeDispatch) def __call__(self, f): nm = f&#39;{f.__qualname__}&#39; self.d[nm].add(f) return self.d[nm] . In fact, typedispatch is not even a function, it&#39;s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg . type(typedispatch) . fastcore.dispatch.DispatchReg . typedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you. . def foo(): return &#39;foo&#39; a = foo def foo(): return &#39;not foo&#39; b = foo . foo() . &#39;not foo&#39; . foo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable. . a() . &#39;foo&#39; . b() . &#39;not foo&#39; . hex(id(a)), hex(id(b)) . (&#39;0x2b9d28bb5e8&#39;, &#39;0x2b9d2ebe048&#39;) . The two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition. . typedispatch.d . defaultdict(fastcore.dispatch.TypeDispatch, {&#39;cast&#39;: (object,object) -&gt; cast, &#39;add&#39;: (int,str) -&gt; add (int,int) -&gt; add}) . So back to our question, why does add(&#39;a&#39;,1) return &#39;a&#39;? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument. . def __call__(self, *args, **kwargs): ts = L(args).map(type)[:2] f = self[tuple(ts)] if not f: return args[0] if self.inst is not None: f = MethodType(f, self.inst) return f(*args, **kwargs) .",
            "url": "https://mediumnok.ml/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "relUrl": "/python/fastai/2020/02/22/Python-Dynamic-Dispatch.html",
            "date": " • Feb 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 | The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 | First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 | I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 | Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 | Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![123](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mediumnok.ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "data augmentation - Understand MixUp and Beta Distribution",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta . Understand Mixup Augmentation &amp; Beta Distribution . Implementation In the original article, the authors suggested three things: . Create two separate dataloaders and draw a batch from each at every iteration to mix them up | Draw a t value following a beta distribution with a parameter alpha (0.4 is suggested in their article) | Mix up the two batches with the same value t. | Use one-hot encoded targets | Source: https://forums.fast.ai/t/mixup-data-augmentation/22764 (Sylvain Gugger) . Beta Distribution . Beta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat . To get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect . import math import torch import matplotlib.pyplot as plt from torch import tensor . # PyTorch has a log-gamma but not a gamma, so we&#39;ll create one Γ = lambda x: x.lgamma().exp() facts = [math.factorial(i) for i in range(7)] plt.plot(range(7), facts, &#39;ro&#39;) plt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1)) plt.legend([&#39;factorial&#39;,&#39;Γ&#39;]); . . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [5.0,1.0,0.4, 1.0] b_ls = [1.0,5.0,0.4, 1.0] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α != β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . A few observations from this graph. . α and β control the curve symmetrically, the blue line is symmetric with the orange line. | when α and β = 1, it reduce to uniform distribution | when α = β, the distribution is a symmetric distribution | . When α != β . _,ax = plt.subplots(1,1, figsize=(5,4)) x = torch.linspace(0.01,0.99, 100000) a_ls = [0.1, 0.4, 0.6, 0.9] b_ls = [0.1, 0.4, 0.6, 0.9] for a, b in zip(a_ls, b_ls): a=tensor(a,dtype=torch.float) b=tensor(b,dtype=torch.float) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(a-1) * (1-x)**(b-1)) / (Γ(a)*Γ(b) / Γ(a+b)) ax.plot(x,y) # ax.set_title(f&quot;α={a.numpy()[0]:.1}&quot;) ax.set_title(&#39;Beta distribution when α = β &#39;) ax.legend([f&#39;α = {float(a):.2}, β = {float(b):.2}&#39; for a,b in zip(a_ls, b_ls)]) . C: ProgramData Anaconda3 envs fastai2 lib site-packages IPython core pylabtools.py:132: UserWarning: Creating legend with loc=&quot;best&quot; can be slow with large amounts of data. fig.canvas.print_figure(bytes_io, **kw) . . As we remember, when α = β =1, it is an uniform distribution. When α = β , when α is small, most density is concentrated around 0 and 1, and when α increase, the distribution get more evenly distributed. . The default for α suggested by the paper is 0.4 .",
            "url": "https://mediumnok.ml/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "relUrl": "/ml/2020/02/09/MixUp-and-Beta-Distribution.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Hydra - Config Composition for Machine Learning Project",
            "content": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example . Machine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations. . Assume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure. . Folder Structure . config.yaml demo.py run_mode - train.yaml - test.yaml hyperparmeter - base.yaml . config.yaml . defaults: - run_mode: train - hyperparameter: base . The benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance. . import hydra from omegaconf import DictConfig @hydra.main(config_path=&quot;config.yaml&quot;) def my_app(cfg : DictConfig) -&gt; None: print(cfg.pretty()) if __name__ == &quot;__main__&quot;: my_app() . python demo.py . gamma: 0.01 learning_rate: 0.01 run_mode: train week: 8 . For example, with a simple example with 4 parameters only, you can simply run the experiment with default . Override default parameters . You can easily overrite the learning rate with an argument, it would be very clear that learning rate is the only changing parameter with this approach . python demo.py learning_rate=0.1 . gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 . In somecase, you may only need to test a model instead of changing it. . python demo.py learning_rate=0.1 run_mode=test . gamma: 0.01 learning_rate: 0.1 run_mode: test week: 8 . It also safeguard your experiment if you pass in some parameters that is not exist . !python demo.py typo=0.2 . Traceback (most recent call last): File &quot;demo.py&quot;, line 7, in &lt;module&gt; my_app() &quot;C: ProgramData Anaconda3 lib site-packages omegaconf dictconfig.py&quot;, line 41, in __setitem__ &quot;Accessing unknown key in a struct : {}&quot;.format(self.get_full_key(key)) KeyError: &#39;Accessing unknown key in a struct : typo&#39; . –Multirun, Combination of parameters . In case you want to gridsearch paramters, which is very common in machine learning, you can use an additional argument multirun to do that easily. . !python demo.py --multirun learning_rate=0.1,0.01,0.001 gamma=0.1,0.01 . [2020-02-08 19:28:46,095][HYDRA] Sweep output dir : multirun/2020-02-08/19-28-46 [2020-02-08 19:28:46,102][HYDRA] Launching 6 jobs locally [2020-02-08 19:28:46,103][HYDRA] #0 : learning_rate=0.1 gamma=0.1 gamma: 0.1 learning_rate: 0.1 run_mode: train week: 8 [2020-02-08 19:28:46,192][HYDRA] #1 : learning_rate=0.1 gamma=0.01 gamma: 0.01 learning_rate: 0.1 run_mode: train week: 8 ... SKIPPED .",
            "url": "https://mediumnok.ml/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "relUrl": "/python/ml/2020/02/08/Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
            "date": " • Feb 8, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mediumnok.ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "plyer - Desktop Notification in Python",
            "content": "from plyer import notification import random class DesktopNotification: @staticmethod def notify(title=&#39;Hey~&#39;, message=&#39;Done!&#39;, timeout=10): ls = [&#39;👍&#39;,&#39;✔&#39;,&#39;✌&#39;,&#39;👌&#39;,&#39;👍&#39;,&#39;😎&#39;] notification.notify( title = title , message = random.choice(ls) * 3 + &#39; &#39; + message, timeout = timeout # seconds ) if __name__ == &#39;__main__&#39;: DesktopNotification.notify() . You could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window. .",
            "url": "https://mediumnok.ml/python/2019/10/19/Deskto-Notification.html",
            "relUrl": "/python/2019/10/19/Deskto-Notification.html",
            "date": " • Oct 19, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "",
          "content": "README . Introduction . . https://noklam.github.io/mediumnok/ . Not a Data Scientist . powered by fastpages . fastpages allow me to blog directly in Notebook, so I don’t have to worry how to convert into markdown anymore. I simple code and write. It also migrate my old markdown post with docusaurus seamlessly. . I find that I have generated way too many repository and notebook, I hope this can become a central repository of my daily exploration, and hopefully I can search my own stuff on Google eventually (Which is faster thatn my Window Explorer). The blog is generated by Docusaurus ✔ with Markdown only. This blog is mean to be a logging journal of myself. I will keep the polished article on Medium Medium). If you are interested to create a repository for yourself, you can follow my steps Instruction . Resource . I am generally interested in tools that increase productivity, please let me know if you have any recommendations. Here is a list of software/topics that I found useful. . Uncertainty Estimation . Uncertainty Quantification in Deep Learning . Visualization . Visualization (University of Washington) . Custom Matplotlib style for Presentation (Larger font size) . https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle . my_style = &#39;https://raw.githubusercontent.com/noklam/mediumnok/master/_demo/python-viz/presentation.mplstyle&#39; with plt.style.context([&#39;ggplot&#39;, my_style]): make_scatter_plot() make_line_plot() . Useful Python Tools . pyinstructment: for profiling python process, which is useful for optimization | torchsnooper -&gt; pytorch profiling, another profiling tool which is for PyTorch, no more print x.shape anymore. | knockknock notification: A single line of code that get you notifications when your 10 hours model training finally done. No more starring at the progress bar. | colorama: Colored printing in terminal (cross platform) | Hypoehsis - Property-based testing, autogenerated input for unit-test. . Reviewing (any suggestions for code metric report/analysis library are welcome!) . | coala - coala provides a unified command-line interface for linting and fixing all your code, regardless of the programming languages you use. | radon - Radon is a Python tool that computes various metrics from the source code | . Syntax Highlight . lunr.js | . A catalog of various machine learning topics. . Graph Neural Network Basics Understand What is the weird D-1/2LD-1/2 | Supplement Chinese Reading | . | Time Series Forecast Motivation | Forecasting Methods Statistical Method | Machine Learning | Deep Neural Network | . | . | Prediction Interval Python Time Series Forecasting Library | . | Contribution | Under Review | . Graph Neural Network Basics . Understand What is the weird D-1/2LD-1/2 . spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | spectral graph theory - Why Laplacian Matrix need normalization and how come the sqrt of Degree Matrix? - Mathematics Stack Exchange | What’s the intuition behind a Laplacian matrix? I’m not so much interested in mathematical details or technical applications. I’m trying to grasp what a laplacian matrix actually represents, and what aspects of a graph it makes accessible. - Quora | Supplement Chinese Reading . Heat Diffusion | GCN use edge to agg node information | How to do batch training with GCN | Time Series Forecast . Motivation . While neural network have gain a lot of success in NLP and computer vision, there are relatively less changes for traditional time series forecasting. This repository aims to study the lastest practical technique for time series prediction, with either statistical method, machine learning or deep neural network. I will also try to summarize practical solution from Kaggles. . Forecasting Methods . Statistical Method . Machine Learning . Deep Neural Network . Gramian Angular Field : Transform time series into an image and use transfer learning with CNN . Prediction Interval . While forecasting accuracy is important, the prediction interval is also important and it is an area that the machine learning world has less focus on. . Traditional statistical forecast (ARIMA, ETS etc) | Bayesian Neural Network | Random Forest jackknife approximation | MCDropout (Use Dropout at inference time as variation inference) | Quantile Regression | VOGN (Optimizer weight pertubation) | Random Forest jackknife appoximation | . Python Time Series Forecasting Library . Prophet (Facebook): Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth. It has build-in modelling for Holiday effect. . pyts : state-of-the-art algorithms for time series transformation and classification . Contribution . Feel free to send a PR or discuss by staring an issue.😁 . Under Review .",
          "url": "https://mediumnok.ml/README.html",
          "relUrl": "/README.html",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mediumnok.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "",
          "content": "Explainable AI . Introduction . Personal collecions of model interpretation utilities. . This repository include general model interpretation methods. Most articles focus on library, but the method are actually general than that. For example, we can use partial indepedence for deep learning model too. You will find most tutorials out there are only using it on Tree. . Same applied on SHAP, many tutorials use Tree as example, but the library actually support much more general algorithm. Your feature don’t even have to be a column. . Partial Depedence Plot | SHAP | Tensorflow (What-if tools)[https://pair-code.github.io/what-if-tool/age.html] | Counterfactual (Most simliar data point, but very different inference value) | Model interpretation . Model interpretation can be divided into local or global. Different method are complementary instead of replacement. For example, SHAP give you an idea what features are important for a particular prediction. But Partial Dependence plot supplement the “What-if” condition, namely, how will your prediction changes when a dependent variable changes. These are two different information which are often overlooked. . Partial Dependence . Local | Global | . Partial depedence can be applied in row level or dataset level. It gives you a sense that how Change of a feature will change the model output accordingly. . We can also “zoom in” if we want, say using a subset of data (e.g. at country level) or even at row level to dig into the model. . The what-if tool allows you to change the value of a feature and run inference. Partial dependence is doing the exact same thing except it run multiple prediction by changing one features to different values to obtain a continuous plot. . . SHAP . Squashing function (e.g. log transformation of Target variable) | . It can affect the “feature importance” as it will change the order of feature importance even with a monotonic transformation .",
          "url": "https://mediumnok.ml/explainable-ai.html",
          "relUrl": "/explainable-ai.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mediumnok.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}