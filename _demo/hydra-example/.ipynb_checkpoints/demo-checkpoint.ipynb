{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defaults Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "defaults:\n",
    " - run_mode: train\n",
    " - hyperparameter: base\n",
    " \n",
    "week: 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Structure\n",
    "```\n",
    "config.yaml\n",
    "demo.py\n",
    "run_mode\n",
    "  - train.yaml\n",
    "  - test.yaml\n",
    "hyperparmeter\n",
    "  - base.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For typical machine learning project, you will have to handle a large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. __hydra__ is useful for composing different config. In essence, it \"union\" differnet config file and generate an ultimate conifg.\n",
    "\n",
    "The benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-95ee2247a870>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-95ee2247a870>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ```python\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "```python\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "@hydra.main(config_path=\"config.yaml\")\n",
    "def my_app(cfg : DictConfig) -> None:\n",
    "    print(cfg.pretty())\n",
    "if __name__ == \"__main__\":\n",
    "    my_app()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.01\n",
      "learning_rate: 0.01\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python demo.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with a simple example with 4 parameters only, you can simply run the experiment with default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# override parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily overrite the learning rate with an argument, it would be very clear that learning rate is the only changing parameter with this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.01\n",
      "learning_rate: 0.1\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python demo.py learning_rate=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In somecase, you may only need to test a model instead of changing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.01\n",
      "learning_rate: 0.1\n",
      "run_mode: test\n",
      "week: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python demo.py learning_rate=0.1 run_mode=test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also safeguard your experiment if you pass in some parameters that is not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"demo.py\", line 7, in <module>\n",
      "    my_app()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hydra\\main.py\", line 24, in decorated_main\n",
      "    strict=strict,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 174, in run_hydra\n",
      "    overrides=args.overrides,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 79, in run\n",
      "    config_file=config_file, overrides=overrides, with_log_configuration=True\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 343, in compose_config\n",
      "    config_file=config_file, overrides=overrides, strict=strict\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hydra\\_internal\\config_loader.py\", line 82, in load_configuration\n",
      "    cfg.merge_with_dotlist(remaining_overrides)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\omegaconf\\config.py\", line 278, in merge_with_dotlist\n",
      "    self.update(key, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\omegaconf\\config.py\", line 299, in update\n",
      "    root[last] = value\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\omegaconf\\dictconfig.py\", line 41, in __setitem__\n",
      "    \"Accessing unknown key in a struct : {}\".format(self.get_full_key(key))\n",
      "KeyError: 'Accessing unknown key in a struct : typo'\n"
     ]
    }
   ],
   "source": [
    "!python demo.py typo=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Multirun,  Combination of parameters\n",
    "In case you want to gridsearch paramters, which is very common in machine learning, you can use an additional argument __multirun__ to do that easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-02-08 19:25:51,080][HYDRA] Sweep output dir : multirun/2020-02-08/19-25-51\n",
      "[2020-02-08 19:25:51,086][HYDRA] Launching 6 jobs locally\n",
      "[2020-02-08 19:25:51,086][HYDRA] \t#0 : learning_rate=0.1 gamma=0.1\n",
      "gamma: 0.1\n",
      "learning_rate: 0.1\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n",
      "[2020-02-08 19:25:51,179][HYDRA] \t#1 : learning_rate=0.1 gamma=0.01\n",
      "gamma: 0.01\n",
      "learning_rate: 0.1\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n",
      "[2020-02-08 19:25:51,269][HYDRA] \t#2 : learning_rate=0.01 gamma=0.1\n",
      "gamma: 0.1\n",
      "learning_rate: 0.01\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n",
      "[2020-02-08 19:25:51,373][HYDRA] \t#3 : learning_rate=0.01 gamma=0.01\n",
      "gamma: 0.01\n",
      "learning_rate: 0.01\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n",
      "[2020-02-08 19:25:51,465][HYDRA] \t#4 : learning_rate=0.001 gamma=0.1\n",
      "gamma: 0.1\n",
      "learning_rate: 0.001\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n",
      "[2020-02-08 19:25:51,573][HYDRA] \t#5 : learning_rate=0.001 gamma=0.01\n",
      "gamma: 0.01\n",
      "learning_rate: 0.001\n",
      "run_mode: train\n",
      "week: 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python demo.py --multirun learning_rate=0.1,0.01,0.001 gamma=0.1,0.01\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
